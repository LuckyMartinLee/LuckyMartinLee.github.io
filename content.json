{"meta":{"title":"Martin Li's Personal Website - 李杨的个人站点","subtitle":null,"description":null,"author":"Martin Li","url":"http://luckymartinlee.github.io"},"pages":[{"title":"Martin Li (李 杨)","date":"2017-06-18T02:01:39.000Z","updated":"2020-12-10T06:07:24.815Z","comments":true,"path":"about/index.html","permalink":"http://luckymartinlee.github.io/about/index.html","excerpt":"","text":"Photo&ensp;&ensp;Click here About Me&ensp;&ensp;My name is Martin Li. And you can call me Li Yang (李 杨). I was born in 1988. &ensp;&ensp;I have a Bachelor degree, and once worked for Iflytek CO.,LTD. (科大讯飞) in Hefei AnHui as a senior software engineer. &ensp;&ensp;I am a software developer focusing on web technology, big data and a strong advocate and believer of Free Software. Now I am employed by LexisNexis (律商联讯) as a Manager Software Engineering. &ensp;&ensp;In my spare time, I have broad interests like many other young people. I like reading book, exchanging with other people in the forum on line, watching movie and taking a leisurely walk outdoors. Skills&ensp;&ensp;Python: ★★★★☆&ensp;&ensp;Scala: ★★★★☆&ensp;&ensp;PHP: ★★★★☆&ensp;&ensp;Java: ★★★★☆&ensp;&ensp;Spark: ★★★★☆&ensp;&ensp;Solr: ★★★★☆&ensp;&ensp;Elasticsearch: ★★★★☆&ensp;&ensp;MySQL: ★★★★☆&ensp;&ensp;MongoDB: ★★★★☆&ensp;&ensp;Hadoop: ★★★☆☆&ensp;&ensp;Docker: ★★★★☆&ensp;&ensp;Git/SVN: ★★★★☆&ensp;&ensp;Nginx: ★★★★☆&ensp;&ensp;NodeJs: ★★★☆☆&ensp;&ensp;Javascript: ★★★☆☆&ensp;&ensp;Html+Css: ★★★☆☆ Works&ensp;&ensp;Global China Law&ensp;&ensp;律商网&ensp;&ensp;雅集网&ensp;&ensp;畅言云系列&ensp;&ensp;&ensp;&ensp;畅言云&ensp;&ensp;&ensp;&ensp;畅言-教学通&ensp;&ensp;&ensp;&ensp;畅言云-资源中心 ContactsEmail:&ensp;&ensp;lucky.martin.lee@gmail.com&ensp;&ensp;541079843@qq.comWechat:&ensp;&ensp;"},{"title":"Martin Li (李 杨)","date":"2018-04-20T07:22:10.000Z","updated":"2020-12-10T06:12:20.137Z","comments":true,"path":"martin_photo/index.html","permalink":"http://luckymartinlee.github.io/martin_photo/index.html","excerpt":"","text":"&nbsp;&nbsp;&ensp;&ensp; &ensp;&ensp;2017年4月,摄于上海, LexisNexis Annual Hackathon 2017 &nbsp;&nbsp;&ensp;&ensp; &ensp;&ensp;2016年10月,摄于 北京"}],"posts":[{"title":"Java IO流详解","slug":"java_1-5","date":"2020-03-14T06:16:21.000Z","updated":"2020-12-26T07:16:17.449Z","comments":true,"path":"2020/03/14/java_1-5/","link":"","permalink":"http://luckymartinlee.github.io/2020/03/14/java_1-5/","excerpt":"","text":"## IO流总览 以上两图即是 Java IO流的总体概览，由上图，我们可以看出 按照“流”的数据流向，可以将其化分为：输入流和输出流。按照“流”中处理数据的单位，可以将其区分为：字节流和字符流。在java中，字节是占1个Byte，即8位；而字符是占2个Byte，即16位。而且，需要注意的是，java的字节是有符号类型，而字符是无符号类型！ 字节流的抽象基类： InputStream，OutputStream 字符流的抽象基类： Reader，Writer 由这四个类派生出来的子类名称都是以其父类名作为子类名的后缀，如InputStream的子类FileInputStream，Reader的子类FileReader。 字符流和字节流的区别字符流和字节流的使用非常相似，但是实际上字节流的操作不会经过缓冲区（内存）而是直接操作文本本身的，而字符流的操作会先经过缓冲区（内存）然后通过缓冲区再操作文件。 缓冲区就是一段特殊的内存区域，很多情况下当程序需要频繁地操作一个资源（如文件或数据库）则性能会很低，所以为了提升性能就可以将一部分数据暂时读写到缓存区，以后直接从此区域中读写数据即可，这样就显著提升了性。对于 Java 字符流的操作都是在缓冲区操作的，所以如果我们想在字符流操作中主动将缓冲区刷新到文件则可以使用 flush() 方法操作。 多数情况下使用字节流会更好，因为大多数时候 IO 操作都是直接操作磁盘文件，所以这些流在传输时都是以字节的方式进行的（图片等也都是按字节存储的）如果对于操作需要通过 IO 在内存中频繁处理字符串的情况使用字符流会好些，因为字符流具备缓冲区，提高了性能。 字符流与字节流转换OutputStreamWriter(OutStreamout):将字节流以字符流输出InputStreamReader(InputStream in)：将字节流以字符流输入 可对读取到的字节数据经过指定编码转换成字符可对读取到的字符数据经过指定编码转换成字节 Java序列化序列化就是一种用来处理对象流的机制，将对象的内容进行流化。可以对流化后的对象进行读写操作，可以将流化后的对象传输于网络之间。序列化是为了解决在对象流读写操作时所引发的问题序列化的实现：将需要被序列化的类实现Serialize接口，没有需要实现的方法，此接口只是为了标注对象可被序列化的，然后使用一个输出流（如：FileOutputStream）来构造一个ObjectOutputStream(对象流)对象，再使用ObjectOutputStream对象的write(Object obj)方法就可以将参数obj的对象写出 File类File类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 RandomAccessFile类该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点：该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw)注意：该对象在实例化时，如果要操作的文件不存在，会自动创建；如果文件存在，写数据未指定位置，会从头开始写，即覆盖原有的内容。 可以用于多线程下载或多个线程同时写数据到文件。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Java http协议与网络基础一","slug":"java_1-6","date":"2020-03-14T06:16:21.000Z","updated":"2020-12-27T01:22:25.804Z","comments":true,"path":"2020/03/14/java_1-6/","link":"","permalink":"http://luckymartinlee.github.io/2020/03/14/java_1-6/","excerpt":"","text":"## OSI 参考模型对比 TCP/IP 参考模型 OSI 参考模型 是 国际标准化组织ISO 提出的不基于具体机型、操作系统或公司的网络体系结构，称为开放系统互连参考模型，即OSI/RM（Open System Interconnection Reference Model），但其模型过于庞大、复杂招致了许多批评。与此相对，美国国防部提出了TCP/IP协议栈参考模型，简化了OSI参考模型，由于TCP/IP协议栈的简单，获得了广泛的应用，并成为后续因特网使用的参考模型。 TCP/IP 参考模型 常见协议 此处重点说下 TCP 协议TCP是面向连接的协议，因此每个TCP连接都有3个阶段：连接建立、数据传送和连接释放。连接建立经历三个步骤，通常称为“三次握手” TCP三次握手过程 1、 第一次握手（客户端发送请求）客户机发送连接请求报文段到服务器，并进入SYN_SENT状态，等待服务器确认。发送连接请求报文段内容：SYN=1，seq=x；SYN=1意思是一个TCP的SYN标志位置为1的包，指明客户端打算连接的服务器的端口；seq=x表示客户端初始序号x，保存在包头的序列号（Sequence Number）字段里。 2、 第二次握手（服务端回传确认）服务器收到客户端连接请求报文，如果同意建立连接，向客户机发回确认报文段（ACK）应答，并为该TCP连接分配TCP缓存和变量。服务器发回确认报文段内容：SYN=1，ACK=1，seq=y，ack=x+1；SYN标志位和ACK标志位均为1，同时将确认序号（Acknowledgement Number）设置为客户的ISN加1，即x+1；seq=y为服务端初始序号y。 3、 第三次握手（客户端回传确认）客户机收到服务器的确认报文段后，向服务器给出确认报文段（ACK），并且也要给该连接分配缓存和变量。此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。客户端发回确认报文段内容：ACK=1，seq=x+1，ack=y+1；ACK=1为确认报文段；seq=x+1为客户端序号加1；ack=y+1,为服务器发来的ACK的初始序号字段+1。 注意：握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。 TCP四次挥手过程由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 1、 TCP客户端发送一个FIN，用来关闭客户端到服务端的数据传送，客户端进入FIN_WAIT_1状态。发送报文段内容：FIN=1，seq=u；FIN=1表示请求切断连接；seq=u为客户端请求初始序号。 2、 服务端收到这个FIN，它发回一个ACK给客户端，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号；服务端进入CLOSE_WAIT状态。发送报文段内容：ACK=1，seq=v，ack=u+1；ACK=1为确认报文；seq=v为服务器确认初始序号；ack=u+1为客户端初始序号加1。 3、服务器关闭客户端的连接后，发送一个FIN给客户端，服务端进入LAST_ACK状态。发送报文段内容：FIN=1，ACK=1，seq=w，ack=u+1；FIN=1为请求切断连接，ACK=1为确认报文，seq=w为服务端请求切断初始序号。 4、 客户端收到FIN后，客户端进入TIME_WAIT状态，接着发回一个ACK报文给服务端确认，并将确认序号设置为收到序号加1，服务端进入CLOSED状态，完成四次挥手。发送报文内容：ACK=1，seq=u+1，ack=w+1；ACK=1为确认报文，seq=u+1为客户端初始序号加1，ack=w+1为服务器初始序号加1。 注意：为什么连接的时候是三次握手，关闭的时候却是四次挥手？因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭socket，所以只能先回复一个ACK报文，告诉客户端，“你发的FIN报文，我收到了”。只有等到服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送，故需要四步挥手。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Java 性能优化的若干原则","slug":"java_1-7","date":"2020-03-14T06:16:21.000Z","updated":"2020-12-26T02:41:21.435Z","comments":true,"path":"2020/03/14/java_1-7/","link":"","permalink":"http://luckymartinlee.github.io/2020/03/14/java_1-7/","excerpt":"","text":"## 正确使用基本数据类型和其包装类类型 虽然包装类型和基本类型在使用过程中是可以相互转换，但它们两者所产生的内存区域是完全不同的，基本类型数据产生和处理都在栈中处理，而包装类型是对象，是在堆中产生实例。在集合类对象，有对象方面需要的处理适用包装类型，其他的情况，建议提倡使用基本类型。 善用 ArrayList &amp; LinkedList一个是线性表，一个是链表，一句话，随机查询尽量使用 ArrayList，ArrayList优于LinkedList，LinkedList还要移动指针，添加删除的操作LinkedList优于 ArrayList，ArrayList还要移动数据，不过这是理论性分析，事实未必如此，重要的是理解好数据结构。 缓存经常使用的对象尽可能将经常使用的对象进行缓存，可以使用数组，或HashMap的容器来进行缓存，但这种方式可能导致系统占用过多的缓存，性能下降，推荐可以使用一些第三方的开源工具，如EhCache，Oscache进行缓存，他们基本都实现了FIFO/FLU等缓存算法。 尽早释放无用对象的引用大部分情况下，方法局部引用变量所引用的对象会随着方法结束而变成垃圾，因此，大部分时候程序无需将局部引用变量显式设为null。如：12345Public void doJob()&#123; Object obj =new Object(); …… Obj=null; // 没有必要&#125; 上面这个就没必要了，随着方法doJob()的执行完成，程序中obj引用变量的作用域会被gc回收。但是如果是改成下面：12345678public void doJob()&#123; Object obj =new Object(); …… Obj=null; // 有必要 //以下执行耗时，耗内存操作，或调用耗时，耗内存的方法 …… &#125; 这时候就有必要将obj赋值为null，可以尽早的释放对Object对象的引用。 使用final修饰符带有final修饰符的类是不可派生的。在Java核心API中，有许多应用 final的例子，例如java.lang.String。为String类指定final防止了使用者覆盖length()方法。另外，如果一个类是 final的，则该类所有方法都是final的。Java编译器会寻找机会内联（inline）所有的final方法（这和具体的编译器实现有关）。此举能够使性能平均提高50%。 没有必要时请不用使用静态变量使用Java的开发者都知道，当某个对象被定义为stataic变量所引用，这个对象所占有的内存将不会被回收。有时，开发者会将经常调用的对象或者变量定义为static，以便提高程序的运行性能。因此，不是常用到的对象或者变量，不要定义为static类型的变量，尤其是静态类对象的定义，一定要仔细考虑是否有必要。如：123public class X&#123; static Y a = new Y(); &#125; 类X创建了，没有被回收的话，静态变量a一直占用内存。 充分利用单例机制使用单例可以减少对资源的加载，缩短运行的时间，提高系统效率。但是，单例并不是所有地方都适用于。简单来说，单例可以适用于以下两个方面：1、 控制资源的使用，通过线程同步来控制资源的并发访问2、 控制实例的产生，以达到节约资源的目的 在finally块中释放资源程序中使用到的资源应当被释放，以避免资源泄漏。这最好在finally块中去做。不管程序执行的结果如何，finally块总是会执行的，以确保资源的正确关闭。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Java 内存模型和参数优化","slug":"java_1-3","date":"2020-03-11T13:46:24.000Z","updated":"2020-12-26T01:01:16.047Z","comments":true,"path":"2020/03/11/java_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2020/03/11/java_1-3/","excerpt":"","text":"## Java程序执行过程 1、 第一步，Java源代码文件(.java后缀)会被Java编译器编译为字节码文件(.class后缀)2、 第二步，由JVM中的类加载器加载各个类的字节码文件，加载完毕之后，交由JVM执行引擎执行3、 第三步，在程序执行过程中，JVM会用一段空间来存储程序执行期间需要用到的数据和相关信息，这段空间一般被称作为Runtime Data Area（运行时数据区），也就是我们常说的JVM内存4、 因此，在Java中我们常常说到的内存管理就是针对这段空间进行管理（如何分配和回收内存空间） JVM 内存模型 上图中可以看出，JVM的内存空间分为3大部分，分别是堆内存、方法区和栈内存。其中栈内存可以再细分为java虚拟机栈和本地方法栈。堆内存可以划分为新生代和老年代。新生代中还可以再次划分为Eden区、From Survivor区和To Survivor区。划分出来的各个区，分别保存不同的数据 堆内存： 用来存储对象本身的以及数组，是JVM内存模型中最大的一块区域，被所有线程共享，是在JVM启动时候进行创建的。几乎所有的对象的空间分配都是在堆内存上进行分配的。 考虑到JVM的内存回收机制，堆内存可以划分为新生代和老年代两个区域（默认新生代与老年代的空间大小为1：2）。新生代可以再划分为Eden区、From Survivor区和To Survivor区（三者比例为8：1：1）。几乎所有的新对象的创建都是在Eden区进行的。在垃圾回收（GC）过程中，Eden中的活跃对象会被转移到Survivor区，当再到达一定的年龄（经历过的Minor GC的次数），会被转移到老年代中。 堆可以处于物理上不连续的内存空间中，但是需要满足逻辑上的连续。在实现时，可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的 方法区：存储了每个类的信息（包括类的名称、方法信息、字段信息）、静态变量、常量以及编译器编译后的代码等。（注：在方法区中有一个非常重要的部分就是运行时常量池，它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到JVM后，对应的运行时常量池就被创建出来。当然并非Class文件常量池中的内容才能进入运行时常量池，在运行期间也可将新的常量放入运行时常量池中，比如String的intern方法。）方法区也叫作永久代，也是被所有的线程共享的。 JDK1.7中，已经把放在永久代的字符串常量池移到堆中。JDK1.8撤销永久代，引入元空间。 方法区不需要连续的内存，可以选择固定大小或者可扩展。并且还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 程序计数器：用于标识当前线程执行的字节码文件的行号指示器。多线程情况下，每个线程都具有各自独立的程序计数器，所以该区域是非线程共享的内存区域。它是CPU中的寄存器，它保存的是程序当前执行的指令的地址（也可以说保存下一条指令的所在存储单元的地址），当CPU需要执行指令时，需要从程序计数器中得到当前需要执行的指令所在存储单元的地址，然后根据得到的地址获取到指令，在得到指令之后，程序计数器便自动加1或者根据转移指针得到下一条指令的地址，如此循环，直至执行完所有的指令；（注：JVM中的程序计数器并不像汇编语言中的程序计数器一样是物理概念上的CPU寄存器，但是逻辑作用上是等同的，在JVM中多线程是通过线程轮流切换来获得CPU执行时间的，在任一具体时刻，一个CPU的内核只会执行一条线程中的指令，为了能够使得每个线程都在线程切换后能够恢复在切换之前的程序执行位置，每个线程都需要有自己独立的程序计数器，并且不能互相被干扰，否则就会影响到程序的正常执行次序。因此，可以这么说，程序计数器是每个线程所私有的） 当执行java方法时候，计数器中保存的是字节码文件的行号；当执行Native方法时，计数器的值为空。 Java栈：也叫作虚拟机栈，Java栈是Java方法执行的内存模型，Java栈中存放的是一个个的栈帧，每个栈帧（包括：局部变量表、操作数栈、运行时常量池（在下文中提到的方法区内）的引用、方法返回地址和一些额外的附加信息）对应一个被调用的方法，当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈（如果方法methodOne方法调用了methodTwo，那么methodOne就会先入栈创建一个栈桢，接着methodTwo再入栈成为栈顶(假设没有其他的方法执行)，methodTwo执行完先出栈，接着methodOne执行完出栈）。由于每个线程正在执行的方法可能不同，因此每个线程都会有一个自己的Java栈，互不干扰。 局部变量表中，可以存放的数据有8种基本数据类型（boolean，byte，char，short，int，float，long，double），对象引用和returnAddress类型。其中long和double因为是64位，会占用两个局部变量的空间。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度（比如递归调用的时候），将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 本地方法栈：Java栈是为执行Java方法服务的，而本地方法栈则是为执行本地方法（Native Method）服务的。它也是线程私有的内存区域，与java栈比较相似，不同之处在于该区域主要是保存Native方法相关的数据。Native方法是非Java语言编写的方法。 与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 举例说明一下各个内存区域中保存的信息 上面的代码中：1、 堆中进行对象的空间分配，比如Hashtable对象和String对象。2、 方法区中保存类信息（TestJVM），方法（put方法，print方法，test方法）和静态变量（NUM）3、 java栈中保存对象引用（score） JVM内存参数设置 1、 新生代分三个区，一个Eden区，两个Survivor区（from和to区），可以通过-XXSurvivorRatio调整比例作用：默认-XX:SurvivorRatio＝8，表示Survivor区与Eden区的大小比值是1:1:8，在MinorGC过程，如果survivor空间不够大，不能够存储所有的从eden空间和from suvivor空间复制过来活动对象，溢出的对象会被复制到old代，溢出迁移到old代，会导致old代的空间快速增长 2、 大部分对象在先在Eden区中申请内存。作用：可以通过设置-XX:PreTenureSizeThreShold大小，令大于这个值的对象直接保存到老年代，避免在Eden区与Survivor区之间频繁地通过复制算法回收内存 3、 当Eden区满时，无法为新的对象分配内存时，会进行Minor GC对其回收无用对象占用的内存，如果还有存活对象，则将存活的对象复制到Survivor From区（两个中Survivor对称）；然后从Eden区存活下来的对象，就会被复制到From，当这个From区满时，此区的存活对象将被复制到To区，接下来Eden区存活下来的对象就会被复制到To区，经历一定的次数Minor GC后，还存活的对象，将被复制“老年代(Tenured)”。作用：Minor默认15次，可通过-MaxTenuringThreshold参数调整新生代回收次数，防止对象过早进入老年代，降低老年代溢出的可能性 4、新生代和老年代的默认比例为1:2，即新生代占堆内存的1/3，老年代占2/3，可调整-XX:NewRatio的大小设置年轻和年老的比例。作用：默认-XX:NewRatio＝2，即young:tenured＝1:2，适当调整新生代大小，可以一定层度上较少Full GC出现的概率 其他参数： -Xms and -Xmx (or: -XX:InitialHeapSize and -XX:MaxHeapSize)指定JVM的初始和最大堆内存大小，两值可以设置相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xmn设置新生代大小。整个堆大小 = 新生代大小 + 老年代大小 + 持久代大小所以增大新生代后，将会减小老年代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -Xss设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 -XX:+HeapDumpOnOutOfMemoryError and -XX:HeapDumpPath让JVM在发生内存溢出时自动的生成堆内存快照（堆内存快照文件有可能很庞大，推荐将堆内存快照生成路径指定到一个拥有足够磁盘空间的地方。） -XX:OnOutOfMemoryError当内存溢发生时，我们甚至可以可以执行一些指令，比如发个E-mail通知管理员或者执行一些清理工作（$ java -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof -XX:OnOutOfMemoryError =”sh ~/cleanup.sh” MyApp） -XX:PermSize and -XX:MaxPermSize设置永久代大小的初始值和最大值（默认：最小值为物理内存的1/64，最大值为物理内存的1/16，永久代在堆内存中是一块独立的区域，这里设置的永久代大小并不会被包括在使用参数-XX:MaxHeapSize 设置的堆内存大小中） -XX:PretenureSizeThreshold令大于这个设置值的对象直接在老年代分配。这样做的目的是避免在Eden区及两个Survivor区之间发生大量的内存复制 总结：JVM内存的系统级的调优主要的目的是减少Minor GC的频率和Full GC的次数，过多的Minor GC和Full GC是会占用很多的系统资源，影响系统的吞吐量。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Java 反射 Reflect","slug":"java_1-4","date":"2019-09-21T06:33:32.000Z","updated":"2020-12-16T03:46:35.614Z","comments":true,"path":"2019/09/21/java_1-4/","link":"","permalink":"http://luckymartinlee.github.io/2019/09/21/java_1-4/","excerpt":"","text":"Class 类面向对象的世界里，万事万物都是对象，类也是对象。在 Java 中，除 静态成员 和 普通数据类型 不是对象，其他皆为对象。其中定义的各种类，是 java.lang.Class 类的实例对象 类对象的表示123456789101112131415161718192021222324252627282930public class Test &#123; public static void main(String[] args) &#123; // 创建一个 Foo 类的实例对象 foo1 Foo foo1 = new Foo(); // foo1 是 Foo 类的实例对象 // 万事万物皆是对象， Foo 类本身也是对象，它是 Class 类的实例对象 // 所有的类都是 Class 类的实例对象， 这个实例对象有三种表示方式 // 方式一, 通过类的静态成员变量 class，每个类都包含这个变量 Class c1 = Foo.class; // 方式二, 通过类的实例对象的 getClass 方法 Class c2 = foo1.getClass(); // 方式三, 通过类的全称(包含包名)获取 Class c3 = Class.forName(\"com.test.Foo\"); // c1,c2,c3 表示了 Foo 类的类类型 (class type), 且一个类只可能是 Class 类的一个类类型， // 即是 一个类的类类型 是唯一的， 所以 以上三者是相等的 bool b1 = c1 == c2; // true bool b2 = c2 == c3; // true // 通过类的类类型，就可以去创建该类的实例对象， // 需要注意，该类需要定义一个无参构造函数，且生成的实例对象需要强制类型转换 Foo foo2 = (Foo)c3.newInstance() &#125;&#125;class Foo &#123;&#125; 动态加载类与静态加载类静态加载类: 编译时刻加载的类就是静态加载类，new 类名 出来的就是静态加载类，编译时刻就需要加载可能使用的所有的类。动态加载类: 运行时刻加载的类就是动态加载类，看下面的例子12345678910111213141516171819202122232425262728293031// 定义借口，同意标准interface Animalale &#123; public void run();&#125;// 定义类， 实现统一标准class Dog implements Animalale &#123; public void run() &#123; System.out.println(\"dog running.\"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; try &#123; // 动态加载类， 运行时刻加载 Class c1 = Class.forName[args[0]] // 通过类的类类型，去创建实例对象， Animalale animal = (Animalale) c1.newInstance(); animal.run(); &#125; catch (Exception e) &#123; e.printStackTrace() &#125; &#125;&#125;// 命令行输入java test Dog// 输出dog running. 上面的代码编译后，如有新的动物加入，只需要新写一个类实现 Animal 接口，编译这个新的类即可，原来的代码不需要改动。开发过程中，功能性的类都可以通过这种动态加载方式实现。 数据类型的类类型1234567// void 关键字 和 基本数据类型 都有类类型Class c1 = int.class;Class c2 = String.class;Class c3 = double.class; // double 数据类型的类类型Class c4 = Double.class; // Double 类的类类型Class c5 = void.class;// 注意此处的 c3 ！= c4, Class 类的常用函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 获取类的成员方法信息 * @param obj 类的实例对象 */public static void printClassMethodMessage(Object obj)&#123; // 获取类信息，首先需要获得类类型 Class c = obj.getClass();// 传递的是哪个子类的实例对象，获取的就是该子类的类类型 // 获取类名(包含包名) System.out.println(\"类名称:\"+c.getName()); // 获取类名(不包含包名) // System.out.println(\"类名称:\"+c.getSimpleName()); /* * Method 类，所有成员方法都是 Method 类的实例对象 * getMethods() // 获取所有 public 类型的成员方法，包括父类继承过来的方法 * getDeclaredMethods() // 获取该类所有自己的声明的成员方法，包括public, private等，不论访问权限 */ Method[] ms = c.getMethods(); for(int i = 0; i &lt; ms.length;i++)&#123; // 获取返回值类型的类类型 Class returnType = ms[i].getReturnType(); System.out.print(returnType.getName()+\" \"); // 获取方法名称 System.out.print(ms[i].getName()+\"(\"); // 获取参数类型的类类型 数组 Class[] paramTypes = ms[i].getParameterTypes(); for (Class class1 : paramTypes) &#123; System.out.print(class1.getName()+\",\"); &#125; System.out.println(\")\"); &#125;&#125;/** * 获取类的成员变量信息 * @param obj 类的实例对象 */public static void printFieldMessage(Object obj) &#123; Class c = obj.getClass(); /* * 类的成员变量也是对象, 是 java.lang.reflect.Field的对象 * getFields() // 获取所有 public 类型的成员变量，包括父类继承过来的 * getDeclaredFields() // 获取该类所有自己的声明的成员变量，不论访问权限 */ //Field[] fs = c.getFields(); Field[] fs = c.getDeclaredFields(); for (Field field : fs) &#123; // 获取成员变量类型的类类型 Class fieldType = field.getType(); // 获取成员变量类型的名字 String typeName = fieldType.getName(); // 获取成员变量的名字 String fieldName = field.getName(); System.out.println(typeName+\" \"+fieldName); &#125;&#125;/** * 获取类的构造函数信息 * @param obj 类的实例对象 */public static void printConMessage(Object obj)&#123; Class c = obj.getClass(); /* * 构造函数也是对象，是java.lang.Constructor的实例对象，它封装的了构造函数的信息 * getConstructors() // 获取所有 public 类型的构造函数 * getDeclaredConstructors() // 获取该类所有自己的声明的构造函数，不论访问权限 */ //Constructor[] cs = c.getConstructors(); Constructor[] cs = c.getDeclaredConstructors(); for (Constructor constructor : cs) &#123; // 获取构造函数名字 System.out.print(constructor.getName()+\"(\"); // 获取构造函数 参数类型的类类型 数组 Class[] paramTypes = constructor.getParameterTypes(); for (Class class1 : paramTypes) &#123; System.out.print(class1.getName()+\",\"); &#125; System.out.println(\")\"); &#125;&#125; 方法反射基本操作通过 Method.invoke(实例对象,[参数列表]) 进行 方法反射操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MethodDemo1 &#123; public static void main(String[] args) &#123; // 获取 print(int,int)方法，先获取类类型 A a1 = new A(); Class c = a1.getClass(); /* * 获取方法，由方法名和参数列表唯一决定一个方法 * getMethod() // 获取所有 public 类型的成员方法 * getDelcaredMethod() // 获取该类所有自己的声明的成员方法，不论访问权限 */ try &#123; // 两种方式获取方法对象，效果一样 //Method m = c.getMethod(\"print\", new Class[]&#123;int.class,int.class&#125;); Method m = c.getMethod(\"print\", int.class,int.class); // 方法反射操作 // 下面两个参数传递方式，效果一样, o 就是 print 方法的返回值，如没有返回值, o 就是 null //Object o = m.invoke(a1,new Object[]&#123;10,20&#125;); Object o = m.invoke(a1, 10,20); 上面通过方法反射操作和 通过 类的实例对象 直接调用，效果一样 //a1.print(10, 20); System.out.println(\"==================\"); // 获取参数为 String 类型的 print(String,String) //Method m1 = c.getMethod(\"print\", new Class[]&#123;String.class,String.class&#125;); Method m1 = c.getMethod(\"print\",String.class,String.class); //a1.print(\"hello\", \"WORLD\"); o = m1.invoke(a1, \"hello\",\"WORLD\"); System.out.println(\"===================\"); // 获取 无参数的 print(String,String) // Method m2 = c.getMethod(\"print\", new Class[]&#123;&#125;); Method m2 = c.getMethod(\"print\"); // 两种调用方式，效果一样 // m2.invoke(a1, new Object[]&#123;&#125;); m2.invoke(a1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;class A&#123; public void print()&#123; System.out.println(\"helloworld\"); &#125; public void print(int a,int b)&#123; System.out.println(a+b); &#125; public void print(String a,String b)&#123; System.out.println(a.toUpperCase()+\",\"+b.toLowerCase()); &#125;&#125; 深入理解 Java 泛型看下面实例123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; ArrayList list = new ArrayList(); ArrayList&lt;String&gt; list1 = new ArrayList&lt;String&gt;(); list1.add(\"hello\"); //list1.add(20); // 编译出错, 不能向 String 集合中加入 整型数据 Class c1 = list.getClass(); Class c2 = list1.getClass(); System.out.println(c1 == c2); // 返回结果: true // 上面说明， 编译之后的集合的泛型是去泛型化的， // 集合的泛型，目的是防止输入错误，也就是泛型只在编译阶段有效， // 编译之后就无效了 // 而反射是在 运行时刻加载，通过反射，我们可以绕过泛型的编译限制，如下 try &#123; Method m = c2.getMethod(\"add\", Object.class); m.invoke(list1, 20); // 泛型绕过编译，绕过泛型，加入集合 System.out.println(list1.size()); System.out.println(list1); // 注意此刻 不能 foreach 遍历，遍历报错，类型不对 /* for (String string : list1) &#123; System.out.println(string); &#125; */ &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Spark 算子详解","slug":"spark_1-2","date":"2019-05-10T10:13:12.000Z","updated":"2020-12-09T08:49:01.150Z","comments":true,"path":"2019/05/10/spark_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2019/05/10/spark_1-2/","excerpt":"","text":"算子分类1、 Transformation算子(转换算子)，此类算子操作是延迟计算的，即是将要从一个RDD转换成另一个RDD的操作，但不是马上执行，并不触发提交Job作业，需要等到有Action操作的时候才会真正触发运算。根据操作数据类型的不同，可细分为 Value数据类型的Transformation算子 和 Key-Value数据类型的Transfromation算子2、Action算子(行动算子), 这类算子会触发 SparkContext 提交Job作业，并将数据输出 Spark系统。 重难 Transformation算子glom该函数是将RDD中每一个分区中各个元素合并成一个Array，这样每一个分区就只有一个数组元素1234val a = sc.parallelize(1 to 9, 3)a.glom.collect//输出res66: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9)) mapPartitions(function)与 map 类似，但函数单独在 RDD 的每个分区上运行12345678910val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list, 3).mapPartitions(iterator =&gt; &#123; val buffer = new ListBuffer[Int] while (iterator.hasNext) &#123; buffer.append(iterator.next() * 100) &#125; buffer.toIterator&#125;).foreach(println)//输出结果100 200 300 400 500 600 join在一个(K, V)和(K, W)类型的RDD 上调用时，返回一个(K, (V, W)) pairs 的 RDD，等价于内连接操作(不含 V或W 为空的)。执行外连接，可以使用：leftOuterJoin (不含 W 为空)rightOuterJoin (不含 W 为空)fullOuterJoin (包含V 和 W 为空) sample数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction,小于等于1)、随机数生成器的种子 (seed)123val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list).sample(withReplacement = false, fraction = 0.5).foreach(println)//输出结果随机 groupByKey按照键进行分组,在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable)123456val list = List((\"hadoop\", 2), (\"spark\", 3), (\"spark\", 5), (\"storm\", 6), (\"hadoop\", 2))sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)//输出：(spark,List(3, 5))(hadoop,List(2, 2))(storm,List(6)) cogroup先同一个 (K, V) RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组12345678val list01 = List((1, \"a\"),(1, \"a\"), (2, \"b\"), (3, \"e\"))val list02 = List((1, \"A\"), (2, \"B\"), (3, \"E\"))val list03 = List((1, \"[ab]\"), (2, \"[bB]\"), (3, \"eE\"),(3, \"eE\"))sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)// 输出(1,(CompactBuffer(a, a),CompactBuffer(A),CompactBuffer([ab])))(3,(CompactBuffer(e),CompactBuffer(E),CompactBuffer(eE, eE)))(2,(CompactBuffer(b),CompactBuffer(B),CompactBuffer([bB])) reduceByKey按照键进行归约操作123456val list = List((\"hadoop\", 2), (\"spark\", 3), (\"spark\", 5), (\"storm\", 6), (\"hadoop\", 2))sc.parallelize(list).reduceByKey(_ + _).foreach(println)//输出(spark,8)(hadoop,4)(storm,6) sortBy(function) &amp; sortByKey按照键进行排序，需要 collect 等action算子后才是有序的123456val list01 = List((100, \"hadoop\"), (90, \"spark\"), (120, \"storm\"))sc.parallelize(list01).sortByKey(ascending = false).collect.foreach(println)// 输出(120,storm)(100,hadoop)(90,spark) 按照指定function进行排序,需要 collect 等action算子后才是有序的123456val list02 = List((\"hadoop\",100), (\"spark\",90), (\"storm\",120))sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=false).collect.foreach(println)// 输出(storm,120)(hadoop,100)(spark,90) aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])当针对(K，V)对的数据集时，返回（K，U）对的数据集，先对分区内执行seqOp函数，zeroValue 聚合每个键的值，再对分区间执行combOp函数。与groupByKey 类似，reduce 任务的数量可通过第二个参数 numPartitions 进行配置。示例如下：12345678910// 为了清晰，以下所有参数均使用具名传参val list = List((\"hadoop\", 3), (\"hadoop\", 2), (\"spark\", 4), (\"spark\", 3), (\"storm\", 6), (\"storm\", 8))sc.parallelize(list,numSlices = 2).aggregateByKey(zeroValue = 0,numPartitions = 3)( seqOp = math.max(_, _), combOp = _ + _ ).collect.foreach(println)//输出结果：(hadoop,3)(storm,8)(spark,7) 这里使用了 numSlices = 2 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下： 基于同样的执行流程，如果 numSlices = 1，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：123(hadoop,3)(storm,8)(spark,4) 同样的，如果每个单词对一个分区，即 numSlices = 6，此时相当于求和操作，执行结果为：123(hadoop,5)(storm,14)(spark,7) aggregateByKey(zeroValue = 0,numPartitions = 3) 的第二个参数 numPartitions 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 getNumPartitions 方法获取分区数量 combineByKeyC C, mergeValue:(C, V) C, mergeCombiners:(C, C) C, partitioner:Partitioner, mapSideCombine:Boolean=true, serializer:Serializer=null):RDD[(K,C)] 参数：createCombiner:V=&gt;C 分组内的创建组合的函数。即是对读进来的数据进行初始化，其把当前的值作为参数，可以对该值做一些转换操作，转换为我们想要的数据格式参数：mergeValue:(C,V)=&gt;C 该函数主要是分区内的合并函数，作用在每一个分区内部。其功能主要是将V合并到之前(createCombiner)的元素C上,注意，这里的C指的是上一函数转换之后的数据格式，而这里的V指的是原始数据格式(上一函数为转换之前的)参数：mergeCombiners:(C,C)=&gt;R 该函数主要是进行分区之间合并，此时是将两个C合并为一个C，例如两个C:(Int)进行相加之后得到一个R:(Int)参数：partitioner:自定义分区数，默认是hashPartitioner参数：mapSideCombine:Boolean=true 该参数是设置是否在map端进行combine操作，为了减小传输量，很多 combine 可以在 map 端先做，比如叠加，可以先在一个 partition 中把所有相同的 key 的 value 叠加，参数：serializerClass： String = null，传输需要序列化，用户可以自定义序列化类 12345678910val ls3 = List((\"001\", \"011\"), (\"001\",\"012\"), (\"002\", \"011\"), (\"002\", \"013\"), (\"002\", \"014\"))val d1 = sc.parallelize(ls3,2)d1.combineByKey((v: (String)) =&gt; (v, 1),(acc: (String, Int),v: (String)) =&gt; (v+\":\"+acc._1,acc._2+1),(p1:(String,Int),p2:(String,Int)) =&gt; (p1._1 + \":\" + p2._1,p1._2 + p2._2)).collect().foreach(println)//输出(002,(014:013:011,3))(001,(012:011,2)) 重难 Action算子takeOrdered按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 n 个元素。需要注意的是 takeOrdered 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 Ordering[T] 实现自定义比较器，然后将其作为隐式参数引入。1234567891011// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序class CustomOrdering extends Ordering[(Int, String)] &#123; override def compare(x: (Int, String), y: (Int, String)): Int = if (x._2.length &gt; y._2.length) 1 else -1&#125;val list = List((1, \"hadoop\"), (1, \"storm\"), (1, \"azkaban\"), (1, \"hive\"))// 引入隐式默认值implicit val implicitOrdering = new CustomOrderingsc.parallelize(list).takeOrdered(5)//输出Array((1,hive), (1,storm), (1,hadoop), (1,azkaban) take(n)将RDD中的前 n 个元素作为一个 array 数组返回,是无序的。 first返回 RDD 中的第一个元素，等价于 take(1)。 top（num：Int）（implicit ord：Ordering[T]）：Array[T]默认返回最大的k个元素，可以定义排序的方式Ordering[T]。12345678910class CustomOrdering extends Ordering[Int] &#123; override def compare(x: Int, y: Int): Int = if (x &gt; y) -1 else 1&#125;val list0 = List(3,5,1,6,2)// 引入隐式默认值implicit val implicitOrdering = new CustomOrderingsc.parallelize(list0).top(5)//输出Array(1,2,3,5,6)","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"Spark","slug":"Spark","permalink":"http://luckymartinlee.github.io/tags/Spark/"}]},{"title":"压力测试工具 ab (apache bench) 使用","slug":"ab_1-1","date":"2019-05-10T10:13:12.000Z","updated":"2020-12-07T12:52:37.204Z","comments":true,"path":"2019/05/10/ab_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2019/05/10/ab_1-1/","excerpt":"","text":"网络服务性能相关概念服务器平均请求处理时间（Time per request: across all concurrent requests）即是在某个并发用户数下服务器处理一条请求的平均时间 服务器平均请求处理时间 = 处理完成所有请求数所花费的时间 / 总请求数Time per request(across all concurrent requests) = Time taken for / testsComplete requests 用户平均请求等待时间（Time per request）即是用户获得相应的平均等待时间 用户平均请求等待时间 = 处理完成所有请求数所花费的时间/ （总请求数 / 并发用户数），即Time per request = Time taken for tests /（ Complete requests / Concurrency Level） 吞吐率（Requests per second，QPS，RPS）即是在某个并发用户数下单位时间内处理的请求数,单位是reqs/s, 也是 “服务器平均请求处理时间” d的倒数。某个并发用户数下单位时间内能处理的最大请求数，称之为最大吞吐率。 吞吐率 = 总请求数 / 处理完成这些请求数所花费的时间Request per second = Complete requests / Time taken for tests 并发连接数（The number of concurrent connections）即是在某个时刻服务器所接受的请求数目，就是会话数量。 并发用户数（The number of concurrent users，Concurrency Level）即是指某个时刻使用系统的用户数(可能有一个用户有一个或者多个连接)。要注意区分和并发连接数之间的区别，一个用户可能同时会产生多个会话，也即连接数。 总结:如果要说QPS时，一定需要指明是多少并发用户数下的QPS，否则豪无意义。因为单用户数的40QPS和20并发用户数下的40QPS是两个不同的概念，前者说明该服务可以在一秒内串行执行40个请求，而后者说明在并发20个请求的情况下，一秒内该应用能处理40个请求。当QPS相同时，越大的并发用户数，说明了网站服务并发处理能力越好。对于当前的web服务器，其处理单个用户的请求肯定戳戳有余，这个时候会存在资源浪费的情况（一方面该服务器可能有多个cpu，但是只处理单个进程，另一方面，在处理一个进程中，有些阶段可能是IO阶段，这个时候会造成CPU等待，但是有没有其他请求进程可以被处理）。而当并发数设置的过大时，每秒钟都会有很多请求需要处理，会造成进程（线程）频繁切换，反正真正用于处理请求的时间变少，每秒能够处理的请求数反而变少，同时用户的请求等待时间也会变大，甚至超过用户的心理底线，等待时间过长。 所以在最小并发数和最大并发数之间，一定有一个最合适的并发数值，在并发数下，QPS能够达到最大。 但是，这个并发并非是一个最佳的并发，因为当QPS到达最大时的并发，可能已经造成用户的等待时间变得超过了其最优值，所以对于一个系统，其最佳的并发数，一定需要结合QPS，用户的等待时间来综合确定。 下面这张图是应用服务器关于并发用户数，QPS，用户平均等待时间的一张关系图，对于实际的系统，也应该是对于不同的并发数，进行多次测试，获取到这些数值后，画出这样一张图出来，以便于分析出系统的最佳并发用户数。 响应时间关系图 ab 简介和使用实例简介ab全称为：apache bench它是apache自带的压力测试工具。ab非常实用，它不仅可以对apache服务器进行网站访问压力测试，也可以对或其它类型的服务器进行压力测试。比如nginx、tomcat、IIS等。我们可以直接安装apache的工具包httpd-tools。如下：1yum -y install httpd-tools 使用ab –V命令检测是否安装成功。如下：1ab -V 实例1ab -n 100 -c 10 -H \"token: fioj3iorm2aoi4ej\" -p /data/postdata.txt -T application/x-www-form-urlencoded \"http://127.0.0.1/test\" 上面命令的含义是设置请求header中参数token = fioj3iorm2aoi4ej，向 http://127.0.0.1/test 地址发送POST请求，POST表单数据存放在本地文件/data/postdata.txt中，其content-type格式为 application/x-www-form-urlencoded，发送的并发请求数为 10，总请求数为 100。成功后返回： 测试结果含义 返回值名称 含义 Server Software web服务器软件及版本 Server Hostname 表示请求的URL中的主机部分名称 Server Port 被测试的Web服务器的监听端口 Document Path 请求的页面路径 Document Length 页面大小 Concurrency Level 并发请求数 Time taken for tests 整个测试持续的时间,测试总共花费的时间 Complete requests 完成的请求数 Failed requests 失败的请求数，这里的失败是指请求的连接服务器、发送数据、接收数据等环节发生异常，以及无响应后超时的情况。对于超时时间的设置可以用ab的-t参数。如果接受到的http响应数据的头信息中含有2xx以外的状态码，则会在测试结果显示另一个名为“Non-2xx responses”的统计项，用于统计这部分请求数，这些请求并不算是失败的请求。 Write errors 写入错误 Total transferred 总共传输字节数,整个场景中的网络传输量,包含http的头信息等。使用ab的-v参数即可查看详细的http头信息。 HTML transferred html字节数，整个场景中的HTML内容传输量。也就是减去了Total transferred中http响应数据中头信息的长度。 Requests per second 每秒处理的请求数，服务器的吞吐量，大家最关心的指标之一 Time per request 用户平均请求等待时间，大家最关心的指标之二 Time per request 服务器平均处理时间，大家最关心的指标之三 Transfer rate 平均传输速率（每秒收到的速率）平均每秒网络上的流量，可以很好的说明服务器在处理能力达到限制时，其出口带宽的需求量，也可以帮助排除是否存在网络流量过大导致响应时间延长的问题。 下面段表示网络上消耗的时间的分解 下面这段是每个请求处理时间的分布情况，50%的处理时间在4930ms内，66%的处理时间在5008ms内…，重要的是看90%的处理时间。 常见问题压力测试需要当登录怎么办？1、先用账户和密码在浏览器登录后，用开发者工具找到会话的Cookie值（Session ID）记下来。2、使用下面命令传入Cookie值1ab －n 100 －C key＝value http://127.0.0.1/test","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://luckymartinlee.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://luckymartinlee.github.io/tags/Shell/"}]},{"title":"Java 集合基础","slug":"java_1-2","date":"2019-04-21T07:02:38.000Z","updated":"2020-12-27T10:05:07.709Z","comments":true,"path":"2019/04/21/java_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2019/04/21/java_1-2/","excerpt":"","text":"简介集合：集合是Java中提供的一种容器，可以用来存储多个数据。集合和数组的区别：（1）数组长度的是固定的，集合的长度是可变的。（2）数组中存储的都是同一类型的元素。集合存储的都是对象，对象的类型可以不一致。Java集合类主要由两个根接口Collection和Map派生出来的。Collection有三个子接口： List、Set、Queue（Java5新增的队列）。Java集合大致也可分成List、Set、Queue、Map四种接口体系，注意：Map不是Collection的子接口。Collection接口：单列数据，定义了存取一组对象的方法的集合。Map接口：双列数据，保存具有映射关系“key-value”对。 Collection子接口：List接口List集合类中元素有序、且可重复，集合中的每个元素都有其对应的顺序索引。List集合默认按照元素的添加顺序设置元素的索引，可以通过索引（类似数组的下标）来访问指定位置的集合元素。 List接口的实现类主要有：ArrayList、LinkedList和VectorArrayList: 有序，元素可重复，线程不安全。它是一个动态数组，允许任何符合规则的元素插入包括null。 它能快速随机访问存储的元素，支持随机访问，查询速度快，增删元素慢。 LinkedList: 有序，元素可重复，线程不安全。可以根据索引访问集合元素外，LinkedList还实现了Deque接口。内部以链表的形式保存集合中的元素，所以随机访问集合中的元素性能较差，但在频繁的插入或删除元素时有较好的性能，线程不安全。 Vector: 有序，元素可重复，线程安全。大多数操作与ArrayList相同，区别之处在于Vector是线程安全的，属于强同步类。因此开销就比ArrayList要大，访问要慢。大多数清空下使用 ArrayList而不是Vector,因为同步完全可以由自己来控制。Vector每次扩容请求其大小的2倍空间，而ArrayList是1.5倍。Vector还有一个子类Stack。 Collection子接口：Set接口Set集合不允许包含相同的元素，如果试把两个相同的元素加入同一个Set集合中，则会添加操作失败。Set集合判断两个对象是否相同是根据 equals() 方法，而不是使用 == 运算符。 HashSet： 无序，元素不可重复，线程不安全。按Hash算法来存储集合中的元素，因此具有很好的存取、查找、删除性能。特点：1、 不能保证元素的排列顺序2、 HashSet不是线程安全的3、 集合元素可以是null HashSet集合判断两个元素相等的标准：两个对象通过 hashCode() 方法比较相等，并且两个对象的 equals() 方法返回值也相等。对于存放在Set容器中的对象，对应的类一定要重写equals()和hashCode(Object obj)方法，以实现对象相等规则。 LinkedHashSet： 有序，元素不可重复，线程不安全。它是HashSet的子类，它也是根据元素的hashCode值来决定元素的存储位置。但它同时使用双向链表维护元素的次序，元素的顺序与添加顺序一致。由于LinkedHashSet需要维护元素的插入顺序，因此性能略低于HashSet，但在迭代访问Set里的全部元素时有很好的性能。LinkedHashSet不允许集合元素重复。 TreeSet： 有序（非输入顺序），元素不可重复，线程不安全。它是SortedSet接口的实现类，TreeSet可以确保集合元素处于排序状态。如果试图把一个对象添加到TreeSet时，则该对象的类必须实现Comparable接口。TreeSet底层使用红黑树结构存储数据元素。 TreeSet两种排序方法：自然排序和定制排序。默认情况下，TreeSet采用自然排序。 Map接口Map与Collection并列存在。用于保存具有映射关系的数据:key-valueMap中的key和value都可以是任何引用类型的数据Map中的key用Set来存放，不允许重复，即同一个Map对象所对应的类，须重写hashCode()和equals()方法 HashMap： 无序，元素不可重复，线程不安全。它是Map接口使用频率最高的实现类。允许使用null键和null值，与HashSet一样，不保证映射的顺序。HashMap判断两个key相等的标准是：两个key通过equals() 方法返回true， hashCode值也相等。HashMap判断两个value相等的标准是：两个value通过equals()方法返回true。HashMap可以使用null值为key或value LinkedHashMap： 有序，元素不可重复，线程不安全。它是 HashMap的子类，在HashMap存储结构的基础上，使用了一对双向链表来记录添加元素的顺序。 该链表负责维护Map的迭代顺序，与插入顺序一致，因此性能比HashMap低，但在迭代访问Map里的全部元素时有较好的性能。 TreeMap： 有序（非输入顺序），元素不可重复，线程不安全。它是根据key-value对进行排序。TreeMap可以保证所有的Key-Value对处于有序状态。底层采用红黑树的数据结构。TreeMap也有两种排序方式，自然排序和定制排序。 Hashtable： 无序，元素不可重复，线程安全。它不允许使用null作为key和value。Hashtable实现原理和HashMap相同，底层都使用哈希表结构，查询速度快。Hashtable和HashMap一样也不能保证其中Key-Value对的顺序。 Properties： 无序，元素不可重复，线程安全。它是Hashtable 的子类，该对象用于处理属性文件。由于属性文件里的key、value都是字符串类型，所以Properties里的key和value都是字符串类型 。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Java 线程","slug":"java_1-1","date":"2019-04-11T02:13:32.000Z","updated":"2020-12-16T02:12:55.248Z","comments":true,"path":"2019/04/11/java_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2019/04/11/java_1-1/","excerpt":"","text":"进程与线程进程: 是程序的一次动态执行过程,经历代码加载，代码执行到执行完毕的一个完整的过程。多进程操作系统能同时达运行多个进程，由于 CPU 具备分时机制，所以每个进程都能循环获得自己的CPU 时间片。由于 CPU 执行速度非常快，使得所有程序好像是在同时运行一样。线程: 是进程在执行过程中产生的多个更小的程序单元，这些更小的单元称为线程，这些线程可以同时存在，同时运行，一个进程可能包含多个同时执行的线程。进程和线程一样，都是实现并发的一个基本单位。线程是比进程更小的执行单位，线程是进程的基础之上进行进一步的划分。 Java 中线程实现Java 中实现多线程有三种手段，一种是继承 Thread 类，一种就是实现 Runnable 接口， 一种就是实现 Callable 接口 继承 Thread 类方式1234567891011121314// 继承Thread类，作为线程的实现类class ThreadExtd extends Thread&#123; private String name ; // 表示线程的名称 public ThreadExtd(String name)&#123; this.name = name ; // 通过构造方法配置name属性 &#125; @Override public void run()&#123; // 覆写run()方法，作为线程 的操作主体 for(int i=0;i&lt;10;i++)&#123; System.out.println(name + \"运行，i = \" + i) ; &#125; &#125;&#125; 实现 Runnable 接口方式123456789101112131415161718public interface Runnable &#123; public abstract void run();&#125;// 实现Runnable接口，作为线程的实现类class RunnableImpl implements Runnable&#123; private String name ; // 表示线程的名称 public RunnableImpl(String name)&#123; this.name = name ; // 通过构造方法配置name属性 &#125; @Override public void run()&#123; // 覆写run()方法，作为线程 的操作主体 for(int i=0;i&lt;10;i++)&#123; System.out.println(name + \"运行，i = \" + i) ; &#125; &#125;&#125; 实现 Callable 接口方式12345678910111213141516171819202122public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125;// 实现 Callable 接口，作为线程的实现类 java.util.concurrent.Callablepublic class CallableImpl implements Callable&lt;String&gt; &#123; private String name ; // 表示线程的名称 public CallableImpl(String name)&#123; this.name = name ; // 通过构造方法配置name属性 &#125; @Override public String call() throws Exception &#123; // 覆写 call() 方法，作为线程 的操作主体 for(int i=0;i&lt;10;i++)&#123; System.out.println(name + \"运行，i = \" + i) ; &#125; return this.name; &#125;&#125; 三者区别与联系 1234567891011121314151617181920212223242526272829303132// FutureTask类实现了RunnableFuture接口，RunnableFuture继承了Future,实现了Runnablepublic class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123;&#125;public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125;public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 调用 Runnable 实现类 Runnable rallable = new RunnableImpl(\"runnableImpl\"); new Thread(rallable).start(); // 调用 Thread 继承类 Thread thread = new ThreadExtd(\"threadExtd\"); thread.start() // 调用 Callable 实现类 Callable&lt;String&gt; callable = new CallableImpl(\"callableImpl\"); // Callable 实现类 线程运行方式一：通过Thread包装来直接执行 FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(callable); // 创建线程 new Thread(task).start(); // 调用get()阻塞主线程，反之，线程不会阻塞 String result1 = task.get(); // Callable 实现类 线程运行方式二：利用ExecutorService的submit方法 ExecutorService executorService = Executors.newFixedThreadPool(10); Future&lt;String&gt; future = executorService.submit(callable); String result2 = future.get();&#125; Callable 和 Runnable 的实现方式是实现其接口，支持多继承，但基本上用不到Thread的实现方式是继承其类Thread实现了Runnable接口并进行了扩展，Thread和Runnable的实质是继承关系，没有可比性。无论使用Runnable还是Thread，都需要new Thread，然后执行 start 方法。用法上，如果有复杂的线程操作需求，那就选择继承Thread，如果只是简单的执行一个任务，那就实现runnable。实现Callable接口的任务线程能返回执行结果，Callable接口的call()方法允许抛出异常，而Runnable接口的run()方法的异常只能在内部消化，不能继续上抛。 Callable接口支持返回执行结果，此时需要调用FutureTask.get()方法实现，此方法会阻塞主线程直到获取结果，当不调用此方法时，主线程不会阻塞。 Java 中线程 状态变化创建状态创建了一个线程对象后，新的线程对象便处于新建状态, 此时它已经有了相应的内存空间和其他资源1Thread thread=new Thread(); 就绪状态调用该线程的 start() 方法就可以启动线程。当线程启动时，线程进入就绪状态。此时，线程将进入线程队列排队，等待 CPU 服务，这表明它已经具备了运行条件1thread.start(); 运行状态线程队列中线程获得CPU资源时，线程就进入了运行状态, 此时，自动调用该线程对象的 run() 方法 阻塞状态处于运行状态的线程在某些特殊情况下，如被人为挂起或需要执行耗时的输入/输出操作，会让 CPU 暂时中止自己的执行，此时，进入阻塞状态。或者，在运行状态下，调用了sleep(),suspend()(过时弃用),wait() 等方法，线程都将进入阻塞状态，发生阻塞时线程不能进入排队队列，只有当引起阻塞的原因被消除后，线程才可以转入就绪状态 死亡状态线程调用 stop() 方法时或 run() 方法执行结束后，即处于死亡状态 Java 中线程常用方法thread.sleep()线程休眠: 线程暂缓执行，进入阻塞状态，等到预计时间再执行。线程休眠会交出CPU，让CPU去执行其他的任务。但是有一点要非常注意，sleep方法不会释放锁，也就是说如果当前线程持有对某个对象的锁，则即使调用sleep方法，其他线程也无法访问这个对象 thread.join()等待线程终止: 指在主线程中调用该方法时就会让主线程休眠，进入阻塞状态，让调用join()方法的线程先执行完毕后再开始执行主线程。 thread.yield()线程让步: 暂停当前正在执行的线程对象，并执行其它线程交出cpu, 但不释放锁，不进入阻塞状态，直接进入就绪状态 thread.interrupt()设置中断标志: 只是改变中断状态而已，它不会中断一个正在运行的线程。具体来说就是，调用interrupt()方法只会给线程设置一个为true的中断标志，而设置之后，则根据线程当前状态进行不同的后续操作1、如果线程的当前状态出于非阻塞状态，那么仅仅将线程的中断标志设置为true而已;2、如果线程的当前状态出于阻塞状态，那么将在中断标志设置为true后，还会出现wait()、sleep()、join()方法之一引起的阻塞，那么会将线程的中断标志位重新设置为false，并抛出一个InterruptedException异常。3、如果在中断时，线程正处于非阻塞状态，则将中断标志修改为true，而在此基础上，一旦进入阻塞状态，则按照阻塞状态的情况来进行处理。例如，一个线程在运行状态时，其中断标志设置为true之后，一旦线程调用了wait()、sleep()、join()方法中的一种，立马抛出一个InterruptedException异常，且中断标志被程序自动清除，重新设置为false。调用Thread类的interrupted()方法，其本质只是设置该线程的中断标志，将中断标志设置为true，并根据线程状态决定是否抛出异常 object.wait()线程等待: 让当前正在执行的线程进入线程阻塞状态的等待状态，该方法时用来将当前线程置入“预执行队列”中，并且调用wait()方法后，该线程在wait()方法所在的代码处停止执行，直到接到一些通知或被中断为止1、wait()方法只能在同步代码块或同步方法中调用，故如果调用wait()方法时没有持有适当的锁时，就会抛出异常。2、wait()方法执行后，当前线程释放锁并且与其他线程相互竞争重新获得锁。 object.notify()线程唤醒: notify()方法要在同步代码块或同步方法中调用,用来通知那些等待该对象的对象锁的线程，对其调用wait()方法的对象发出通知让这些线程不再等待，继续执行.如果有多个线程都在等待，则由线程规划器随机挑选出一个呈wait状态的线程将其线程唤醒，继续执行该线程.注意：调用notify()方法后，当前线程并不会马上释放该对象锁，要等到执行notify()方法的线程执行完才会释放对象锁 object.notifyAll()线程唤醒: notifyAll()方法将同一对象锁的所有等待线程全部唤醒 线程状态转换关系图","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Spark RDD的Stage划分","slug":"spark_1-1","date":"2018-10-13T02:23:11.000Z","updated":"2020-12-10T01:00:21.602Z","comments":true,"path":"2018/10/13/spark_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/10/13/spark_1-1/","excerpt":"","text":"什么是RDDRDD(resilient distributed dataset) 弹性分布式数据集,RDD表示一个不可变的、可分区的、支持并行计算的元素集合（类似于 Scala 中的不可变集合），RDD可以通过 HDFS、Scala集合、RDD转换、外部的数据集（支持InputFormat）获取，并且 Spark 将 RDD 存储在内存中，可以非常高效的重复利用或者在某些计算节点故障时自动数据恢复。 RDD依赖 - lineage(血统)在对RDD应用转换操作时，产生的新 RDD 对旧 RDD 会有一种依赖关系称为 Lineage(血统).Spark应用在计算时会根据 Lineage 逆向推导出所有Stage（阶段），每一个 Stage 的分区数量决定了任务的并行度，一个 Stage 实现任务的本地计算（大数据计算时网络传输时比较耗时的. RDD 两种 Lineage 关系，宽窄依赖,v它们和Stage划分有极为紧密关系窄依赖 (Narrow Dependency): 父RDD的一个分区对应一个子RDD的分区（1:1）或者多个父RDD的分区对应一个子RDD的分区（N：1）. 宽依赖 (Wide Dependency): 父RDD的一个分区对应多个子RDD的分区（1：N）.","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"Spark","slug":"Spark","permalink":"http://luckymartinlee.github.io/tags/Spark/"}]},{"title":"RabbitMQ 基础","slug":"RabbitMQ_1-1","date":"2018-08-03T12:23:21.000Z","updated":"2020-12-27T03:03:01.349Z","comments":true,"path":"2018/08/03/RabbitMQ_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/08/03/RabbitMQ_1-1/","excerpt":"","text":"简介RabbitMQ 是一个开源的消息代理和队列服务器，用来通过普通协议在完全不同的应用之间共享数据，RabbitMQ是使用 Erlang语言来编写的，并且RabbitMQ是基于AMQP协议的。AMQP，即Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦，消息的发送者无需知道消息使用者的存在，反之亦然。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。 RabbitMQ 特点:1、开源、性能优秀、稳定性保障2、 提供可靠性消息投递模式(confirm)、返回模式(return)3、 与SpringAMQP完美的整合、API丰富4、 集群模式丰富，表达式配置，HA模式，镜像队列模型5、 保证数据不丢失的前提下做到高可靠性、可用性 RabbitMQ 架构: RabbitMQ 消息传递流程: 其中，Broker：消息队列服务进程，此进程包括两个部分：Exchange和QueueExchange：消息队列交换机，按一定的规则将消息路由转发到某个队列，对消息进行过虑。Queue：消息队列，存储消息的队列，消息到达队列并转发给指定的Producer：消息生产者，即生产方客户端，生产方客户端将消息发送Consumer：消息消费者，即消费方客户端，接收MQ转发的消息。 生产者发送消息流程：1、生产者和Broker建立TCP连接。2、生产者和Broker建立通道。3、生产者通过通道消息发送给Broker，由Exchange将消息进行转发。4、Exchange将消息转发到指定的Queue（队列） 消费者接收消息流程：1、消费者和Broker建立TCP连接2、消费者和Broker建立通道3、消费者监听指定的Queue（队列）4、当有消息到达Queue时Broker默认将消息推送给消费者。5、消费者接收到消息。6、ack回复 6种消息模型基本消息模型 一个生产者，一个消费者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129// 连接工具类public class ConnectionUtil &#123; /** * 建立与RabbitMQ的连接 * @return * @throws Exception */ public static Connection getConnection() throws Exception &#123; //定义连接工厂 ConnectionFactory factory = new ConnectionFactory(); //设置服务地址 factory.setHost(\"192.168.1.123\"); //端口 factory.setPort(5672); //设置账号信息，用户名、密码、vhost factory.setVirtualHost(\"/martin\");//设置虚拟机，一个mq服务可以设置多个虚拟机，每个虚拟机就相当于一个独立的mq factory.setUsername(\"martin\"); factory.setPassword(\"123456\"); // 通过工厂获取连接 Connection connection = factory.newConnection(); return connection; &#125;&#125;// 生产者public class Send &#123; private final static String QUEUE_NAME = \"simple_queue\"; public static void main(String[] argv) throws Exception &#123; // 1、获取到连接 Connection connection = ConnectionUtil.getConnection(); // 2、从连接中创建通道，使用通道才能完成消息相关的操作 Channel channel = connection.createChannel(); // 3、声明（创建）队列 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 4、消息内容 String message = \"Hello World!\"; // 向指定的队列中发送消息 //参数：String exchange, String routingKey, BasicProperties props, byte[] body /** * 参数明细： * 1、exchange，交换机，如果不指定将使用mq的默认交换机（设置为\"\"） * 2、routingKey，路由key，交换机根据路由key来将消息转发到指定的队列，如果使用默认交换机，routingKey设置为队列的名称 * 3、props，消息的属性 * 4、body，消息内容 */ channel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes()); System.out.println(\" [x] Sent '\" + message + \"'\"); //关闭通道和连接(资源关闭最好用try-catch-finally语句处理) channel.close(); connection.close(); &#125;&#125;// 消费者public class Recv &#123; private final static String QUEUE_NAME = \"simple_queue\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); // 声明队列 //参数：String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments /** * 参数明细 * 1、queue 队列名称 * 2、durable 是否持久化，如果持久化，mq重启后队列还在 * 3、exclusive 是否独占连接，队列只允许在该连接中访问，如果connection连接关闭队列则自动删除,如果将此参数设置true可用于临时队列的创建 * 4、autoDelete 自动删除，队列不再使用时是否自动删除此队列，如果将此参数和exclusive参数设置为true就可以实现临时队列（队列不用了就自动删除） * 5、arguments 参数，可以设置一个队列的扩展参数，比如：可设置存活时间 */ channel.queueDeclare(QUEUE_NAME, false, false, false, null); //实现消费方法 DefaultConsumer consumer = new DefaultConsumer(channel)&#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 /** * 当接收到消息后此方法将被调用 * @param consumerTag 消费者标签，用来标识消费者的，在监听队列时设置channel.basicConsume * @param envelope 信封，通过envelope * @param properties 消息属性 * @param body 消息内容 * @throws IOException */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //交换机 String exchange = envelope.getExchange(); //消息id，mq在channel中用来标识消息的id，可用于确认消息已接收 long deliveryTag = envelope.getDeliveryTag(); // body 即消息体 String msg = new String(body,\"utf-8\"); System.out.println(\" [x] received : \" + msg + \"!\"); // 手动进行ACK /* * void basicAck(long deliveryTag, boolean multiple) throws IOException; * deliveryTag:用来标识消息的id * multiple：是否批量.true:将一次性ack所有小于deliveryTag的消息。 */ channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列，第二个参数：是否自动进行消息确认。 //参数：String queue, boolean autoAck, Consumer callback /** * 参数明细： * 1、queue 队列名称 * 2、autoAck 自动回复，当消费者接收到消息后要告诉mq消息已接收，如果将此参数设置为tru表示会自动回复mq，如果设置为false要通过编程实现回复 * 3、callback，消费方法，当消费者接收到消息要执行的方法 */ // channel.basicConsume(QUEUE_NAME, true, consumer); // 监听队列，第二个参数false，手动进行ACK channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; RabbitMQ的ACK机制的两种方式：自动ACK：消息一旦被接收，消费者自动发送ACK如果消息不太重要，丢失也没有影响，那么自动ACK会比较方便手动ACK：消息接收后，不会发送ACK，需要手动调用如果消息非常重要，不容丢失。那么最好在消费完成后手动ACK，否则接收消息后就自动ACK，RabbitMQ就会把消息从队列中删除。如果此时消费者宕机，那么消息就丢失了。 竞争消费者模式 也叫work queues模式，或者工作队列模式，与入门程序相比，多了一个消费端，两个消费端共同消费同一个队列中的消息，但是一个消息只能被一个消费者获取。多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。通过 BasicQos 方法设置prefetchCount = 1, 实现能者多劳，避免一个消费者忙碌，另一个消费者空闲的情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 生产者 循环发送50条消息public class Send &#123; private final static String QUEUE_NAME = \"test_work_queue\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 循环发布任务 for (int i = 0; i &lt; 50; i++) &#123; // 消息内容 String message = \"task .. \" + i; channel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes()); System.out.println(\" [x] Sent '\" + message + \"'\"); Thread.sleep(i * 2); &#125; // 关闭通道和连接 channel.close(); connection.close(); &#125;&#125;// 消费者1，消费者2public class Recv &#123; private final static String QUEUE_NAME = \"test_work_queue\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); //创建会话通道,生产者和mq服务所有通信都在channel通道中完成 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 设置每个消费者，同一时间点只能处理一条消息，此设置只在手动ack模式下有效 channel.basicQos(1); //实现消费方法 DefaultConsumer consumer = new DefaultConsumer(channel)&#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body,\"utf-8\"); System.out.println(\" [消费者1] received : \" + msg + \"!\"); //模拟任务耗时1s try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; channel.basicAck(envelope.getDeliveryTag(), false); &#125; &#125;; // 监听队列，第二个参数：是否自动进行消息确认。 channel.basicConsume(QUEUE_NAME, false, consumer); &#125;&#125; 订阅模型分类说明下：1、一个生产者多个消费者2、每个消费者都有一个自己的队列3、生产者没有将消息直接发送给队列，而是发送给exchange(交换机、转发器)4、每个队列都需要绑定到交换机上5、生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者消费例子：注册-&gt;发邮件、发短信 X（Exchanges）：交换机一方面：接收生产者发送的消息。另一方面：知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失。 Exchange类型有以下几种：Fanout：广播，将消息交给所有绑定到交换机的队列Direct：定向，把消息交给符合指定routing key 的队列Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列Header：header模式与routing不同的地方在于，header模式取消routingkey，使用header中的 key/value（键值对）匹配队列。headers类型的Exchange不依赖于routing key与binding key的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配。在绑定Queue与Exchange时指定一组键值对；当消息发送到Exchange时，RabbitMQ会取到该消息的headers（也是一个键值对的形式），对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。 Publish/subscribe（交换机类型：Fanout，也称为广播 ）将消息交给所有绑定到交换机的队列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// 生产者public class Send &#123; private final static String EXCHANGE_NAME = \"test_fanout_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为fanout channel.exchangeDeclare(EXCHANGE_NAME, \"fanout\"); // 消息内容 String message = \"注册成功！！\"; // 发布消息到Exchange channel.basicPublish(EXCHANGE_NAME, \"\", null, message.getBytes()); System.out.println(\" [生产者] Sent '\" + message + \"'\"); channel.close(); connection.close(); &#125;&#125;// 消费者1 （注册成功发给短信服务）public class Recv &#123; private final static String QUEUE_NAME = \"fanout_exchange_queue_sms\";//短信队列 private final static String EXCHANGE_NAME = \"test_fanout_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"\"); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body ) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(\" [短信服务] received : \" + msg + \"!\"); &#125; &#125;; // 监听队列，自动返回完成 channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125;// 消费者2 （注册成功发给邮件服务）public class Recv2 &#123; private final static String QUEUE_NAME = \"fanout_exchange_queue_email\";//邮件队列 private final static String EXCHANGE_NAME = \"test_fanout_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"\"); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery( String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body ) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(\" [邮件服务] received : \" + msg + \"!\"); &#125; &#125;; // 监听队列，自动返回完成 channel.basicConsume(QUEUE_NAME, true, consumer); &#125; Routing 路由模型（交换机类型：direct）把消息交给符合指定routing key 的队列P：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。X：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列C1：消费者，其所在队列指定了需要routing key 为 error 的消息C2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// 生产者public class Send &#123; private final static String EXCHANGE_NAME = \"test_direct_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为direct channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT); // 消息内容， String message = \"注册成功！请短信回复[T]退订\"; // 发送消息，并且指定routing key 为：sms，只有短信服务能接收到消息 channel.basicPublish(EXCHANGE_NAME, \"sms\", null, message.getBytes()); System.out.println(\" [x] Sent '\" + message + \"'\"); channel.close(); connection.close(); &#125;&#125;// 消费者1public class Recv &#123; private final static String QUEUE_NAME = \"direct_exchange_queue_sms\";//短信队列 private final static String EXCHANGE_NAME = \"test_direct_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。可以指定多个 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"sms\");//指定接收发送方指定routing key为sms的消息 //channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"email\"); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(\" [短信服务] received : \" + msg + \"!\"); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125;// 消费者2public class Recv2 &#123; private final static String QUEUE_NAME = \"direct_exchange_queue_email\";//邮件队列 private final static String EXCHANGE_NAME = \"test_direct_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。可以指定多个 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"email\");//指定接收发送方指定routing key为email的消息 // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(\" [邮件服务] received : \" + msg + \"!\"); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; Topics 通配符模式（交换机类型：topics）每个消费者监听自己的队列，并且设置带统配符的routingkey,生产者将消息发给broker，由交换机根据routingkey来转发消息到指定的队列。Routingkey一般都是有一个或者多个单词组成，多个单词之间以“.”分割，例如：inform.sms通配符规则：#：匹配一个或多个词*：匹配不多不少恰好1个词 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// 生产者public class Send &#123; private final static String EXCHANGE_NAME = \"test_topic_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明exchange，指定类型为topic channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); // 消息内容 String message = \"这是一只行动迅速的橙色的兔子\"; // 发送消息，并且指定routing key为：quick.orange.rabbit channel.basicPublish(EXCHANGE_NAME, \"quick.orange.rabbit\", null, message.getBytes()); System.out.println(\" [动物描述：] Sent '\" + message + \"'\"); channel.close(); connection.close(); &#125;&#125;// 消费者1public class Recv &#123; private final static String QUEUE_NAME = \"topic_exchange_queue_Q1\"; private final static String EXCHANGE_NAME = \"test_topic_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。订阅所有的橙色动物 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"*.orange.*\"); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(\" [消费者1] received : \" + msg + \"!\"); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125;// 消费者2public class Recv2 &#123; private final static String QUEUE_NAME = \"topic_exchange_queue_Q2\"; private final static String EXCHANGE_NAME = \"test_topic_exchange\"; public static void main(String[] argv) throws Exception &#123; // 获取到连接 Connection connection = ConnectionUtil.getConnection(); // 获取通道 Channel channel = connection.createChannel(); // 声明队列 channel.queueDeclare(QUEUE_NAME, false, false, false, null); // 绑定队列到交换机，同时指定需要订阅的routing key。订阅关于兔子以及懒惰动物的消息 channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"*.*.rabbit\"); channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, \"lazy.＃\"); // 定义队列的消费者 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; // 获取消息，并且处理，这个方法类似事件监听，如果有消息的时候，会被自动调用 @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // body 即消息体 String msg = new String(body); System.out.println(\" [消费者2] received : \" + msg + \"!\"); &#125; &#125;; // 监听队列，自动ACK channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; RPC 模式流程说明：1、 当客户端启动的时候，它创建一个匿名独享的回调队列。2、 在 RPC 请求中，客户端发送带有两个属性的消息：一个是设置回调队列的 reply_to 属性，另一个是设置唯一值的 correlation_id 属性。3、 将请求发送到一个 rpc_queue 队列中。4、 服务器等待请求发送到这个队列中来。当请求出现的时候，它执行他的工作并且将带有执行结果的消息发送给 reply_to 字段指定的队列。5、 客户端等待回调队列里的数据。当有消息出现的时候，它会检查 correlation_id 属性。如果此属性的值与请求匹配，将它返回给应用。","categories":[],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://luckymartinlee.github.io/tags/RabbitMQ/"}]},{"title":"Elasticsearch从入门到放弃(三) -- 查询一","slug":"elasticsearch_1-4","date":"2018-07-11T07:11:21.000Z","updated":"2020-12-17T09:58:43.105Z","comments":true,"path":"2018/07/11/elasticsearch_1-4/","link":"","permalink":"http://luckymartinlee.github.io/2018/07/11/elasticsearch_1-4/","excerpt":"","text":"查询方式1 使用Search Lite API，并将所有的搜索参数都通过URL传递12345// 下面是在所有的字段中搜索带有\"John\"的结果，相当于指定 _all 字段curl -XGET 'localhost:9200/megacorp/employee/_search?q=John'// 指定地段查询， 如指定 interests 字段curl -XGET 'localhost:9200/megacorp/employee/_search?q=interests:music' 2 使用Elasticsearch DSL，其可以通过传递一个JSON请求来获取结果。DSL方式提供了更加灵活的方式来构建更加复杂的查询（我们将在后面看到），甚至指定你想要的返回结果12345678910// 等价于上面的 在所有的字段中搜索带有\"John\"的结果curl -XGET 'localhost:9200/megacorp/_search' -d '&#123; \"query\": &#123; \"multi_match\" : &#123; \"query\" : \"John\", \"fields\" : [\"_all\"] &#125; &#125;&#125;' 基本查询term查询和terms查询不进行分词查询，直接去倒排索引中匹配确切的term。term:查询某个字段里含有某个关键词的文档terms:查询某个字段里含有多个关键词的文档，关键词之间是或关系123456789get /lib3/user/_search/&#123; \"query\":&#123;\"term\":&#123; \"interests\":\"youyong\"&#125;&#125;&#125; get lib3/user/_search/&#123; \"query\":&#123;\"terms\":&#123;\"interests\":[\"shufa\",\"youyong\"]&#125;&#125;&#125; match查询对查询关键字进行查询分词match:先进行分词操作，然后再查询 match_all:查询所有文档 match_phrase:短语匹配查询，可以指定slop分词间隔多远。相隔多远的意思是，你需要移动一个词条多少次来让查询和文档匹配。 match_phrase_prefix: 前缀查询，根据短语中最后一个分词来做前缀匹配，注意和max_expanions搭配。其实默认是50 multi_match：多字段查询，使用相当的灵活，可以完成match_phrase和match_phrase_prefix的工作。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778GET lib3/user/_search&#123; \"query\":&#123;\"match\":&#123;\"age\": 20&#125;&#125;&#125;// match_all的值为空，表示没有查询条件，那就是查询全部。就像select * from table_name一样GET lib3/user/_search&#123; \"query\":&#123; \"match_all\": &#123;&#125; &#125;&#125; get lib3/user/_search&#123; \"query\":&#123; \"match_phrase\":&#123;\"interests\": \"youyong shufa\",\"slop\": 2&#125; &#125;&#125;get lib3/user/_search&#123; \"query\":&#123; \"match_phrase_prefix\":&#123;\"interests\": \"you\",\"max_expansions\": 1&#125; &#125;&#125;GET lib3/user/_search&#123; \"query\":&#123; \"multi_match\": &#123; \"query\": \"youyong\", \"fields\":[\"interests\",\"name\"] &#125; &#125;&#125;// multi_match 实现 match_phrase_prefix 功能GET lib3/user/_search&#123; \"query\": &#123; \"multi_match\": &#123; \"query\": \"gi\", \"fields\": [\"title\"], \"type\": \"phrase_prefix\" &#125; &#125;&#125;// multi_match 实现 match_phrase功能GET lib3/user/_search&#123; \"query\": &#123; \"multi_match\": &#123; \"query\": \"girl\", \"fields\": [\"title\"], \"type\": \"phrase\" &#125; &#125;&#125;GET lib3/user/_search&#123; \"multi_match\" : &#123; \"query\" : \"北京天安门\", // type 默认是best_field，词条匹配度越高，得分越高，此外还有 // most_fields, 词条命中数量越多，得分越高 // cross_fields, 字段命中的越多，得分越高 \"type\" : \"best_fields\", // 词条之间 逻辑关系，默认是 or \"operator\": \"and\", // \"tie_breaker\" : 0.3, \"fields\" : [ \"title\", \"body\" ], \"minimun_should_match\" : \"30%\" // 至少要有30%的词条被搜索到，才命中,对most_fields 无效 &#125;&#125; Boosting 字段权重调整如搜索请求在多个field中查询，想提高某个field的查询权重,下面的例子中，我们把interests的权重调成3，这样就提高了其在结果中的权重。Boosting不仅仅意味着计算出来的分数(calculated score)直接乘以boost factor，最终的boost value会经过归一化以及其他一些内部的优化。123456789curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"multi_match\" : &#123; \"query\" : \"rock\", \"fields\": [\"about\", \"interests^3\"] &#125; &#125;&#125;' Bool Query 布尔查询布尔查询可以接受一个must参数(等价于AND)，一个must_not参数(等价于NOT)，以及一个should参数(等价于OR)。比如，我想查询about中出现music或者climb关键字的员工，员工的名字是John，但姓氏不是smith，我们可以这么来查询：1234567891011121314151617181920curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"bool\": &#123; \"must\": &#123; \"bool\" : &#123; \"should\": [ &#123; \"match\": &#123; \"about\": \"music\" &#125;&#125;, &#123; \"match\": &#123; \"about\": \"climb\" &#125;&#125; ] &#125; &#125;, \"must\": &#123; \"match\": &#123; \"first_nale\": \"John\" &#125; &#125;, \"must_not\": &#123; \"match\": &#123;\"last_name\": \"Smith\" &#125; &#125; &#125; &#125;&#125;' Fuzzy Queries 模糊查询模糊查询可以在Match和 Multi-Match查询中使用以便解决拼写的错误，模糊度是基于 Levenshtein Edit Distance 计算与原单词的距离。使用如下：123456789101112curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"multi_match\" : &#123; \"query\" : \"rock climb\", \"fields\": [\"about\", \"interests\"], \"fuzziness\": \"AUTO\" &#125; &#125;, \"_source\": [\"about\", \"interests\", \"first_name\"], \"size\": 1&#125;' fuzziness 参数的取值如下:1) 0,1,2表示最大可允许的莱文斯坦距离 2) AUTO会根据词项的长度来产生可编辑距离，它还有两个可选参数，形式为AUTO:[low],[high]， 分别表示短距离参数和长距离参数；如果没有指定，默认值是 AUTO:3,6 表示的意义如下 2.1) 0..2 单词长度为 0 到 2 之间时必须要精确匹配，这其实很好理解，单词长度太短是没有相似度可言的，例如 &apos;a&apos; 和 &apos;b&apos;。 2.2) 3..5 单词长度 3 到 5 个字母时，最大编辑距离为 1 2.3) &gt;5 单词长度大于 5 个字母时，最大编辑距离为 2 如果不设置 fuziness 参数，查询是精确匹配的。fuzziness 在绝大多数场合都应该设置成 AUTO Wildcard Query 通配符查询通配符查询允许我们指定一个模式来匹配，而不需要指定完整的trem。? 将会匹配如何字符； 将会匹配零个或者多个字符。如想查找所有名字中以J字符开始的记录，我们可以如下使用：1234567891011121314curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"wildcard\" : &#123; \"first_name\" : \"s*\" &#125; &#125;, \"_source\": [\"first_name\", \"last_name\"], \"highlight\": &#123; \"fields\" : &#123; \"first_name\" : &#123;&#125; &#125; &#125;&#125;' Regexp Query 正则表达式查询如查找作者名字以J字符开头，中间是若干个a-z之间的字符，并且以字符n结束的记录，可以如下查询：1234567891011121314curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"regexp\" : &#123; \"first_name\" : \"J[a-z]*n\" &#125; &#125;, \"_source\": [\"first_name\", \"age\"], \"highlight\": &#123; \"fields\" : &#123; \"first_name\" : &#123;&#125; &#125; &#125;&#125;' Match Phrase Query 匹配短语查询匹配短语查询要求查询字符串中的trems要么都出现Document中、要么trems按照输入顺序依次出现在结果中。在默认情况下，查询输入的trems必须在搜索字符串紧挨着出现，否则将查询不到。不过我们可以指定slop参数，来控制输入的trems之间，通过最多几次转换能够搜索到12345678910111213141516171819curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"multi_match\": &#123; \"query\": \"climb rock\", \"fields\": [ \"about\", \"interests\" ], \"type\": \"phrase\", \"slop\": 3 &#125; &#125;, \"_source\": [ \"title\", \"about\", \"interests\" ]&#125;' Match Phrase Prefix Query 匹配短语前缀查询匹配短语前缀查询可以指定单词的一部分字符前缀即可查询到该单词，和match phrase query一样我们也可以指定slop参数；同时其还支持max_expansions参数限制被匹配到的terms数量来减少资源的使用1234567891011121314151617curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"match_phrase_prefix\": &#123; \"summary\": &#123; \"query\": \"cli ro\", \"slop\": 3, \"max_expansions\": 10 &#125; &#125; &#125;, \"_source\": [ \"about\", \"interests\", \"first_name\" ]&#125;' Query Stringquery_string查询提供了一种手段可以使用一种简洁的方式运行multi_match queries, bool queries, boosting, fuzzy matching, wildcards, regexp以及range queries的组合查询。在下面的例子中，我们运行了一个模糊搜索(fuzzy search)，搜索关键字是search algorithm，并且作者包含grant ingersoll或者tom morton。并且搜索了所有的字段，其中summary字段的权重为2123456789101112131415curl -XGET 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"query_string\" : &#123; \"query\": \"(saerch~1 algorithm~1) AND (grant ingersoll) OR (tom morton)\", \"fields\": [\"_all\", \"summary^2\"] &#125; &#125;, \"_source\": [ \"title\", \"summary\", \"authors\" ], \"highlight\": &#123; \"fields\" : &#123; \"summary\" : &#123;&#125; &#125; &#125;&#125;' Simple Query String 简单查询字符串simple_query_string是query_string的另一种版本，其更适合为用户提供一个搜索框中，因为其使用+/|/- 分别替换AND/OR/NOT，如果用输入了错误的查询，其直接忽略这种情况而不是抛出异常。使用如下：123456789101112131415curl -POST 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"simple_query_string\" : &#123; \"query\": \"(saerch~1 algorithm~1) + (grant ingersoll) | (tom morton)\", \"fields\": [\"_all\", \"summary^2\"] &#125; &#125;, \"_source\": [ \"title\", \"summary\", \"authors\" ], \"highlight\": &#123; \"fields\" : &#123; \"summary\" : &#123;&#125; &#125; &#125;&#125;' Sorted 结果排序对输出结果按照多层进行排序12345678910111213curl -XPOST 'localhost:9200/megacorp/employee/_search' -d '&#123; \"query\": &#123; \"term\" : &#123; \"interests\": \"music\" &#125; &#125;, \"_source\" : [\"interests\",\"first_name\",\"about\"], \"sort\": [ &#123; \"publish_date\": &#123;\"order\":\"desc\"&#125;&#125;, &#123; \"id\": &#123; \"order\": \"desc\" &#125;&#125; ]&#125;' Range Query 范围查询123456789101112curl -XPOST 'localhost:9200/person/worker/_search?pretty' -d '&#123; \"query\": &#123; \"range\" : &#123; \"birthday\": &#123; \"gte\": \"2017-02-01\", \"lte\": \"2017-05-01\" &#125; &#125; &#125;, \"_source\" : [\"first_name\",\"last_name\",\"birthday\"]&#125;' Filtered Query 过滤查询123456789101112131415161718192021curl -XPOST :9200/megacorp/employee/_search?pretty' -d '&#123; \"query\": &#123; \"filtered\": &#123; \"query\" : &#123; \"multi_match\": &#123; \"query\": \"music\", \"fields\": [\"about\",\"interests\"] &#125; &#125;, \"filter\": &#123; \"range\" : &#123; \"birthday\": &#123; \"gte\": 2017-02-01 &#125; &#125; &#125; &#125; &#125;, \"_source\" : [\"first_name\",\"last_name\",\"about\", \"interests\"]&#125;' 过滤查询(Filtered queries)并不强制过滤条件中指定查询,如果没有指定查询条件，则会运行match_all查询，其将会返回index中所有文档，然后对其进行过滤，在实际运用中，过滤器应该先被执行，这样可以减少需要查询的范围，而且，第一次使用fliter之后其将会被缓存，这样会对性能代理提升。Filtered queries在Elasticsearch 5.0中移除了，我们可以使用bool查询来替换他，下面是使用bool查询来实现上面一样的查询效果，返回结果一样：123456789101112131415161718192021curl -XPOST 'localhost:9200/megacorp/employee/_search?pretty' -d '&#123; \"query\": &#123; \"bool\": &#123; \"must\" : &#123; \"multi_match\": &#123; \"query\": \"music\", \"fields\": [\"about\",\"interests\"] &#125; &#125;, \"filter\": &#123; \"range\" : &#123; \"birthday\": &#123; \"gte\": 2017-02-01 &#125; &#125; &#125; &#125; &#125;, \"_source\" : [\"first_name\",\"last_name\",\"about\", \"interests\"]&#125;' Multiple Filters 多过滤器查询123456789101112131415161718192021222324252627curl -XPOST 'localhost:9200/iteblog_book_index/book/_search?pretty' -d '&#123; \"query\": &#123; \"filtered\": &#123; \"query\" : &#123; \"multi_match\": &#123; \"query\": \"elasticsearch\", \"fields\": [\"title\",\"summary\"] &#125; &#125;, \"filter\": &#123; \"bool\": &#123; \"must\": &#123; \"range\" : &#123; \"num_reviews\": &#123; \"gte\": 20 &#125; &#125; &#125;, \"must_not\": &#123; \"range\" : &#123; \"publish_date\": &#123; \"lte\": \"2014-12-31\" &#125; &#125; &#125;, \"should\": &#123; \"term\": &#123; \"publisher\": \"oreilly\" &#125; &#125; &#125; &#125; &#125; &#125;, \"_source\" : [\"title\",\"summary\",\"publisher\", \"num_reviews\", \"publish_date\"]&#125;' Function Score: Field Value Factor 自定义得分计算在某些场景下，你可能想对某个特定字段设置一个因子(factor)，并通过这个因子计算某个文档的相关度(relevance score)。这是典型地基于文档(document)的重要性来抬高其相关性的方式。在下面例子中，我们想找到更受欢迎的图书(是通过图书的评论实现的)，并将其权重抬高，这里可以通过使用field_value_factor来实现12345678910111213141516171819curl -XPOST 'localhost:9200/iteblog_book_index/book/_search?pretty' -d '&#123; \"query\": &#123; \"function_score\": &#123; \"query\": &#123; \"multi_match\" : &#123; \"query\" : \"search engine\", \"fields\": [\"title\", \"summary\"] &#125; &#125;, \"field_value_factor\": &#123; \"field\" : \"num_reviews\", \"modifier\": \"log1p\", \"factor\" : 2 &#125; &#125; &#125;, \"_source\": [\"title\", \"summary\", \"publish_date\", \"num_reviews\"]&#125;'","categories":[],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"}]},{"title":"Hadoop从入门到放弃(四) -- YARN","slug":"hadoop-1-4","date":"2018-06-22T07:15:21.000Z","updated":"2020-12-07T12:52:05.467Z","comments":true,"path":"2018/06/22/hadoop-1-4/","link":"","permalink":"http://luckymartinlee.github.io/2018/06/22/hadoop-1-4/","excerpt":"","text":"什么是 YARNYARN 是 Hadoop2.0 以后的资源管理器，负责整个集群资源的管理和调度 基本概念ResourceManager负责: 分配和调度资源 启动并监控 ApplicationMaster 监控 NodeManager ApplicationMaster负责: 为 MapReduce 类型程序申请资源，并分配给内部任务 负责数据的切分 监控任务的执行以及容错 NodeManager负责: 管理单个节点 处理来自 ResouceManager 的命令 处理来自 ApplicationMaster 的命令","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://luckymartinlee.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"YARN","slug":"YARN","permalink":"http://luckymartinlee.github.io/tags/YARN/"}]},{"title":"Hadoop从入门到放弃(三) -- MapReduce","slug":"hadoop-1-3","date":"2018-06-12T07:15:40.000Z","updated":"2020-12-07T12:51:53.831Z","comments":true,"path":"2018/06/12/hadoop-1-3/","link":"","permalink":"http://luckymartinlee.github.io/2018/06/12/hadoop-1-3/","excerpt":"","text":"MapReduce 编程模型举个栗子：123输入一个大文件，通过Split切分后，将其分成多个分片，每个文件分片，由单独的机器去处理，这就是 Map 方法，然后，将各个机器的计算结果进行汇总并得到最终的结果，这就是 Reduce 方法。","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://luckymartinlee.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://luckymartinlee.github.io/tags/MapReduce/"}]},{"title":"MySQL -- 查询优化 一","slug":"mysql-2-1","date":"2018-05-22T10:45:50.000Z","updated":"2021-01-03T07:11:18.575Z","comments":true,"path":"2018/05/22/mysql-2-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/05/22/mysql-2-1/","excerpt":"","text":"索引建立技巧单个字段 “等于” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` = 15 这种情况下，毫无疑问，需要给字段 (f1) 加上索引。 形如:1SELECT `f2`,`f3` FROM `tabel1` WHERE `f1` = 15 此时应该创建 (f1,f2,f3) 索引，此索引起到覆盖索引的作用，效率比 (f1) 索引高。切记，不应该创建 (f2,f3,f1) 索引，因为根据索引 “最左原则”，它对 f1 字段起不到查询过滤作用。 多个字段 “等于” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` = 15 AND `f2` = \"abc\" 这种情况下，应该创建 (f1,f2)索引 或者 (f2,f1)索引 都是可以的。有人会问，如果创建两个单独是索引，分别是 (f1)索引 和 (f2)索引 可以吗？这里不建议这么做，虽然MySQL根据index_merge算法能同时使用这两个索引，但这样效率依旧不如上面联合索引。 字段 “等于” 和 “不等于” 混合查询形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f2` = \"abc\" 对于这种情况，我们要小心处理，因为只要有一列使用了不等于计算，那么它将阻止其他列使用索引。此时我们应该创建 (f2,f1) 索引，这时候f1和f2两个条件都会走索引，这才是我们想要的。而不是 (f1,f2) 索引，这种情况下，只有 f1 会使用索引，相对来说效率较低。 形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f3` &lt; 100 AND `f2` = \"abc\" 这是有两个 “不等于” 查询，因此我们不可能做到 f1,f2,f3都做到被索引覆盖，此时需要依据实际数据情况，建立 (f2,f1)索引 或 (f2,f3)索引，其中关键是，一定要把 “等于” 字段，放在索引的最左侧。 多个字段 “等于” 和 “排序” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` = 15 AND `f2` = \"abc\" ORDER BY `f3` 此时我们建立索引的字段顺序，应该是: “先是过滤字段，后是排序字段”，所以此处应该建立 (f1,f2,f3)索引 或者 (f2,f1,f3)索引。而不应该 建立 (f3,f1,f2)索引 或者 (f3,f2,f1)索引，因为这些只使用了索引排序，没有使用索引过滤。 形如:1SELECT `f4`,`f5` FROM `tabel1` WHERE `f1` = 15 AND `f2` = \"abc\" ORDER BY `f3` 此时我们可以创建 (f1,f2,f3,f4,f5)索引 或 (f2,f1,f3,f4,f5)索引，起到了 过滤，排序和覆盖 三个作用。 字段 “不等于” 和 “排序” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f2` = \"abc\" ORDER BY `f3` 此时，需要根据实际数据情况，选择建立 (f2,f1)索引 或 (f2,f3)索引 形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 ORDER BY `f3` 此时，只可能一个字段使用到索引，要么使用 (f1)索引，要么使用 (f2)索引，这要依据集体的数据情况，一般情况下会使用过滤索引，也就是 (f1)索引。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]},{"title":"Hadoop从入门到放弃(二) -- HDFS","slug":"hadoop-1-2","date":"2018-05-19T07:15:45.000Z","updated":"2020-12-07T12:52:08.189Z","comments":true,"path":"2018/05/19/hadoop-1-2/","link":"","permalink":"http://luckymartinlee.github.io/2018/05/19/hadoop-1-2/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Git 分支策略","slug":"git-1-1","date":"2018-04-28T02:46:43.000Z","updated":"2018-05-19T02:45:22.000Z","comments":true,"path":"2018/04/28/git-1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/04/28/git-1-1/","excerpt":"","text":"常驻分支（主分支）12master 分支develop 分支 master 分支即是 Git 默认主分支，只用来发布重大版本，是生产环境出于准备就绪状态的最新源码分支。需要对此分支进行严格的控制，可以为每次 master 分支的提交都挂一个钩子脚本，向生产环境自动化构建并发布我们的软件产品。 develop 分支develop 分支作为日常开发分支，可以理解为准备下一次发布的，开发人员最后一次提交的源码分支，这个分支也叫做 集成分支，此分支也作为每日构建（nightly build）自动化任务的源码分支 临时分支（支持型分支）123feature 功能开发分支（也叫 topic 分支）release 预发布分支hotfix 修补 issue 分支 这些分支是为了准备发布新产品，开发新的功能特性，快速或者紧急修复上线等任务而设立的分支，这些分支都是临时的，使命完成后，都应该删除。 release 分支123派生自：develop 分支需要合并回：develop 或者 master 分支分支命名规范：release-版本号 release 分支派生自 develop 分支。假设，当前的生产环境发布的版本（ mster 分支）是 1.1，我们确定新的版本号为 1.2 。通过下面命令派生一个新的 release 分支并以新的版本号为其命名：12$ git checkout -b release-1.2 develop$ git commit -a -m \"Bumped version number to 1.2\" 这个新的 release 分支，从创建到发布出去会存在一段时间，在此期间，可能会有issue修复（bug 修复直接在 release 分支上进行）分支，完成后并入 develop 分支，并放入下一次发布。release 分支真正发布成功后，还有下面的事要做：1234567891011// release 分支合并到master$ git checkout master$ git merge --no-ff release-1.2// 在 master 分支上的打一个 tag，作为标签以便作为版本历史的参考$ git tag -a 1.2// release 分支产生的改动合并回 develop，以便后续的发布同样包含对这些 bug 的修复$ git checkout develop// -no-ff 标记使得合并操作总是产生一次新的提交，避免所有提交的历史信息混在一起$ git merge --no-ff release-1.2// 至此 release 分支使命已经完成，应该删除它$ git branch -d release-1.2 feature 分支123派生自：develop 分支需要合并回：develop分支命名规范：feature-功能特性编号 feature 分支是用来开发即将发布的新的功能特性。feature 分支的生命周期会和新功能特性的开发周期保持同步，但是最终会合并回 develop 分支或被抛弃(功能特性不需要了)。feature 分支通常仅存在于开发者的代码库中，并不出现在 origin 里。123456789// 从 develop 派生出 feature 分支$ git checkout -b feature-12345 develop...// 功能开发完成后，合并回 develop 分支$ git checkout develop$ git merge --no-ff myfeature// feature 分支使命完成，应该删除$ git branch -d myfeature$ git push origin develop hotfix 分支123派生自：master 分支需要合并回：develop 和 master分支命名规范：hotfix-issue编号 hotfix 分支 是在实时的生产环境版本出现意外需要快速响应时，从 master 分支相应的 tag 被派生出来。这样做的原因，是为了让团队其中一个人来快速修复生产环境的问题，其他成员可以按工作计划继续工作下去而不受太大影响。假设，当前 master 版本是1.2 ，生产环境出现了较严重的 issue (假设，记录 bug 编号为 10002)，此时就要从 master 分支派生一个 hotfix 分支：` bash// 从 master 派生出 hotfix 分支$ git checkout -b hotfix-10002 master…// 修复 bug，提交代码$ git commit -m “Fixed severe production problem”// bug 修复完成后，hotfix 分支需要并回 master 和 develop 分支，以保证接下来的发布也都已经解决了这个 bug$ git checkout master$ git merge –no-ff hotfix-10002// 变更小版本号$ git tag -a 1.2.1$ git checkout develop$ git merge –no-ff hotfix-10002// hotfix 分支使命完成，应该删除$ git branch -d myfeature 下图形象的总结了以上分支之间的派生关系 本文是阅读了A successful Git branching model之后的自我总结。","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://luckymartinlee.github.io/tags/Git/"}]},{"title":"Hadoop从入门到放弃(一) -- 基础概念","slug":"hadoop_1-1","date":"2018-03-09T02:47:04.000Z","updated":"2020-12-07T12:51:03.015Z","comments":true,"path":"2018/03/09/hadoop_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/03/09/hadoop_1-1/","excerpt":"","text":"Hadoop 是什么Hadoop 是一个开源的大数据框架，一个分布式计算的解决方案。1Hadoop = HDFS(分布式文件系统) + MapReduce(分布式计算) HDFS 分布式文件系统: 海量存储是大数据的技术的基础MapReduce 编程模型: 分布式计算是大数据应用的解决方案 HDFS 概念数据块HDFS 上面的文件是按照数据块为单元来存储的，其默认大小为 64MB , 我们可以依据自己的情况进行设置，一般我们设置为 128MB, 备份数是3，也是可以修改的。数据块的大小设置，如设置的太小，小文件也会被切割成多个数据块，访问的时候就要查找多个数据块地址，效率比较低，同时 NameNode 存储了太多的数据块信息，对内存消耗比较多，内存压力大。如果数据块设置过大，就会降低数据并行操作的效率，同时如果系统重启，数据块越大，系统重启的时间就越长使用数据块存储的好处有： 屏蔽了文件的概念，无论 200KB 还是 200PB 的文件都是按照数据块进行存储，简化了存储系统的设计 数据块方便数据备份，提高数据容错能力 NameNodeNameNode 相当于 master - slave 体系中的 master , 他的职责有： 管理文件系统的命名空间 存放文件元数据 维护文件系统的所有文件和目录 维护文件与数据块的映射 记录每个文件的各个块所在数据节点 DataNode 的信息 DataNodeDataNode 是 HDFS 文件系统的工作节点，负责存储和检索数据块，向 NameNode 更新所存储块的列表 HDFS 优/缺点优点： 适合大文件存储，支持 TB、PB 级别数据存储，支持副本策略 HDFS 可以构建在廉价的普通机器上，具备容错和恢复机制 支持流逝数据访问，一次写入多次读取效率高缺点: 不适合大量小文件存储 不适合并发写入，不支持文件随机修改 不适合随机度等低延时的访问方式 HDFS 写流程1234561. Client 向 NameNode 发出写请求，表明要将 data 写入到集群当中。2. NameNode 中存储了集群中所有 DataNode 信息，收到 Client 请求后，就将可用的 DataNode 信息发送给Client3. Client 依据收到 NameNode 的信息，先将数据进行分块，如分成两块。然后将 数据块1 和 从NameNode接收到的 DataNode所有节点信息，都发送给 DataNode-14. DataNode-1 接收到信息后，先将 数据块1 进行保存，在依据接收的DataNode节点集群信息，将 数据块1 备份到 DataNode-2 和 DataNode-3。 当 DataNode-1，DataNode-2，DataNode-3 完成 数据块1 存储之后，反馈给 NameNode5. NameNode 收到 DataNode 发来的反馈信息后，更新自己的 DataNode 元数据信息列表。然后告诉 Client 数据块1 已经存储好了，可以存储后面的数据块了6. Client 收到 NameNode 信息后，开始重复 数据块1的存储步骤，存储 数据块2，至此 HDFS 写数据流程结束 HDFS 写流程1231. Client 向 NameNode 发出读请求，表明要从集群中读取文件 data。2. NameNode 收到 Client 请求后，就将存储了 data 文件数据块的DataNode 节点信息发送给 Client，如上图，DataNode-1 存储了 数据块1，DataNode-2 存储了 数据块2，DataNode-3 存储了 数据块1 和 数据块23. Client 依据收到 NameNode 的信息，先从 DataNode-1 读取 数据块1 ，然后再从 DataNode-2 读取 数据块2，如果 DataNode-2 宕机，Client 就会向 DataNode-3 读取 数据块2， 至此 HDFS 读数据流程结束 HDFS 常用命令 类Linux系统命令: ls, cat, mkdir, rm, chomd, chown, … HDFS文件交互命令: copyFromLocal, copyToLocal, get, put","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://luckymartinlee.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"}]},{"title":"Elasticsearch从入门到放弃(三) -- 疑难配置详解","slug":"elasticsearch_1-3","date":"2018-02-01T07:41:28.000Z","updated":"2020-12-17T03:39:20.185Z","comments":true,"path":"2018/02/01/elasticsearch_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2018/02/01/elasticsearch_1-3/","excerpt":"","text":"elasticsearch.ymlnode.master: true指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master node.data: true指定该节点是否存储索引数据，默认为true。 path.data: /path/to/data设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：path.data: /path/to/data1,/path/to/data2但是我们同一个分片的数据会放在同一个路径 index.number_of_shards: 5设置默认索引分片个数，默认为5片。分片个数是索引创建后一次生成的,后续不可更改设置 index.number_of_replicas: 1设置默认索引每个分片副本个数，默认每个分片1个副本，如分片数为5，那么副本分片也为5个，总共10个分片。可以通过API去实时修改设置的。（集群健康度可用 curl ‘localhost:9200/_cat/health?v’ 查看， 分为绿色、黄色或红色。绿色代表一切正常，集群功能齐全，黄色意味着所有的数据都是可用的，但是某些副本没有被分配，红色则代表因为某些原因，某些数据不可用) bootstrap.mlockall: true设置为true来锁住内存。因为当jvm开始swapping时es的效率 会降低，所以要保证它不swap，可以把ES_MIN_MEM和ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。 同时也要允许elasticsearch的进程可以锁住内存，linux下可以通过ulimit -l unlimited命令。 network.bind_host: 192.168.0.1设置该节点绑定的ip地址，可以绑定多个ip地址，允许哪些ip可以访问这个这节，包括外部访问和es集群内节点互相访问，可以是ipv4或ipv6的，默认为0.0.0.0。network.bind_host: [“192.168.0.1”,”10.210.32.xx”]代表集群节点之间 使用 192.168.0.1 ip交换数据，同时 允许所有10.210.32.xx IP段 访问集群 network.publish_host: 192.168.0.1设置es节点之间交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。 network.host: 192.168.0.1这个参数是用来同时设置bind_host和publish_host上面两个参数，项目局域网内可以使用者一个参数代替上面两个参数配置，但如涉及到内网，外网都要访问es集群，就需要单独设置上面两个参数 transport.tcp.port: 9300设置节点间交互的tcp端口，默认是9300。 transport.tcp.compress: true设置在节点间传输数据时是否压缩，默认为false，不压缩 http.max_content_length:100mb设置请求返回内容的最大容量,默认100mb。 discovery.zen.minimum_master_nodes: 1设置在选举Master节点时需要参与的最少的候选主节点数，默认为1.这个配置就是告诉 Elasticsearch 当没有足够 master 候选节点的时候，就不要进行 master 节点选举，等 master 候选节点足够了才进行选举。如果使用默认值，则当网络不稳定时有可能会出现脑裂(一种两个主节点同时存在于一个集群的现象)。合理的数值为(master_eligible_nodes/2)+1，其中master_eligible_nodes表示集群中的候选主节点数。 discovery.zen.ping.timeout: 3s设置在集群中自动发现其他节点时ping连接的超时时间，默认为3秒。在较差的网络环境下需要设置得大一点，防止因误判该节点的存活状态而导致分片的转移。 gateway.recover_after_nodes: 8设置整个集群提供服务之前你希望有多少个节点在线。看下面的例子： 想象一下假设你有 10 个节点，每个节点只保存一个分片，这些分片有 5 个主分片和5 个副本分片的索引。有时你需要为整个集群做离线维护（比如，为了安装一个新的驱动程序）， 当你重启你的集群，恰巧出现了 5 个节点已经启动，还有 5 个还没启动的场景。假设其它 5 个节点出问题，或者他们根本没有收到立即重启的命令。不管什么原因，你有 5 个节点在线上，这五个节点会相互通信，选出一个 master，从而形成一个集群。 他们注意到数据不再均匀分布，因为有 5 个节点在集群中丢失了，所以他们之间会立即启动分片复制。最后，你的其它 5 个节点打开加入了集群。这些节点会发现 它们 的数据正在被复制到其他节点，所以他们删除本地数据（因为这份数据要么是多余的，要么是过时的）。 然后整个集群重新进行平衡，因为集群的大小已经从 5 变成了 10。在整个过程中，你的节点会消耗磁盘和网络带宽，来回移动数据，因为没有更好的办法。对于有 TB 数据的大集群, 这种无用的数据传输需要 很长时间 。如果等待所有的节点重启好了，整个集群再上线，所有的本地的数据都不需要移动。 这种情况下，我们设置为 8，这意味着至少要有 8 个节点，该集群才可用。 gateway.expected_nodes: 10设置这个集群中节点的数量，默认为2 gateway.recover_after_time: 5m设置初始化数据恢复进程的超时时间，默认是5分钟 以上三个配置参数，告诉ES 集群：等待集群至少存在 8 个节点等待 5 分钟，或者10 个节点上线后，才进行数据恢复，这取决于哪个条件先达到这样集群重启的时候避免过多的分片交换。这可能会让数据恢复从数个小时缩短为几秒钟。 discovery.zen.ping.unicast.hosts: [“host1”, “host2:port”, “host3[portX-portY]”]设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。没有任何网络配置，Elasticsearch将绑定到可用的回环地址，并将扫描端口9300到9305以尝试连接到在同一服务器上运行的其他节点。 这提供了自动群集体验，无需进行任何配置。 jvm heap size 配置一般在运行elasticsearch 的时候最小需要是内存是1G，少于1G我们会经常启动不了。-Xms1g # 最小值为1G-Xmx1g # 最大值为1G对于这个值的设置，官方为了适应不同的java版本,特做了一些适应配置-Xms1g 不受版本影响，默认8:-Xmx2g 只适应java8版本8-:-Xmx2g 适应java8及以上版本8-9:-Xmx2g 适应java8-java9版本 官方文档写到：以往经验得出， 最大值和最小值设置为一样的值，否则在系统使用的时候会因jvm值变化而导致服务暂停 过多的内存，会导致用于缓存的内存越多，最终导致回收内存的时间也加长 设置的内存不要超过物理内存的50%，以保证有足够的内存留给操作系统 不要将内存设置超过32GB","categories":[],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch从入门到放弃(二) -- 零基础环境搭建","slug":"elasticsearch_1-2","date":"2018-01-28T01:33:35.000Z","updated":"2020-12-17T03:39:21.638Z","comments":true,"path":"2018/01/28/elasticsearch_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2018/01/28/elasticsearch_1-2/","excerpt":"","text":"本文主要讲述 Linux 虚拟机环境下 Elasticsearch 5.5 版本的安装。安装 Elasticsearch 之前，请确保你的机器 Java 8 环境已经搭建好，保证环境变量 JAVA_HOME 设置正确。如你还没有安装好 JAVA 8 环境，请参考linux系统中JAVA环境搭建 下载并解压安装包123$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.0.zip$ unzip elasticsearch-5.5.0.zip$ cd elasticsearch-5.5.0/ 修改配置文件Elasticsearch 默认只允许本机访问，远程访问，需要修改 config/elasticsearch.yml,改为运行所有人访问 0.0.0.0（生产环境，切不可这样改，可以指定特定 ip 访问 Elasticsearch）1234$ vim config/elasticsearch.yml#network.host: 192.168.0.1network.host: 0.0.0.0 启动程序直接运行 bin 目录下的 elasticsearch1$ ./bin/elasticsearch 此过程中，可能会报出以下错误1max virtual memory areas vm.maxmapcount [65530] is too low 此时，切换到管理员用户，运行下面的命令即可1# sysctl -w vm.max_map_count=262144 重新运行 elasticsearch 即可，这时候访问 9200 端口，得到如下信息：1234567891011121314$ curl http://127.0.0.1:9200&#123; \"name\" : \"uA7Io-i\", # node 名称 \"cluster_name\" : \"elasticsearch\", # 集群名称 \"cluster_uuid\" : \"f7y_hJefSw-kQFN-zt-3Cw\", # 集群唯一 id \"version\" : &#123; \"number\" : \"5.5.0\", # Elasticsearch 版本号 \"build_hash\" : \"260387d\", \"build_date\" : \"2017-06-30T23:16:05.735Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" # 依赖的 Lucene 版本号 &#125;, \"tagline\" : \"You Know, for Search\"&#125; 到这里说明你的 Elasticsearch 已经安装成功了，按下 Ctrl + C，Elasticsearch 就会停止运行。 想要后台运行 Elasticsearch，输入下面命令：1$ ./bin/elasticsearch -d 此时想要停止 Elasticsearch ，想要先找到 Elasticsearch 进程，然后 kill 。12345$ ps -ef |grep elasticsearchmartin 3035 2200 0 Apr19 ? 00:04:29 /usr/bin/java -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch -server -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -Djdk.io.permissionsUseCanonicalPath=true -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j.skipJansi=true -XX:+HeapDumpOnOutOfMemoryError -Des.path.home=/home/marting/elasticsearch-5.5.0 -cp /home/martin/elasticsearch-5.5.0/lib/* org.elasticsearch.bootstrap.Elasticsearch$ kill -2 3035","categories":[],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch从入门到放弃(一) -- 基本概念","slug":"elasticsearch_1-1","date":"2018-01-26T07:23:08.000Z","updated":"2020-12-17T03:39:23.169Z","comments":true,"path":"2018/01/26/elasticsearch_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/01/26/elasticsearch_1-1/","excerpt":"","text":"相关概念通用搜索 与 垂直搜索&ensp;&ensp;通用搜索，通俗点说就是在网络上的所有可以获取的信息中进行搜索，涉及所有领域，所有展现形式，包括音频、视频、文字、图片。一句话就是只要信息内容匹配你的搜索条件，就返回给你，知名站点有 Baidu, Google, Bing.&ensp;&ensp;垂直搜索，相对于通用搜索，它是针对特定行业信息的搜索。比如，用于图标搜索的EasyIcon、小说搜索Owllook 等。 全文检索 与 倒排索引&ensp;&ensp;全文检索，概念就是通过计算机实现，你的搜索词出现在文章当中，甚至搜索词与文章有相关性，那么这篇文章就应该被搜索出来。计算机实现这个功能整个过程叫做全文检索。&ensp;&ensp;倒排索引，是实现全文检索的技术手段之一。简单的说就是把文章拆解成一个个简单的词语（此过程叫分词），将文章与这些词语建立起关联关系（即是索引）。用户查询时，也讲搜索词进行分词，再在前面建立的关联关系中查找文章，以此实现全文检索。想更进一步知道什么倒排索引的实现，请看我的另一篇文章《什么是倒排索引》 Lucene&ensp;&ensp;Lucene，是一个开源免费的成熟的Java系的信息检索程序库，全文检索引擎工具包，由Apache软件基金会支持和提供。严格的说，Lucene 不是全文检索引擎或者搜索引擎，它提供了查询组件，索引组件以及文本分析组件，它是一个全文检索引擎的框架。 Elasticsearch何为 Elasticsearch&ensp;&ensp;Elasticsearch，是一款基于Lucene的开源的高扩展的分布式全文检索引擎。Elastic 是 Lucene 的封装，提供了 RESTFull API 的操作接口，开箱即用。它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。 Elasticsearch 核心概念Near Realtime (NRT)&ensp;&ensp;近实时概念包含两层含义，一是数据从写入到可搜索到，时间达到秒级别；二是Elasticsearch 查询、聚合、分析，时间到达秒级别。 Cluster&ensp;&ensp;Elasticsearch是一个分布式的全文检索引擎，多个程序节点构成一个大的集群，集群中节点即是数据备份节点，也是数据查询分析负载均衡节点。 Node&ensp;&ensp;节点，即是一个Elasicsearch的实例，一台机器可以运行一个或多个节点。 Shard(primary shard)&ensp;&ensp;分片是为了解决海量数据存储问题，Elasticsearch 使用分片机制，将海量数据切分为多个分片存储在不同的节点上。Elasticseach分片就是它的主分片，Elasticsearch默认主分片数量是5。 replica(replica shard)&ensp;&ensp;分片副本，意思显而易见，就是分片的备份，作用是提高集群数据安全，提升系统高可用性，同时副本节点在海量数据检索时也分担检索压力，具备提升 Elasticsearch 请求吞吐量和性能作用。 Index&ensp;&ensp;这里的 Index 不是查询索引的概念，可以理解为同类型数据的库，相当于 MySQL 中的库，但又有不同，不同之处在于此处的Index中存储的都是字段类型基本一致的同类型数据。 Type&ensp;&ensp;这里的 Type ，可以理解为 MySQL 中的表，一个 Index 中可以有多个 Type，且一个 Type存储同种数据。如，一个名为 animal 的 Index 中有 bird 和 fish 两个 Type, 他们有很多共同的属性。 Document&ensp;&ensp;Document，即是 Type 中具体的文档。每个文档都有自己唯一的 id. Field&ensp;&ensp;Field，是文档的属性或者叫做字段，不同字段，类型不同，分词方式也不同。","categories":[],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"}]},{"title":"Scala 函数与方法的区分与理解","slug":"scala_1-4","date":"2017-12-11T01:33:25.000Z","updated":"2020-12-15T09:04:22.621Z","comments":true,"path":"2017/12/11/scala_1-4/","link":"","permalink":"http://luckymartinlee.github.io/2017/12/11/scala_1-4/","excerpt":"","text":"方法：12345scala&gt; def add(x:Int, y: Int) = x + yadd: (x: Int, y: Int)Intscala&gt; add(1, 2)res0: Int = 3 函数：1234567scala&gt; val add_f = (x: Int, y: Int) =&gt; x + yadd_f: (Int, Int) =&gt; Int = &lt;function2&gt;// 根据内容可以看出add_f是一个函数Functionscala&gt; add_f(1, 2)res1: Int = 3 上面 ‘=’号右边的内容 (x: Int, y: Int) =&gt; x + y是一个函数体，方法只能用 def 接收，函数可以用 def 接收，也可以用 val 接收。当函数用 def 来接收之后，不再显示为 function ，转换为方法。方法可以省略参数，函数不可以。 函数可以作为方法的参数。 看下面的例子：12345scala&gt; val a = () =&gt; 100a: () =&gt; Int = &lt;function0&gt;scala&gt; val a = =&gt; 100&lt;console&gt;:1: error: illegal start of simple expression 看这里: val a = =&gt; 100 // 当函数参数为空时报错","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"Redis 基础","slug":"redis_1-1","date":"2017-12-11T01:33:25.000Z","updated":"2020-12-27T03:51:40.463Z","comments":true,"path":"2017/12/11/redis_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2017/12/11/redis_1-1/","excerpt":"","text":"Redis 简介Redis是一个开源的、基于内存的数据结构存储器，可以用作缓存和消息中间件。它是一种高级的key:value存储系统，其中value支持五种数据类型：1.字符串（strings）2.字符串列表（lists）3.字符串集合（sets）4.有序字符串集合（sorted sets）5.哈希（hashes） 而关于key的几点建议：1.key不要太长，尽量不要超过1024字节，这不仅消耗内存，而且会降低查找的效率；2.key也不要太短，太短的话，key的可读性会降低；3.在一个项目中，key最好使用统一的命名模式，例如user:10000:passwd。 数据结构 – strings如果只使用redis中的字符串类型，且不使用redis的持久化功能，那么，redis就和memcache非常非常的像了12redis&gt; set mystr \"hello world!\" //设置字符串类型redis&gt; get mystr //读取字符串类型 因为是二进制安全的，所以你完全可以把一个图片文件的内容作为字符串来存储,还可以通过字符串类型进行数值操作12345678redis&gt; set mynum \"2\"OKredis&gt; get mynum\"2\"redis&gt; incr mynum(integer) 3redis&gt; get mynum\"3\" 由于INCR等指令本身就具有原子操作的特性，所以我们完全可以利用redis的INCR、INCRBY、DECR、DECRBY等指令来实现原子计数的效果，假如，在某种场景下有3个客户端同时读取了mynum的值（值为2），然后对其同时进行了加1的操作，那么，最后mynum的值一定是5。不少网站都利用redis的这个特性来实现业务上的统计计数需求。 数据结构 – listsRedis中的lists在底层实现上并不是数组，而是链表，也就是说对于一个具有上百万个元素的lists来说，在头部和尾部插入一个新元素，其时间复杂度是常数级别的，比如用LPUSH在10个元素的lists头部插入新元素，和在上千万元素的lists头部插入新元素的速度应该是相同的。虽然lists有这样的优势，但同样有其弊端，那就是，链表型lists的元素定位会比较慢，而数组型lists的元素定位就会快得多。lists的常用操作包括LPUSH、RPUSH、LRANGE等。我们可以用LPUSH在lists的左侧插入一个新元素，用RPUSH在lists的右侧插入一个新元素，用LRANGE命令从lists中指定一个范围来提取元素。12345678910111213141516171819//新建一个list叫做mylist，并在列表头部插入元素\"1\"redis&gt; lpush mylist \"1\" //返回当前mylist中的元素个数(integer) 1 //在mylist右侧插入元素\"2\"redis&gt; rpush mylist \"2\" (integer) 2//在mylist左侧插入元素\"0\"redis&gt; lpush mylist \"0\" (integer) 3//列出mylist中从编号0到编号1的元素redis&gt; lrange mylist 0 1 1) \"0\"2) \"1\"//列出mylist中从编号0到倒数第一个元素redis&gt; lrange mylist 0 -1 1) \"0\"2) \"1\"3) \"2\" lists的应用实例1.我们可以利用lists来实现一个消息队列，而且可以确保先后顺序，不必像MySQL那样还需要通过ORDER BY来进行排序。2.利用LRANGE还可以很方便的实现分页的功能。3.在博客系统中，每片博文的评论也可以存入一个单独的list中。 数据结构 – setsredis的集合，是一种无序的集合，集合中的元素没有先后顺序。集合相关的操作也很丰富，如添加新元素、删除已有元素、取交集、取并集、取差集等。 1234567891011121314151617181920212223242526272829//向集合myset中加入一个新元素\"one\"redis&gt; sadd myset \"one\" (integer) 1redis&gt; sadd myset \"two\"(integer) 1//列出集合myset中的所有元素redis&gt; smembers myset 1) \"one\"2) \"two\"//判断元素1是否在集合myset中，返回1表示存在redis&gt; sismember myset \"one\" (integer) 1//判断元素3是否在集合myset中，返回0表示不存在redis&gt; sismember myset \"three\" (integer) 0//新建一个新的集合yoursetredis&gt; sadd yourset \"1\" (integer) 1redis&gt; sadd yourset \"2\"(integer) 1redis&gt; smembers yourset1) \"1\"2) \"2\"//对两个集合求并集redis&gt; sunion myset yourset 1) \"1\"2) \"one\"3) \"2\"4) \"two\" 数据结构 – sorted setsredis不但提供了无需集合（sets），还很体贴的提供了有序集合（sorted sets）。有序集合中的每个元素都关联一个序号（score），这便是排序的依据。很多时候，我们都将redis中的有序集合叫做zsets，这是因为在redis中，有序集合相关的操作指令都是以z开头的，比如zrange、zadd、zrevrange、zrangebyscore等等 12345678910111213141516171819202122//新增一个有序集合myzset，并加入一个元素baidu.com，给它赋予的序号是1：redis&gt; zadd myzset 1 baidu.com (integer) 1//向myzset中新增一个元素360.com，赋予它的序号是3redis&gt; zadd myzset 3 360.com (integer) 1//向myzset中新增一个元素google.com，赋予它的序号是2redis&gt; zadd myzset 2 google.com (integer) 1//列出myzset的所有元素，同时列出其序号，可以看出myzset已经是有序的了。redis&gt; zrange myzset 0 -1 with scores 1) \"baidu.com\"2) \"1\"3) \"google.com\"4) \"2\"5) \"360.com\"6) \"3\"//只列出myzset的元素redis&gt; zrange myzset 0 -1 1) \"baidu.com\"2) \"google.com\"3) \"360.com\" 数据结构 – hashes哈希是从redis-2.0.0版本之后才有的数据结构。hashes存的是字符串和字符串值之间的映射，比如一个用户要存储其全名、姓氏、年龄等等，就很适合使用哈希。12345678910111213141516171819202122//建立哈希，并赋值redis&gt; HMSET user:001 username antirez password P1pp0 age 34 OK//列出哈希的内容redis&gt; HGETALL user:001 1) \"username\"2) \"antirez\"3) \"password\"4) \"P1pp0\"5) \"age\"6) \"34\"//更改哈希中的某一个值redis&gt; HSET user:001 password 12345 (integer) 0//再次列出哈希的内容redis&gt; HGETALL user:001 1) \"username\"2) \"antirez\"3) \"password\"4) \"12345\"5) \"age\"6) \"34\" Redis 持久化Redis提供了两种持久化的方式，分别是RDB（Redis DataBase）和AOF（Append Only File）。RDB，简而言之，就是在不同的时间点，将redis存储的数据生成快照并存储到磁盘等介质上；AOF，则是换了一个角度来实现持久化，那就是将redis执行过的所有写指令记录下来，在下次redis重新启动时，只要把这些写指令从前到后再重复执行一遍，就可以实现数据恢复了。 其实RDB和AOF两种方式也可以同时使用，在这种情况下，如果redis重启的话，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式的数据恢复完整度更高。如果你没有数据持久化的需求，也完全可以关闭RDB和AOF方式，这样的话，redis将变成一个纯内存数据库。 Redis 主从结构Redis是支持主从同步的，而且也支持一主多从以及多级从结构。主从结构，一是为了纯粹的冗余备份，二是为了提升读性能，比如很消耗性能的SORT就可以由从服务器来承担。 redis的主从同步是异步进行的，这意味着主从同步不会影响主逻辑，也不会降低redis的处理性能。 主从架构中，可以考虑关闭主服务器的数据持久化功能，只让从服务器进行持久化，这样可以提高主服务器的处理性能。 在主从架构中，从服务器通常被设置为只读模式，这样可以避免从服务器的数据被误修改。但是从服务器仍然可以接受CONFIG等指令，所以还是不应该将从服务器直接暴露到不安全的网络环境中。如果必须如此，那可以考虑给重要指令进行重命名，来避免命令被外人误执行。 Redis 事务处理MULTI、EXEC、DISCARD、WATCH 这四个指令构成了redis事务处理的基础。 1.MULTI用来组装一个事务；2.EXEC用来执行一个事务；3.DISCARD用来取消一个事务；4.WATCH用来监视一些key，一旦这些key在事务执行之前被改变，则取消事务的执行。 123456789101112131415redis&gt; MULTI //标记事务开始OKredis&gt; INCR user_id //多条命令按顺序入队QUEUEDredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; PINGQUEUEDredis&gt; EXEC //执行1) (integer) 12) (integer) 23) (integer) 34) PONG 例子中，我们看到了QUEUED的字样，这表示我们在用MULTI组装事务时，每一个命令都会进入到内存队列中缓存起来，如果出现QUEUED则表示我们这个命令成功插入了缓存队列，在将来执行EXEC时，这些被QUEUED的命令都会被组装成一个事务来执行。 如果redis开启了AOF持久化的话，那么一旦事务被成功执行，事务中的命令就会通过write命令一次性写到磁盘中去，如果在向磁盘中写的过程中恰好出现断电、硬件故障等问题，那么就可能出现只有部分命令进行了AOF持久化，这时AOF文件就会出现不完整的情况，这时，我们可以使用redis-check-aof工具来修复这一问题，这个工具会将AOF文件中不完整的信息移除，确保AOF文件完整可用。 WATCH 指令，它可以帮我们实现类似于“乐观锁”的效果，即CAS（check and set）。WATCH本身的作用是“监视key是否被改动过”，而且支持同时监视多个key，只要还没真正触发事务，WATCH都会尽职尽责的监视，一旦发现某个key被修改了，在执行EXEC时就会返回nil，表示事务无法触发。 1234567891011121314redis&gt; set age 23OKredis&gt; watch age //开始监视ageOKredis&gt; set age 24 //在EXEC之前，age的值被修改了OKredis&gt; multiOKredis&gt; set age 25QUEUEDredis&gt; get ageQUEUEDredis&gt; exec //触发EXEC(nil) //事务无法被执行","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://luckymartinlee.github.io/tags/Redis/"}]},{"title":"什么是倒排索引","slug":"inverted_index_1","date":"2017-12-03T01:06:12.000Z","updated":"2018-05-02T08:24:26.000Z","comments":true,"path":"2017/12/03/inverted_index_1/","link":"","permalink":"http://luckymartinlee.github.io/2017/12/03/inverted_index_1/","excerpt":"","text":"在搜索引擎的设计逻辑中，为每一搜索目标文件都生成一个唯一的 ID ，而文件的内容可以看成是很多提取出来的关键词的集合，提取关键词的过程叫做 “分词”。例如，文件1的 ID 是 1001，经过分词，总共提取出30个关键词，那么搜索引擎就会记录每个关键词出现在文章当中的位置和出现次数。 下面说说什么是 倒排索引，有倒排索引，相应的肯定就有正向索引。 正向索引正向索引的结构：1231001(文档1) &gt; &#123;中国(关键词1):&#123;出现次数:2;出现位置：10,15&#125;,&#123;劳动(关键词2):&#123;出现次数:3;出现位置：2,7,11&#125;,...1002(文档2) &gt; &#123;中国(关键词1):&#123;出现次数:3;出现位置：6,23,45&#125;,&#123;体育(关键词3):&#123;出现次数:1;出现位置：1&#125;,...... 如图所示: 对于 正向索引，假设，用户搜索关键词“中国”，搜索引擎就要完整遍历索引中所有信息，找到包含“中国”的文件，然后依据特定的打分排序算法，整理出文件先后顺序，再返回给用户。可以看出，正向索引明显的弊端就是，对于海量数据的搜索引擎系统，正向索引结构效率低下，无法快速响应用户。 倒排索引倒排索引的结构：1234中国(关键词1) &gt; &#123;1001(文档1):&#123;出现次数:2;出现位置：10,15&#125;,&#123;1002(文档2):&#123;出现次数:3;出现位置：6,23,45&#125;,...劳动(关键词2)&gt; &#123;1001(文档1):&#123;出现次数:3;出现位置：2,7,11&#125;,...体育(关键词3)&gt; &#123;1002(文档1):&#123;出现次数:1;出现位置：1&#125;,...... 如图所示:对于 倒排索引，同样假设，用户搜索关键词“中国”，搜索引擎就要不必完整遍历索引中所有信息，很快就能找到包含“中国”的文件，然后依据特定的打分排序算法，整理出文件先后顺序，返回给用户。可以看出，倒排索引很好解决了正向索引的弊端就是，对于海量数据的搜索引擎系统，倒排索引结构效率较高，可以较快响应用户。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://luckymartinlee.github.io/tags/搜索引擎/"}]},{"title":"Scala 闭包(closure) 深入理解","slug":"scala_1-2","date":"2017-10-30T11:24:55.000Z","updated":"2021-01-02T13:25:39.392Z","comments":true,"path":"2017/10/30/scala_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2017/10/30/scala_1-2/","excerpt":"","text":"什么是 Scala 闭包闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。如下实例：12345678scala&gt; var more =1more: Int = 1 scala&gt; val addMore = (x:Int) =&gt; x + moreaddMore: Int =&gt; Int = &lt;function1&gt;scala&gt; addMore (100)res1: Int = 101 其中，我们定义函数变量 addMore 成为一个“闭包”，因为它引用到函数外面定义的变量 more，定义这个函数的过程是将这个自由变量捕获而构成一个封闭的函数。有意思的是，当这个自由变量发生变化时，Scala 的闭包能够捕获到这个变化，因此 Scala 的闭包捕获的是变量本身而不是当时变量的值。 同样的，如果变量在闭包内发生变化，也会反映到函数外面定义的闭包的值。如下实例:12345678910scala&gt; val someNumbers = List ( -11, -10, -5, 0, 5, 10)someNumbers: List[Int] = List(-11, -10, -5, 0, 5, 10)scala&gt; var sum =0sum: Int = 0scala&gt; someNumbers.foreach ( sum += _)scala&gt; sumres4: Int = -11 上面可以看到在闭包中修改sum的值，其结果还是传递到闭包的外面。&nbsp;那如果一个闭包所访问的变量有几个不同的版本，比如一个闭包使用了一个函数的局部变量（参数），然后这个函数调用很多次，那么所定义的闭包应该使用所引用的局部变量的哪个版本呢？ 简单的说，该闭包定义所引用的变量为定义该闭包时变量的值，也就是定义闭包时相当于保存了当时程序状态的一个快照。比如我们定义下面一个函数闭包:1234567891011121314scala&gt; def makeIncreaser(more:Int) = (x:Int) =&gt; x + moremakeIncreaser: (more: Int)Int =&gt; Intscala&gt; val inc1=makeIncreaser(1)inc1: Int =&gt; Int = &lt;function1&gt;scala&gt; val inc9999=makeIncreaser(9999)inc9999: Int =&gt; Int = &lt;function1&gt;scala&gt; inc1(10)res5: Int = 11scala&gt; inc9999(10)res6: Int = 10009 当你调用makeIncreaser(1)时，你创建了一个闭包，该闭包定义时more的值为1, 而调用makeIncreaser(9999)所创建的闭包的more的值为9999。此后你也无法修改已经返回的闭包的more的值。因此inc1始终为加一，而inc9999始终为加9999.","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"Scala 柯里化(curry) 深入理解","slug":"scala_1-1","date":"2017-10-23T01:44:17.000Z","updated":"2020-12-10T07:47:23.427Z","comments":true,"path":"2017/10/23/scala_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2017/10/23/scala_1-1/","excerpt":"","text":"什么是柯里化函数有多个参数列表 的函数就是柯里化函数，所谓的参数列表就是使用小括号括起来的函数参数列表, 函数柯里化就是 把接受多个参数的函数变换成接受一个单一参数(最初函数的第一个参数)的函数，并且返回接受余下的参数且返回结果的新函数的过程。如下所示12345// 非柯里化函数def sum(x:Int,y:Int)=x+y// 柯里化函数def sum(x:Int)(y:Int) = x + y 实例如下： sum(1)(2) 实际上是依次调用两个普通函数（非柯里化函数），第一次调用使用一个参数 x，返回一个函数类型的值，第二次使用参数y调用这个函数类型的值。 实质上最先演变成这样一个方法：1def sum(x:Int)=(y:Int) =&gt; x+y 那么这个函数是什么意思呢？ 接收一个x为参数，返回一个匿名函数，该匿名函数的定义是：接收一个Int型参数y，函数体为x+y. 柯里化的意义柯里化的意义在于把多个参数的function等价转化成多个单参数function的级联，这样方便做lambda演算。 同时curry化对类型推演也有帮助，scala的类型推演是局部的，在同一个参数列表中后面的参数不能借助前面的参数类型进行推演，curry化以后，放在两个参数列表里，后面一个参数列表里的参数可以借助前面一个参数列表里的参数类型进行推演。这就是为什么 foldLeft这种函数的定义都是curry的形式。函数柯里化在提高函数适用性和延迟执行或者固定易变因素等方面有着重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"Scala 偏函数与部分应用函数的区分与理解","slug":"scala_1-3","date":"2017-07-11T13:33:18.000Z","updated":"2020-12-15T09:31:22.079Z","comments":true,"path":"2017/07/11/scala_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2017/07/11/scala_1-3/","excerpt":"","text":"部分应用函数 (Partial Applied Function)部分应用函数是缺少部分参数的函数，是一个逻辑上概念, 比如：1def sum(x: Int)(y: Int) = x + y 当调用sum的时候，如果不提供所有的参数或某些参数还未知时，比如sum _ , sum(3)(: Int), sum(: Int)(3), 这样就生成了所谓的部分应用函数。部分应用函数只是逻辑上的一个表达，scala编译器会用Function1， Function2这些类来表示它. 偏函数 Partial Function偏函数是只对函数定义域的一个子集进行定义的函数, 对于这个参数范围外的参数则抛出异常，这样的函数就是偏函数（顾名思异就是这个函数只处理传入来的部分参数）。 scala中用scala.PartialFunction[T,S] Trait 来表示,其中接收一个类型为 T 的参数，返回一个类型为 S 的结果。 1234val signal: PartialFunction[Int, Int] = &#123; case x if x &gt;= 1 =&gt; 1 case x if x &lt;= -1 =&gt; -1&#125; 这个signal所引用的函数除了0值外，对所有整数都定义了相应的操作。 signal(0) 会抛出异常，因此使用前最好先signal.isDefinedAt(0)判断一下。 偏函数主要用于这样一种场景：对某些值现在还无法给出具体的操作（即需求还不明朗），也有可能存在几种处理方式（视乎具体的需求）；我们可以先对需求明确的部分进行定义，比如上述除了0外的所有整数域，然后根据具体情况补充对其他域的定义，比如 :12345val composed_signal: PartialFunction[Int,Int] = signal.orElse&#123;case 0 =&gt; 0&#125;composed_signal(0) // 返回 0 或者对定义域进行一定的偏移（假如需求做了变更, 1 为无效的点） 1234567val new_signal: Function1[Int, Int] = signal.compose&#123; case x =&gt; x - 1&#125;new_signal(1) // throw exceptionnew_signal(0) // 返回 -1 new_signal(2) // 返回 1 还可以用andThen将两个相关的偏函数串接起来12345678910111213val another_signal: PartialFunction[Int, Int] = &#123; case 0 =&gt; 0 case x if x &gt; 0 =&gt; x - 1 case x if x &lt; 0 =&gt; x + 1&#125;val then_signal = another_signal andThen signalthen_signal(0) // throw exceptionthen_signal(-1) // throw exceptionthen_signal(1) // throw exceptionthen_signal(2) // 返回 1then_signal(-2) // 返回 -1 这里的then_signal 剔除了-1, 0, 1三个点的定义","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"curl 工具使用","slug":"curl_1-1","date":"2017-04-20T06:33:23.000Z","updated":"2020-12-10T07:43:40.522Z","comments":true,"path":"2017/04/20/curl_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2017/04/20/curl_1-1/","excerpt":"","text":"工具简介curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思。它的功能非常强大，命令行参数多达几十种。如果熟练的话，完全可以取代 Postman 这一类的图形界面工具。 参数简介123456789101112131415161718192021222324252627282930313233343536373839404142434445# 调试类-v, --verbose 输出信息-q, --disable 在第一个参数位置设置后 .curlrc 的设置直接失效，这个参数会影响到 -K, --config -A, --user-agent -e, --referer-K, --config FILE 指定配置文件-L, --location 跟踪重定向 (H)# CLI显示设置-s, --silent Silent模式。不输出任务内容-S, --show-error 显示错误. 在选项 -s 中，当 curl 出现错误时将显示-f, --fail 不显示 连接失败时HTTP错误信息-i, --include 显示 response的header (H/F)-I, --head 仅显示 响应文档头-l, --list-only 只列出FTP目录的名称 (F)-#, --progress-bar 以进度条 显示传输进度# 数据传输类-X, --request [GET|POST|PUT|DELETE|…] 使用指定的 http method 例如 -X POST-H, --header &lt;header&gt; 设定 request里的header 例如 -H \"Content-Type: application/json\"-e, --referer 设定 referer (H)-d, --data &lt;data&gt; 设定 http body 默认使用 content-type application/x-www-form-urlencoded (H) --data-raw &lt;data&gt; ASCII 编码 HTTP POST 数据 (H) --data-binary &lt;data&gt; binary 编码 HTTP POST 数据 (H) --data-urlencode &lt;data&gt; url 编码 HTTP POST 数据 (H)-G, --get 使用 HTTP GET 方法发送 -d 数据 (H)-F, --form &lt;name=string&gt; 模拟 HTTP 表单数据提交 multipart POST (H) --form-string &lt;name=string&gt; 模拟 HTTP 表单数据提交 (H)-u, --user &lt;user:password&gt; 使用帐户，密码 例如 admin:password-b, --cookie &lt;data&gt; cookie 文件 (H)-j, --junk-session-cookies 读取文件中但忽略会话cookie (H)-A, --user-agent 指定客户端的用户代理标头，user-agent设置，默认用户代理字符串是curl/[version] (H)# 传输设置-C, --continue-at OFFSET 断点续转-x, --proxy [PROTOCOL://]HOST[:PORT] 在指定的端口上使用代理-U, --proxy-user USER[:PASSWORD] 代理用户名及密码# 文件操作-T, --upload-file &lt;file&gt; 上传文件-a, --append 添加要上传的文件 (F/SFTP)# 输出设置-o, --output &lt;file&gt; 将输出写入文件，而非 stdout-O, --remote-name 将输出写入远程文件-D, --dump-header &lt;file&gt; 将头信息写入指定的文件-c, --cookie-jar &lt;file&gt; 操作结束后，要写入 Cookies 的文件位置 常用实例GET 请求1curl http://www.yahoo.com/login.cgi?user=XXXXXXXXX&amp;password=XXXXXX POST 请求123curl -d \"user=XXXXXXXX&amp;password=XXXXX\" http://www.yahoo.com/login.cgi// POST 文件curl -F upload= $localfile -F $btn_name=$btn_value http://192.168.10.1/www/focus/up_file.cgi 分块下载1234567curl -r 0 -10240 -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3 &amp;\\curl -r 10241 -20480 -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3 &amp;\\curl -r 20481 -40960 -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3 &amp;\\curl -r 40961 - -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3...// 合并块文件cat zhao.part* &gt; zhao.mp3 ftp 下载1curl -O ftp://用户名:密码@192.168.10.1:21/www/focus/enhouse/index.php ftp 上传1curl -T upload_test.php ftp://用户名:密码@192.168.10.1:21/www/focus/enhouse/","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://luckymartinlee.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://luckymartinlee.github.io/tags/Shell/"}]},{"title":"MongoDB从入门到放弃(一) -- 基础1","slug":"mongodb_1-1","date":"2016-10-25T05:25:15.000Z","updated":"2018-05-09T00:49:52.000Z","comments":true,"path":"2016/10/25/mongodb_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2016/10/25/mongodb_1-1/","excerpt":"","text":"MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的 NoSQL 数据库，为 WEB 应用提供可扩展的高性能数据存储解决方案。MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 NoSQLNoSQL(NoSQL = Not Only SQL )，指的是非关系型的数据库，是对不同于传统的关系型数据库的数据库管理系统的统称。NoSQL 用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 RDBMS vs NoSQLRDBMS 高度组织化结构化数据 结构化查询语言(SQL) 数据和关系都存储在单独的表中。 数据操纵语言，数据定义语言 严格的一致性 基础事务 NoSQL 代表着不仅仅是SQL 没有声明性查询语言 没有预定义的模式 键值对(key=&gt;value)存储，列存储，文档存储，图形数据库 最终一致性，而非ACID属性 非结构化和不可预知的数据 CAP定理 高性能，高可用性和可伸缩性 NoSQL的优/缺点优点: 高可扩展性 分布式计算 低成本 架构的灵活性，半结构化数据 没有复杂的关系 缺点: 没有标准化 有限的查询功能（到目前为止） 最终一致是不直观的程序 MongoDB 特点 MongoDB安装简单，MongoDB 是一个面向文档存储的数据库，操作起来简单、容易。 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是分片。 Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。 GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。 MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。 MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 MongoDB 基本概念在 MongoDB 中基本的概念有数据库、集合、文档、域、索引等，相较传统的关系型数据库对比图如下： SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 嵌入文档 表连接,MongoDB不支持表连接,但是有内嵌文档可以替代 primary key primary key 主键,MongoDB自动将_id字段设置为主键 举例如下： 安装(64 位 Linux上的安装)下载并解压tgz包1234$ curl -O https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.6.tgz # 下载$ tar -zxvf mongodb-linux-x86_64-3.0.6.tgz # 解压$ mv mongodb-linux-x86_64-3.0.6/ /usr/local/mongodb # 将解压包拷贝到指定目录 添加 MongoDB 可执行文件路径到 PATH1$ sudo export PATH=/usr/local/mongodb/bin:$PATH 创建数据存储目录，MongoDB 默认数据存储路径(dbpath)是 /data/db1$ sudo mkdir -p /data/db 启动 MongoDB 服务1$ /usr/local/mongodb/bin/mongod MongoDB web 界面MongoDB 3.2 及之前版本，提供了简单的 HTTP 用户界面，此功能需要在启动服务的时候添加参数 rest, 默认端口280171$ /usr/local/mongodb/bin/mongod --dbpath=/data/db --rest 如下图：","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://luckymartinlee.github.io/tags/MongoDB/"}]},{"title":"Javascript -- 闭包","slug":"javascript_1-1","date":"2013-08-02T11:14:11.000Z","updated":"2021-01-03T12:57:37.972Z","comments":true,"path":"2013/08/02/javascript_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2013/08/02/javascript_1-1/","excerpt":"","text":"闭包（closure）是Javascript语言的一个难点，也是它的特色，很多高级应用都要依靠闭包实现。 变量作用域要理解闭包，首先必须理解Javascript特殊的变量作用域。变量的作用域无非就是两种：全局变量和局部变量。Javascript语言的特殊之处，就在于函数内部可以直接读取全局变量。如：12345var n=999;function f1()&#123; alert(n);&#125;f1(); // 999 另一方面，在函数外部自然无法读取函数内的局部变量。1234function f1()&#123; var n=999;&#125;alert(n); // error 这里有一个地方需要注意，函数内部声明变量的时候，一定要使用var命令。如果不用的话，你实际上声明了一个全局变量！12345function f1()&#123; n=999;&#125;f1();alert(n); // 999 如何从外部读取局部变量?出于种种原因，我们有时候需要得到函数内的局部变量。但是，前面已经说过了，正常情况下，这是办不到的，只有通过变通方法才能实现。那就是在函数的内部，再定义一个函数。123456function f1()&#123; var n=999; function f2()&#123; alert(n); // 999 &#125;&#125; 在上面的代码中，函数f2就被包括在函数f1内部，这时f1内部的所有局部变量，对f2都是可见的。但是反过来就不行，f2内部的局部变量，对f1就是不可见的。这就是Javascript语言特有的”链式作用域”结构（chain scope），子对象会一级一级地向上寻找所有父对象的变量。所以，父对象的所有变量，对子对象都是可见的，反之则不成立。 既然f2可以读取f1中的局部变量，那么只要把f2作为返回值，我们不就可以在f1外部读取它的内部变量了吗.123456789function f1()&#123; var n=999; function f2()&#123; alert(n); &#125; return f2;&#125;var result=f1();result(); // 999 闭包的概念上一节代码中的f2函数，就是闭包。各种专业文献上的”闭包”（closure）定义非常抽象，很难看懂。我的理解是，闭包就是能够读取其他函数内部变量的函数。由于在Javascript语言中，只有函数内部的子函数才能读取局部变量，因此可以把闭包简单理解成”定义在一个函数内部的函数”。所以，在本质上，闭包就是将函数内部和函数外部连接起来的一座桥梁。 闭包可以用在许多地方。它的最大用处有两个，一个是前面提到的可以读取函数内部的变量，另一个就是让这些变量的值始终保持在内存中。1234567891011121314function f1()&#123; var n=999; nAdd=function()&#123;n+=1&#125; function f2()&#123; alert(n); &#125; return f2;&#125;var result=f1();result(); // 999nAdd();result(); // 1000 在这段代码中，result实际上就是闭包f2函数。它一共运行了两次，第一次的值是999，第二次的值是1000。这证明了，函数f1中的局部变量n一直保存在内存中，并没有在f1调用后被自动清除。 为什么会这样呢？原因就在于f1是f2的父函数，而f2被赋给了一个全局变量，这导致f2始终在内存中，而f2的存在依赖于f1，因此f1也始终在内存中，不会在调用结束后，被垃圾回收机制回收。 这段代码中另一个值得注意的地方，就是”nAdd=function(){n+=1}”这一行，首先在nAdd前面没有使用var关键字，因此nAdd是一个全局变量，而不是局部变量。其次，nAdd的值是一个匿名函数（anonymous function），而这个匿名函数本身也是一个闭包，所以nAdd相当于是一个setter，可以在函数外部对函数内部的局部变量进行操作。 仔细理解下面两段代码代码一：1234567891011 var name = \"The Window\"; var object = &#123; name : \"My Object\", getNameFunc : function()&#123; return function()&#123; return this.name; &#125;; &#125; &#125;; alert(object.getNameFunc()()); // The Window 代码二：1234567891011121314 var name = \"The Window\"; var object = &#123; name : \"My Object\", getNameFunc : function()&#123; var that = this; return function()&#123; return that.name; &#125;; &#125; &#125;; alert(object.getNameFunc()()); // My Object 使用闭包需要注意的地方1、 由于闭包会使得函数中的变量都被保存在内存中，内存消耗很大，所以不能滥用闭包，否则会造成网页的性能问题，在IE中可能导致内存泄露。解决方法是，在退出函数之前，将不使用的局部变量全部删除。2、 闭包会在父函数外部，改变父函数内部变量的值。所以，如果你把父函数当作对象（object）使用，把闭包当作它的公用方法（Public Method），把内部变量当作它的私有属性（private value），这时一定要小心，不要随便改变父函数内部变量的值。 上面的内容 摘自 阮一峰 - 学习Javascript闭包（Closure） 一文","categories":[],"tags":[{"name":"Js","slug":"Js","permalink":"http://luckymartinlee.github.io/tags/Js/"}]},{"title":"MySQL -- 锁，阻塞与死锁","slug":"mysql_1-6","date":"2013-06-09T01:35:31.000Z","updated":"2021-01-02T07:24:32.290Z","comments":true,"path":"2013/06/09/mysql_1-6/","link":"","permalink":"http://luckymartinlee.github.io/2013/06/09/mysql_1-6/","excerpt":"","text":"事务的隔离级别隔离级别是指若干个并发的事务之间的隔离程度，与我们开发时候主要相关的场景包括：脏读取、重复读、幻读。 脏读： 一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做”脏读”. 不可重复读： 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。 幻读： 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 “脏读”、“不可重复读”和“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。数据库实现事务隔离的方式，基本上可分为以下两种。 1.一种是在读取数据前，对其加锁，阻止其他事务对数据进行修改。2.另一种是不用加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot)，并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本，因此，这种技术叫做数据多版本并发控制（MultiVersion Concurrency Control，简称MVCC或MCC），也经常称为多版本数据库。 MySQL 的四种隔离级别（InnoDB 默认 可重复读(Repeated Read))未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)： 只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)： 可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB（mysql）默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻读 串行读(Serializable)： 完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 InnoDB行锁实现方式InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 1、 在不通过索引条件查询的时候，InnoDB使用的是表锁，而不是行锁。 2、 由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。如表 tab_with_index 对 id 加索引，有下面记录+——+——+| id | name |+——+——+| 1 | 1 || 1 | 4 || 2 | 2 |+——+——+ 对于 session_1:1234// 设置不自动提交mysql&gt; set autocommit=0;mysql&gt; select * from tab_with_index where id = 1 and name = '1' for update;// 此时未提交 对于 session_2 :123// 虽然 session_2 访问的是和 session_1 不同的记录，但是因为使用了相同的索引，所以需要等待锁：mysql&gt; select * from tab_with_index where id = 1 and name = '4' for update;//已提交，但是等待 session_1 释放锁 3、 当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁。如，同样是上面的例子，再对 name 加索引， 对于 session_1:1234// 设置不自动提交mysql&gt; set autocommit=0;mysql&gt; select * from tab_with_index where id = 1 for update;// 此时未提交 对于 session_2 :1234567// Session_2使用name的索引访问记录，因为记录没有被索引，所以可以获得锁：mysql&gt; select * from tab_with_index where name = '2' for update;1 row in set (0.00 sec)// 由于访问的记录已经被session_1锁定，所以等待获得锁。：mysql&gt; select * from tab_with_index where name = '4' for update;//已提交，但是等待 session_1 释放锁 4、 即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。 数据库阻塞阻塞： 第一个连接占有资源没有释放，而第二个连接需要获取这个资源。如果第一个连接没有提交或者回滚，第二个连接会一直等待下去，直到第一个连接释放该资源为止。上面出现的等待现象就是阻塞，当session_1提交，阻塞就消失。 数据库死锁死锁： 是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用，它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。那么对应的解决死锁问题的关键就是：让不同的session加锁有次序。 如何避免死锁：1、 为了减少死锁的可能性，请使用事务而不是LOCK TABLES语句；2、 保持插入或更新数据的事务足够小，以使其长时间不保持打开状态；因为大事物占用资源多耗时长，与其他事物冲突的概率也会变大。3、 当不同的事务更新多个表或大范围的行时，请SELECT … FOR UPDATE在每个事务中使用相同的操作顺序。4、 在SELECT … FOR UPDATE和 UPDATE … WHERE 语句中使用的列上创建索引。如果不走索引会为表的每一行记录加锁，死锁的概率会大大增加。 解除死锁的两种方法：1、 终止（或撤销）进程： 终止（或撤销）系统中的一个或多个死锁进程，直至打破循环环路，使系统从死锁状态中解除出来。2、 抢占资源： 从一个或多个进程中抢占足够数量的资源，分配给死锁进程，以打破死锁状态。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]},{"title":"MySQL -- 索引 三 FULLTEXT","slug":"mysql_1-5","date":"2013-06-07T05:31:56.000Z","updated":"2021-01-02T13:28:33.791Z","comments":true,"path":"2013/06/07/mysql_1-5/","link":"","permalink":"http://luckymartinlee.github.io/2013/06/07/mysql_1-5/","excerpt":"","text":"建立全文索引对于MySQL，全文索引建立在char、varchar和text的字段上，以前只能建立在 MyISAM 引擎上，现在新版MySSL 也可建立全文索引。 设置 MySQL 配置文件12345[mysqld]ft_wordlist_charset #表示词典的字符集ft_wordlist_file #词表文件，每行一个词及其词频ft_stopword_file #过滤掉不索引的词表，一行一个ft_min_word_len #加入索引的词的最小长度，默认为4，为了支持中文单字故可设置为2 三种建立方式12345678910111213141516171819202122232425// 方式一CREATE TABLE article ( id INT AUTO_INCREMENT NOT NULL PRIMARY KEY, title VARCHAR(200), body TEXT, FULLTEXT(title, body) ) TYPE=MYISAM;// 方式二ALTER TABLE student ADD FULLTEXT INDEX ft_stu_name (name);ALTER TABLE student ADD FULLTEXT ft_stu_name (name);// 方式三CREATE FULLTEXT INDEX ft_email_name ON student (name);// 也可指定长度CREATE FULLTEXT INDEX ft_email_name ON student (name(20));``` 两种删除方式``` bash// 方式一DROP INDEX full_idx_name ON tommy.girl;// 方式二ALTER TABLE tommy.girl DROP INDEX ft_email_abcd; 搜索语法MATCH (col1,col2,…) AGAINST (expr[search_modifier]) col1,col2,…为已建立FULLTEXT索引并要从中查找数据的列 expr为要查找的文本内容 search_modifier的每个取值代表一种类型的全文搜索 search_modifier取值: IN NATURAL LANGUAGE MODE： 自然语言全文搜索（默认）, 把搜索字符串解释为一系列单词并查找包含这些单词的数据行。 123SELECT * FROM articles WHERE MATCH (title,body) AGAINST ('精神' IN NATURAL LANGUAGE MODE);或SELECT * FROM articles WHERE MATCH (title,body) AGAINST ('精神'); IN BOOLEAN MODE： 布尔全文搜索 把搜索字符串解释为一系列单词，但允许使用一些操作符字符来&quot;修饰&quot;这些单词以表明特定的要求，如某给定单词必须出现（或不出现）在匹配数据行里，某个数据行必须包含一个精确的短语，等等 123456789101112131415161718192021222324252627282930SELECT * FROM articles WHERE MATCH (title,body) AGAINST ('+精神 -贯彻' IN BOOLEAN MODE); “+” 用在词的前面，表示一定要包含该词，并且必须在开始位置。 eg: +Apple 匹配：Apple123, \"tommy, Apple\"“-” 不包含该词，所以不能只用「-yoursql」这样是查不到任何row的，必须搭配其他语法使用。 eg: MATCH (girl_name) AGAINST ('-林志玲 +张筱雨') 匹配到： 所有不包含林志玲，但包含张筱雨的记录 空字符表明后跟的单词是可选的,但出现的话会增加该行的相关性 apple banana 找至少包含上面词中的一个的记录行 +apple +juice 两个词均在被包含 +apple macintosh 包含词 “apple”，但是如果同时包含 “macintosh”，它的排列将更高一些 +apple -macintosh 包含 “apple” 但不包含 “macintosh”“@distance” 用于指定两个或多个单词相互之间的距离（以单词度量）需在指定的范围内“&gt;” 提高该字的相关性，查询的结果会排在比较靠前的位置。“&lt;” 降低相关性，查询的结果会排在比较靠后的位置。 总结： 1. 只要使用 &gt;&lt;的总比没用的 靠前； 2. 使用 &gt;的一定比 &lt;的排的靠前 (这就符合相关性提高和降低)； 3. 使用同一类的，使用的越早，排的越前。“()” 用于将单词分组为子表达式且可以嵌套 eg: +aaa +(&gt;bbb &lt;ccc) // 找到有aaa和bbb和ccc，aaa和bbb，或者aaa和ccc(因为bbb，ccc前面没有+，所以表示可有可无)， // 然后 aaa&amp;bbb &gt; aaa&amp;bbb&amp;ccc &gt; aaa&amp;ccc“~” 将其相关性由正转负，表示拥有该字会降低相关性，但不像「-」将之排除，只是排在较后面。 eg: +apple ~macintosh 先匹配apple，但如果同时包含macintosh，就排名会靠后。“*” 为普通的通配符，若为单词指定了通配符，那么即使该单词过短或者出现在了停止字列表中它也不会被移除,这个只能接在字符串后面. MATCH (girl_name) AGAINST ('+*ABC*') #错误，不能放前面 MATCH (girl_name) AGAINST ('+张筱雨*') #正确“\" \"” 整体匹配，用双引号将一段句子包起来表示要完全相符，不可拆字。 eg: \"tommy huang\" 可以匹配 tommy huang xxxxx 但是不能匹配 tommy is huang。 WITH QUERY EXPANSION： 查询扩展全文搜索,这种搜索分两阶段进行。第一次，查出用户给定的关键词对应的记录；第二次，用第一次查出的结果里的关键词，再去查一次，把两次的结果返回给用户。 1SELECT id,title,body FROM articles WHERE MATCH(title,body) AGAINST('fulltext' with query expansion ) ORDER BY id ASC;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]},{"title":"MySQL -- 索引 二 字符串普通索引","slug":"mysql_1-4","date":"2013-06-02T02:15:21.000Z","updated":"2021-01-02T06:36:18.824Z","comments":true,"path":"2013/06/02/mysql_1-4/","link":"","permalink":"http://luckymartinlee.github.io/2013/06/02/mysql_1-4/","excerpt":"","text":"全字段索引就是给整个字段加索引，索引存储整个字段的值。数据量较小时，查询成本高，准确度高；数据量较大时，比较耗费空间；如对 summery (varchar(64)) 字段，添加全字段索引1mysql&gt; ALTER TABLE tb1 ADD INDEX summery_idx(summery); 前缀索引MySQL支持定义字符串的前面的一部分字节作为索引。查询成本低，比较节省空间；使用前缀索引查询时，每次遇到符号查询条件的记录都要回表判断一次，增加查询语句读数据的次数，也就是增加扫描行数；使用前缀索引时无法使用覆盖索引对查询的性能优化；1mysql&gt; ALTER TABLE tb1 ADD INDEX summery_idx(summery(10)); 对于 TEXT 类型字段，如使用普通字符串索引，只能使用前缀索引。 查询隐式转换1、 当操作符左右两边的数据类型不一致时，会发生隐式转换。2、 当where查询操作符左边为数值类型时发生了隐式转换，那么对效率影响不大，但还是不推荐这么做。3、 当where查询操作符左边为字符类型时发生了隐式转换，那么会导致索引失效，造成全表扫描效率极低。4、 字符串转换为数值类型时，非数字开头的字符串会转化为0，以数字开头的字符串会截取从第一个字符到第一个非数字内容为止的值为转化结果。 如：12345// 得到正确结果，使用索引，查询效率高mysql&gt; SELECT summery FROM tb1 WHERE summery = '123';// 得到正确结果，但不使用索引，查询效率低mysql&gt; SELECT summery FROM tb1 WHERE summery = 123; 所以，在写SQL时要养成良好的习惯，查询的字段是什么类型，等号右边的条件就写成对应的类型。特别当查询的字段是字符串时，等号右边的条件一定要用引号引起来标明这是一个字符串，否则会造成索引失效触发全表扫描。 常见问题1、 如存储学号时，比如201306070110。前四个字节表示入学年份，区分度较低，此时可以采用倒序存储， 也就是 字段值存储 011070603102，然后再使用前缀索引的方式提高查询效率，查询的时候也要用学号倒序来查询； 2、存储邮箱时，比如 streamXXX@email.com，此时正序和倒序的区分度都比较低，这时就可以考虑添加一个字段存储hash后的值，再对这个hash字段建立索引，查询的时候也要使用hash后的值进行查询。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]},{"title":"浏览器跨域问题","slug":"cross_origin_1-1","date":"2013-06-02T02:15:21.000Z","updated":"2021-01-03T07:09:12.858Z","comments":true,"path":"2013/06/02/cross_origin_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2013/06/02/cross_origin_1-1/","excerpt":"","text":"同源策略同源策略（Same origin policy）是一种约定，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，则浏览器的正常功能可能都会受到影响。可以说 Web 是构建在同源策略基础之上的，浏览器只是针对同源策略的一种实现。同源策略分为以下两种： DOM 同源策略：禁止对不同源页面 DOM 进行操作。这里主要场景是 iframe 跨域的情况，不同域名的 iframe 是限制互相访问的。XMLHttpRequest 同源策略：禁止使用 XHR 对象向不同源的服务器地址发起 HTTP 请求。 如果没有 DOM 同源策略，也就是说不同域的 iframe 之间可以相互访问，那么黑客可以这样进行攻击：1、 做一个假网站，里面用 iframe 嵌套一个银行网站 http://mybank.com。2、 把 iframe 宽高啥的调整到页面全部，这样用户进来除了域名，别的部分和银行的网站没有任何差别。3、 这时如果用户输入账号密码，我们的主网站可以跨域访问到 http://mybank.com 的 dom 节点，就可以拿到用户的账户密码了。 如果 XMLHttpRequest 同源策略，那么黑客可以进行 CSRF（跨站请求伪造） 攻击：1、 用户登录了自己的银行页面 http://mybank.com，http://mybank.com 向用户的 cookie 中添加用户标识。2、 用户浏览了恶意页面 http://evil.com，执行了页面中的恶意 AJAX 请求代码。3、 http://evil.com 向 http://mybank.com 发起 AJAX HTTP 请求，请求会默认把 http://mybank.com 对应 cookie 也同时发送过去。4、 银行页面从发送的 cookie 中提取用户标识，验证用户无误，response 中返回请求数据。此时数据就泄露了。而且由于 Ajax 在后台执行，用户无法感知这一过程。 跨域上面知道到了浏览器同源策略的作用，也正是有了跨域限制，才使我们能安全的上网。但是在实际中，有时候我们需要突破这样的限制 这就是跨域。因此下面将介绍几种跨域的解决方法。 一、 CORS（Cross-origin resource sharing，跨域资源共享）使用自定义的 HTTP 头部让浏览器与服务器进行沟通，从而决定请求或响应是应该成功，还是应该失败。整个 CORS 通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS 通信与同源的 AJAX 通信没有差别，代码完全一样。浏览器一旦发现 AJAX 请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。因此，实现 CORS 通信的关键是服务器。只要服务器实现了 CORS 接口，就可以跨源通信。浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。 只要同时满足以下两大条件，就属于简单请求。请求方法是以下三种方法之一：HEADGETPOSTHTTP的头信息不超出以下几种字段：AcceptAccept-LanguageContent-LanguageLast-Event-IDContent-Type：只限于三个值 application/x-www-form-urlencoded、multipart/form-data、text/plain凡是不同时满足上面两个条件，就属于非简单请求。 简单请求1、 在请求中需要附加一个额外的 Origin 头部，其中包含请求页面的源信息（协议、域名和端口），以便服务器根据这个头部信息来决定是否给予响应。例如：Origin: http://www.test.cn2、如果服务器认为这个请求可以接受，就在 Access-Control-Allow-Origin 头部中回发相同的源信息（如果是公共资源，可以回发 * ）。例如：Access-Control-Allow-Origin：http://www.test.cn3、 没有这个头部或者有这个头部但源信息不匹配，浏览器就会驳回请求。正常情况下，浏览器会处理请求。注意，请求和响应都不包含 cookie 信息。4、 如果需要包含 cookie 信息，ajax 请求需要设置 xhr 的属性 withCredentials 为 true，服务器需要设置响应头部 Access-Control-Allow-Credentials: true。 非简单请求浏览器在发送真正的请求之前，会先发送一个 Preflight 请求给服务器，这种请求使用 OPTIONS 方法，发送下列头部： Origin：与简单的请求相同。 Access-Control-Request-Method: 请求自身使用的方法。 Access-Control-Request-Headers: （可选）自定义的头部信息，多个头部以逗号分隔。例如：123Origin: http://www.testweb.cnAccess-Control-Request-Method: POSTAccess-Control-Request-Headers: NCZ 发送这个请求后，服务器可以决定是否允许这种类型的请求。服务器通过在响应中发送如下头部与浏览器进行沟通： Access-Control-Allow-Origin：与简单的请求相同。 Access-Control-Allow-Methods: 允许的方法，多个方法以逗号分隔。 Access-Control-Allow-Headers: 允许的头部，多个方法以逗号分隔。 Access-Control-Max-Age: 应该将这个 Preflight 请求缓存多长时间（以秒表示）。 例如：1234Access-Control-Allow-Origin: http://www.testweb.cnAccess-Control-Allow-Methods: GET, POSTAccess-Control-Allow-Headers: NCZAccess-Control-Max-Age: 1728000 一旦服务器通过 Preflight 请求允许该请求之后，以后每次浏览器正常的 CORS 请求，就都跟简单请求一样了。 CORS 优点1、 CORS 通信与同源的 AJAX 通信没有差别，代码完全一样，容易维护。2、 支持所有类型的 HTTP 请求。CORS 缺点1、 存在兼容性问题，特别是 IE10 以下的浏览器。2、 第一次发送非简单请求时会多一次请求。 二、 JSONP由于 script 标签不受浏览器同源策略的影响，允许跨域引用资源。因此可以通过动态创建 script 标签，然后利用 src 属性进行跨域，这也就是 JSONP 跨域的基本原理。 直接通过下面的例子来说明 JSONP 实现跨域的流程：1、 使用script标签的src属性来完成JSONP跨域请求 12345678910111213141516171819202122232425262728293031323334// 前端代码// 1. 定义一个 回调函数 handleResponse 用来接收返回的数据function handleResponse(data) &#123; console.log(data);&#125;;// 2. 动态创建一个 script 标签，并且告诉后端回调函数名叫 handleResponsevar body = document.getElementsByTagName('body')[0];var script = document.getElement('script');script.src = 'http://www.testweb.cn/json?callback=handleResponse';body.appendChild(script);// 3. 通过 script.src 请求 ‘http://www.testweb.cn/json?callback=handleResponse’，// 4. 后端能够识别这样的 URL 格式并处理该请求，然后返回 handleResponse(&#123;\"name\": \"testweb\"&#125;) 给浏览器// 5. 浏览器在接收到 handleResponse(&#123;\"name\": \"testweb\"&#125;) 之后立即执行 ，也就是执行 handleResponse 方法，获得后端返回的数据，这样就完成一次跨域请求了。// 后端代码protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setCharacterEncoding(\"UTF-8\"); response.setContentType(\"text/html;charset=UTF-8\"); //数据 List&lt;Student&gt; studentList = getStudentList(); JSONArray jsonArray = JSONArray.fromObject(studentList); String result = jsonArray.toString(); //前端传过来的回调函数名称 String callback = request.getParameter(\"callback\"); //用回调函数名称包裹返回数据，这样，返回数据就作为回调函数的参数传回去了 result = callback + \"(\" + result + \")\"; response.getWriter().write(result);&#125; 2、使用jquery的JSONP方式实现跨域 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 前端代码&lt;script&gt; function showData (data) &#123; console.info(\"调用showData\"); var result = JSON.stringify(data); $(\"#text\").val(result); &#125; $(document).ready(function () &#123; $(\"#btn\").click(function () &#123; $.ajax(&#123; url: \"http://localhost:9090/student\", type: \"GET\", // 此处即使改为 POST, jquery 也会自动会转为GET方式 dataType: \"jsonp\", //指定服务器返回的数据类型 jsonp: \"theFunction\", //指定参数名称，默认是callback jsonpCallback: \"showData\", //指定回调函数名称 success: function (data) &#123; console.info(\"调用success\"); &#125; &#125;); &#125;); &#125;);&lt;/script&gt;// 执行结果是，先调用callbak毁掉函数，// 再调用 success// 后端代码protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; response.setCharacterEncoding(\"UTF-8\"); response.setContentType(\"text/html;charset=UTF-8\"); //数据 List&lt;Student&gt; studentList = getStudentList(); JSONArray jsonArray = JSONArray.fromObject(studentList); String result = jsonArray.toString(); //前端传过来的回调函数名称 String callback = request.getParameter(\"theFunction\"); //用回调函数名称包裹返回数据，这样，返回数据就作为回调函数的参数传回去了 result = callback + \"(\" + result + \")\"; response.getWriter().write(result);&#125; JSONP 优点使用简便，没有兼容性问题，目前最流行的一种跨域方法。JSONP 缺点只支持 GET 请求。由于是从其它域中加载代码执行，因此如果其他域不安全，很可能会在响应中夹带一些恶意代码。要确定 JSONP 请求是否失败并不容易。虽然 HTML5 给 script 标签新增了一个 onerror 事件处理程序，但是存在兼容性问题。 三、 图像 Ping 跨域由于 img 标签不受浏览器同源策略的影响，允许跨域引用资源。因此可以通过 img 标签的 src 属性进行跨域，这也就是图像 Ping 跨域的基本原理。 直接通过下面的例子来说明图像 Ping 实现跨域的流程：123456789var img = new Image();// 通过 onload 及 onerror 事件可以知道响应是什么时候接收到的，但是不能获取响应文本img.onload = img.onerror = function() &#123; console.log(\"Done!\");&#125;// 请求数据通过查询字符串形式发送img.src = 'http://www.testweb.cn/test?name=testweb'; 图像 Ping 跨域,用于实现跟踪用户点击页面或动态广告曝光次数有较大的优势。但是，同样只支持 GET 请求，且只能浏览器与服务器的单向通信，因为浏览器不能访问服务器的响应文本 四、 服务器代理浏览器有跨域限制，但是服务器不存在跨域问题，所以可以由服务器请求所要域的资源再返回给客户端。服务器代理是万能的，但是因为经过了两次请求，所以效率不高。以 nginx 为例，通过nginx配置一个代理服务器（域名与domain1相同，端口不同）做跳板机，反向代理访问domain2接口，并且可以顺便修改cookie中domain信息，方便当前域cookie写入，实现跨域登录。1234567891011121314// proxy服务器server &#123; listen 81; server_name www.domain1.com; location / &#123; proxy_pass http://www.domain2.com:8080; #反向代理 proxy_cookie_domain www.domain2.com www.domain1.com; #修改cookie里域名 index index.html index.htm; # 当用webpack-dev-server等中间件代理接口访问nignx时，此时无浏览器参与，故没有同源限制，下面的跨域配置可不启用 add_header Access-Control-Allow-Origin http://www.domain1.com; #当前端只跨域不带cookie时，可为* add_header Access-Control-Allow-Credentials true; &#125;&#125; 五、 document.domain 跨域对于主域名相同，而子域名不同的情况，可以使用 document.domain 来跨域。这种方式非常适用于 iframe 跨域的情况。 比如，有一个页面，它的地址是 http://www.testweb.cn/a.html，在这个页面里面有一个 iframe，它的 src 是 http://testweb.cn/b.html。很显然，这个页面与它里面的 iframe 框架是不同域的，所以我们是无法通过在页面中书写 js 代码来获取 iframe 中的东西的。 这个时候，document.domain 就可以派上用场了，我们只要把 http://www.testweb.cn/a.html 和 http://testweb.cn/b.html 这两个页面的 document.domain 都设成相同的域名就可以了。但要注意的是，document.domain 的设置是有限制的，我们只能把 document.domain 设置成自身或更高一级的父域，且主域必须相同。例如：a.b.testweb.cn 中某个文档的 document.domain 可以设成 a.b.testweb.cn、b.testweb.cn 、testweb.cn 中的任意一个，但是不可以设成 c.a.b.testweb.cn ，因为这是当前域的子域，也不可以设成 baidu.com，因为主域已经不相同了。 例如，在页面 http://www.testweb.cn/a.html 中设置document.domain：1234567&lt;iframe src=\"http://testweb.cn/b.html\" id=\"myIframe\" onload=\"test()\"&gt;&lt;script&gt; document.domain = 'testweb.cn'; // 设置成主域 function test() &#123; console.log(document.getElementById('myIframe').contentWindow); &#125;&lt;/script&gt; 在页面 http://testweb.cn/b.html 中也设置 document.domain，而且这也是必须的，虽然这个文档的 domain 就是 testweb.cn，但是还是必须显式地设置 document.domain 的值：123&lt;script&gt; document.domain = 'testweb.cn'; // document.domain 设置成与主页面相同&lt;/script&gt; 这样，http://www.testweb.cn/a.html 就可以通过 js 访问到 http://testweb.cn/b.html 中的各种属性和对象了。 六、 window.name 跨域window 对象有个 name 属性，该属性有个特征：即在一个窗口（window）的生命周期内，窗口载入的所有的页面（不管是相同域的页面还是不同域的页面）都是共享一个 window.name 的，每个页面对 window.name 都有读写的权限，window.name 是持久存在一个窗口载入过的所有页面中的，并不会因新页面的载入而进行重置。通过下面的例子介绍如何通过 window.name 来跨域获取数据的。 页面 http://www.testweb.cn/a.html 的代码：123456789101112131415161718&lt;iframe src=\"http://testweb.cn/b.html\" id=\"myIframe\" onload=\"test()\" style=\"display: none;\"&gt;&lt;script&gt; // 2. iframe载入 \"http://testweb.cn/b.html\" 页面后会执行该函数 function test() &#123; var iframe = document.getElementById('myIframe'); // 重置 iframe 的 onload 事件程序， // 此时经过后面代码重置 src 之后， // http://www.testweb.cn/a.html 页面与该 iframe 在同一个源了，可以相互访问了 iframe.onload = function() &#123; var data = iframe.contentWindow.name; // 4. 获取 iframe 里的 window.name console.log(data); // hello world! &#125;; // 3. 重置一个与 http://www.testweb.cn/a.html 页面同源的页面 iframe.src = 'http://www.testweb.cn/c.html'; &#125;&lt;/script&gt; 页面 http://testweb.cn/b.html 的代码1234&lt;script type=\"text/javascript\"&gt; // 1. 给当前的 window.name 设置一个 http://www.testweb.cn/a.html 页面想要得到的数据值 window.name = \"hello world!\";&lt;/script&gt; 七、 location.hash 跨域location.hash 方式跨域，是子框架具有修改父框架 src 的 hash 值，通过这个属性进行传递数据，且更改 hash 值，页面不会刷新。但是传递的数据的字节数是有限的。 页面 http://www.testweb.cn/a.html 的代码：123456789&lt;iframe src=\"http://testweb.cn/b.html\" id=\"myIframe\" onload=\"test()\" style=\"display: none;\"&gt;&lt;script&gt; // 2. iframe载入 \"http://testweb.cn/b.html\" 页面后会执行该函数 function test() &#123; // 3. 获取通过 http://testweb.cn/b.html 页面设置 hash 值 var data = window.location.hash; console.log(data); &#125;&lt;/script&gt; 页面 http://testweb.cn/b.html 的代码：1234&lt;script type=\"text/javascript\"&gt; // 1. 设置父页面的 hash 值 parent.location.hash = \"world\";&lt;/script&gt; 八、 postMessage 跨域window.postMessage(message，targetOrigin) 方法是 HTML5 新引进的特性，可以使用它来向其它的 window 对象发送消息，无论这个 window 对象是属于同源或不同源。这个应该就是以后解决 dom 跨域通用方法了。 调用 postMessage 方法的 window 对象是指要接收消息的那一个 window 对象，该方法的第一个参数 message 为要发送的消息，类型只能为字符串；第二个参数 targetOrigin 用来限定接收消息的那个 window 对象所在的域，如果不想限定域，可以使用通配符 *。 需要接收消息的 window 对象，可是通过监听自身的 message 事件来获取传过来的消息，消息内容储存在该事件对象的 data 属性中。 页面 http://www.testweb.cn/a.html 的代码：1234567891011&lt;iframe src=\"http://testweb.cn/b.html\" id=\"myIframe\" onload=\"test()\" style=\"display: none;\"&gt;&lt;script&gt; // 1. iframe载入 \"http://testweb.cn/b.html\" 页面后会执行该函数 function test() &#123; // 2. 获取 http://testweb.cn/b.html 页面的 window 对象， // 然后通过 postMessage 向 http://testweb.cn/b.html 页面发送消息 var iframe = document.getElementById('myIframe'); var win = iframe.contentWindow; win.postMessage('我是来自 http://www.testweb.cn/a.html 页面的消息', '*'); &#125;&lt;/script&gt; 页面 http://testweb.cn/b.html 的代码：1234567&lt;script type=\"text/javascript\"&gt; // 注册 message 事件用来接收消息 window.onmessage = function(e) &#123; e = e || event; // 获取事件对象 console.log(e.data); // 通过 data 属性得到发送来的消息 &#125;&lt;/script&gt; 九、 websocket 跨域Websocket是HTML5的一个持久化的协议，它实现了浏览器与服务器的全双工通信，同时也是跨域的一种解决方案。WebSocket和HTTP都是应用层协议，都基于 TCP 协议。但是 WebSocket 是一种双向通信协议，在建立连接之后，WebSocket 的 server 与 client 都能主动向对方发送或接收数据。同时，WebSocket 在建立连接时需要借助 HTTP 协议，连接建立好了之后 client 与 server 之间的双向通信就与 HTTP 无关了。 原生WebSocket API使用起来不太方便，我们使用Socket.io，它很好地封装了webSocket接口，提供了更简单、灵活的接口，也对不支持webSocket的浏览器提供了向下兼容。 我们先来看个例子：本地文件socket.html向localhost:3000发生数据和接受数据12345678910111213141516171819202122// socket.html&lt;script&gt; // 高级api 不兼容 但是有一个socket.io这个库，是兼容的(一般用这个) let socket = new WebSocket('ws://localhost:3000'); socket.onopen = function () &#123; socket.send('hello');//向服务器发送数据 &#125; socket.onmessage = function (e) &#123; console.log(e.data);//接收服务器返回的数据 &#125;&lt;/script&gt;// server.jslet express = require('express');let app = express();let WebSocket = require('ws');//记得安装wslet wss = new WebSocket.Server(&#123;port:3000&#125;);wss.on('connection',function(ws) &#123; ws.on('message', function (data) &#123; console.log(data); ws.send('this is server') &#125;);&#125;)","categories":[],"tags":[{"name":"网络安全","slug":"网络安全","permalink":"http://luckymartinlee.github.io/tags/网络安全/"}]},{"title":"常见网络攻击和防范","slug":"hack_1-1","date":"2013-06-02T02:15:21.000Z","updated":"2021-01-03T03:31:35.706Z","comments":true,"path":"2013/06/02/hack_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2013/06/02/hack_1-1/","excerpt":"","text":"CSRFCSRF(Cross Site Request Forgery)，跨站请求伪造，它利用用户已登录的身份，在用户毫不知情的情况下，以用户的名义完成非法操作。攻击者盗用了你的身份，以你的名义发送恶意请求，对服务器来说这个请求是完全合法的，但是却完成了攻击者所期望的一个操作，比如以你的名义发送邮件、发消息，盗取你的账号，添加系统管理员，甚至于购买商品、虚拟货币转账等。 如下：其中Web A为存在CSRF漏洞的网站，Web B为攻击者构建的恶意网站，User 为Web A网站的合法用户。 攻击过程：1、 用户打开浏览器，访问受信任网站A，输入用户名和密码请求登录网站A；2、 在用户信息通过验证后，网站A产生Cookie信息并返回给浏览器，此时用户登录网站A成功，可以正常发送请求到网站A；3、 用户未退出网站A之前，在同一浏览器中，打开一个TAB页访问网站B；4、 网站B接收到用户请求后，返回一些攻击性代码，并发出一个请求要求访问第三方站点A；5、 浏览器在接收到这些攻击性代码后，根据网站B的请求，在用户不知情的情况下携带Cookie信息，向网站A发出请求。网站A并不知道该请求其实是由B发起的，所以会根据用户C的Cookie信息以C的权限处理该请求，导致来自网站B的恶意代码被执行。 防御手段：1、 验证 HTTP Referer 字段根据 HTTP 协议，在 HTTP 头中有一个字段叫 Referer，它记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，比如需要访问 http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory，用户必须先登陆 bank.example，然后通过点击页面上的按钮来触发转账事件。这时，该转帐请求的 Referer 值就会是转账按钮所在的页面的 URL，通常是以 bank.example 域名开头的地址。而如果黑客要对银行网站实施 CSRF 攻击，他只能在他自己的网站构造请求，当用户通过黑客的网站发送请求到银行时，该请求的 Referer 是指向黑客自己的网站。因此，要防御 CSRF 攻击，银行网站只需要对于每一个转账请求验证其 Referer 值，如果是以 bank.example 开头的域名，则说明该请求是来自银行网站自己的请求，是合法的。如果 Referer 是其他网站的话，则有可能是黑客的 CSRF 攻击，拒绝该请求。存在的问题： 1、部分浏览器存在漏洞，可以篡改 Referer 值，绕过验证 2、存在信息安全，隐私泄露问题，Referer 值 运行为空，无法验证 2、 在请求地址参数中添加 token 并验证CSRF 攻击之所以能够成功，是因为黑客可以完全伪造用户的请求，该请求中所有的用户验证信息都是存在于 cookie 中，因此黑客可以在不知道这些验证信息的情况下直接利用用户自己的 cookie 来通过安全验证。要抵御 CSRF，关键在于在请求中放入黑客所不能伪造的信息，并且该信息不存在于 cookie 之中。可以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。对于 GET 请求，token 将附在请求地址之后，这样 URL 就变成 http://url?csrftoken=tokenvalue。 而对于 POST 请求来说，要在 form 的最后加上 &lt;input type=”hidden” name=”csrftoken” value=”tokenvalue”/&gt;，这样就把 token 以参数的形式加入请求了存在的问题：难以保证 token 本身的安全, 黑客可以在目标网站（尤其是一些论坛网站）上面发布自己个人网站的地址，黑客的网站也同样可以通过 Referer 来得到这个 token 值以发动 CSRF 攻击。这也是一些用户喜欢手动关闭浏览器 Referer 功能的原因。 3、 在 HTTP 头中添加 token 并验证这种方法也是使用 token 并进行验证，和上一种方法不同的是，这里并不是把 token 以参数的形式置于 HTTP 请求参数之中，而是把它放到 HTTP 头中自定义的属性里。这样解决了上种方法在请求中加入 token 的不便，同时，通过 这种 ajax 请求的地址不会被记录到浏览器的地址栏，也不用担心 token 会透过 Referer 泄露到其他网站中去。然而这种方法的局限性非常大，这种请求通常用于 Ajax 方法中对于页面局部的异步刷新，并非所有的请求都适合用这个类来发起，而且通过该类请求得到的页面不能被浏览器所记录下，从而进行前进，后退，刷新，收藏等操作，给用户带来不便。 XSSXSS(Cross Site Scripting),跨站脚本攻击XSS是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。1、 反射型 XSS反射性xss一般指攻击者通过特定的方式来诱惑受害者去访问一个包含恶意代码的URL。当受害者点击恶意链接url的时候，恶意代码会直接在受害者的主机上的浏览器执行。反射型XSS的攻击步骤如下： 攻击者在url后面的参数中加入恶意攻击代码。 当用户打开带有恶意代码的URL的时候，网站服务端将恶意代码从URL中取出，拼接在html中并且返回给浏览器端。 用户浏览器接收到响应后执行解析，其中的恶意代码也会被执行到。 攻击者通过恶意代码来窃取到用户数据并发送到攻击者的网站。攻击者会获取到比如cookie等信息，然后使用该信息来冒充合法用户的行为，调用目标网站接口执行攻击等操作。 2、 存储型 XSS存储型 XSS主要是将恶意代码上传或存储到服务器中，下次只要受害者浏览包含此恶意代码的页面就会执行恶意代码。存储型XSS的攻击步骤如下： 攻击者将恶意代码提交到目标网站数据库中。 用户打开目标网站时，网站服务器将恶意代码从数据库中取出，然后拼接到html中返回给浏览器中。 用户浏览器接收到响应后解析执行，那么其中的恶意代码也会被执行。 那么恶意代码执行后，就能获取到用户数据，比如上面的cookie等信息，那么把该cookie发送到攻击者网站中，那么攻击者拿到该cookie然后会冒充该用户的行为，调用目标网站接口等违法操作。 3、DOM-based型 XSSDOM型XSS的攻击步骤如下： 攻击者构造出特殊的URL、在其中可能包含恶意代码。 用户打开带有恶意代码的URL。 用户浏览器收到响应后解析执行。前端使用js取出url中的恶意代码并执行。 执行时，恶意代码窃取用户数据并发送到攻击者的网站中，那么攻击者网站拿到这些数据去冒充用户的行为操作。调用目标网站接口执行攻击者一些操作。 防御手段：1、服务器端设置 cookie 为 http-only在服务器端设置cookie的时候设置 http-only, 这样就可以防止用户通过JS获取cookie。对cookie的读写或发送一般有如下字段进行设置： http-only: 只允许http或https请求读取cookie、JS代码是无法读取cookie的(document.cookie会显示http-only的cookie项被自动过滤掉)。发送请求时自动发送cookie.secure-only: 只允许https请求读取，发送请求时自动发送cookie。host-only: 只允许主机域名与domain设置完成一致的网站才能访问该cookie。 2、对 html, html 属性, URL, CSS 进行编码转义，不应该直接输出到页面 3、开启CSP网页安全策略CSP(Content Security Policy)，网页安全策略，是一种由开发者定义的安全性政策申明，通过CSP所约束的责任指定可信的内容来源，通过 Content-Security-Policy 网页的开发者可以控制整个页面中 外部资源 的加载和执行。比如可以控制哪些 域名下的静态资源可以被页面加载，哪些不能被加载。这样就可以很大程度的防范了 来自 跨站(域名不同) 的脚本攻击。如：12345&lt;meta http-equiv=\"Content-Security-Policy\" content=\"default-src http: https: *.xxx.com 'self' 'unsafe-inline' ;style-src 'self' 'unsafe-inline' *.yyy.com;script-src 'self' 'unsafe-inline' 'unsafe-eval' ;\"&gt; 默认设置（default-src）：信任 http ,https协议资源，信任当前域名资源，信任符合.xxx.com的域名资源CSS设置（style-src）：信任当前域名资源，允许内嵌的CSS资源，信任来自.yyy.com下的CSS资源。JS设置（script-src）：信任当前域名资源，允许内嵌的JS执行，允许将字符串当作代码执行 4、浏览器开启X-XSS-Protection目前该属性被所有的主流浏览器默认开启XSS保护。该参数是设置在响应头中目的是用来防范XSS攻击的。它有如下几种配置(默认为1):0：禁用XSS保护。1：启用XSS保护。1;mode=block; 启用xss保护，并且在检查到XSS攻击是，停止渲染页面。 DoS/DDsSDoS(Denial of Service),拒绝服务攻击, 目的就是让一个公开网站无法访问.DDoS(Distributed Denial of Service), 分布式拒绝服务攻击,是 DoS 的升级版.就是利用大量合法的分布式服务器对目标发送请求，从而导致正常合法用户无法获得服务。通俗点讲就是利用网络节点资源如：IDC服务器、个人PC、手机、智能设备、打印机、摄像头等对目标发起大量攻击请求，从而导致服务器拥塞而无法对外提供正常服务.针对常见的cc攻击的防御手段：1、备份网站防范 DDOS 的第一步，就是你要有一个备份网站，或者最低限度有一个临时主页。生产服务器万一下线了，可以立刻切换到备份网站，不至于毫无办法。备份网站不一定是全功能的，如果能做到全静态浏览，就能满足需求。最低限度应该可以显示公告，告诉用户，网站出了问题，正在全力抢修。这种临时主页建议放到 Github Pages 或者 Netlify，它们的带宽大，可以应对攻击，而且都支持绑定域名，还能从源码自动构建。2、HTTP 请求拦截HTTP 请求的特征一般有两种：IP 地址和 User Agent 字段。比如，恶意请求都是从某个 IP 段发出的，那么把这个 IP 段封掉就行了。或者，它们的 User Agent 字段有特征（包含某个特定的词语），那就把带有这个词语的请求拦截。可以从三个层次来拦截：（1）专用硬件Web 服务器的前面可以架设硬件防火墙，专门过滤请求。这种效果最好，但是价格也最贵。（2）本机防火墙操作系统都带有软件防火墙，Linux 服务器一般使用 iptables。比如，拦截 IP 地址1.2.3.4的请求，可以执行下面的命令。1$ iptables -A INPUT -s 1.2.3.4 -j DROP （3）Web 服务器Web 服务器也可以过滤请求。拦截 IP 地址1.2.3.4，nginx 的写法如下123location / &#123; deny 1.2.3.4;&#125; Apache 的写法是在.htaccess文件里面，加上下面一段1234&lt;RequireAll&gt; Require all granted Require not ip 1.2.3.4&lt;/RequireAll&gt; 如果想要更精确的控制（比如自动识别并拦截那些频繁请求的 IP 地址），就要用到 WAF。Web 服务器的拦截非常消耗性能，尤其是 Apache。稍微大一点的攻击，这种方法就没用了。 3、 带宽扩容上面说的HTTP 拦截有一个前提，就是请求必须有特征。但是，真正的 DDOS 攻击是没有特征的，它的请求看上去跟正常请求一样，而且来自不同的 IP 地址，所以没法拦截。这就是为什么 DDOS 特别难防的原因。要想防范这类攻击，那就只有想办法扩容，把这些请求都消化掉。如：每个主机保 5G 流量以下的攻击，那就一口气买了5个。网站架设在其中一个主机上面，但是不暴露给用户，其他主机都是镜像，用来面对用户，DNS 会把访问量均匀分配到这四台镜像服务器。一旦出现攻击，这种架构就可以防住 20G 的流量，如果有更大的攻击，那就买更多的临时主机，不断扩容镜像。 4、CDNCDN 指的是网站的静态内容分发到多个服务器，用户就近访问，提高速度。因此，CDN 也是带宽扩容的一种方法，可以用来防御 DDOS 攻击。网站内容存放在源服务器，CDN 上面是内容的缓存。用户只允许访问 CDN，如果内容不在 CDN 上，CDN 再向源服务器发出请求。这样的话，只要 CDN 够大，就可以抵御很大的攻击。不过，这种方法有一个前提，网站的大部分内容必须可以静态缓存。对于动态内容为主的网站（比如论坛），就要想别的办法，尽量减少用户对动态数据的请求。上面提到的镜像服务器扩容，本质就是自己搭建一个微型 CDN。各大云服务商提供的高防 IP，背后也是这样做的：网站域名指向高防 IP，它提供一个缓冲层，清洗流量，并对源服务器的内容进行缓存。这里有一个关键点，一旦上了 CDN，千万不要泄露源服务器的 IP 地址，否则攻击者可以绕过 CDN 直接攻击源服务器，前面的努力都白费。cloudflare 是一个免费 CDN 服务，并提供防火墙，高度推荐。","categories":[],"tags":[{"name":"网络安全","slug":"网络安全","permalink":"http://luckymartinlee.github.io/tags/网络安全/"}]},{"title":"Laravel 框架 重难点","slug":"laravel_1-1","date":"2013-06-02T02:15:21.000Z","updated":"2021-01-03T08:35:08.547Z","comments":true,"path":"2013/06/02/laravel_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2013/06/02/laravel_1-1/","excerpt":"","text":"Facade以数据库访问为实例，如：1$users = DB::connection('foo')-&gt;select(...); Facade 实现原理IOC容器是 Laravel 框架的最最重要的部分。它提供了两个功能，IOC和容器。IOC(Inversion of Control)，也叫控制反转。说白了，就是控制对象的生成，使开发者不用关心对象的如何生成，只需要关心它的使用即可。而通过IOC机制生成的对象实例需要一个存放的位置，以便之后继续使用，便是它的容器功能。关于IOC容器，需要记住两点即可：1、 根据配置生成对象实例；2、 保存对象实例，方便随时取用； 看下面简化后的源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475&lt;?phpnamespace facades;abstract class Facade&#123; protected static $app; /** * Set the application instance. * * @param \\Illuminate\\Contracts\\Foundation\\Application $app * @return void */ public static function setFacadeApplication($app) &#123; static::$app = $app; &#125; /** * Get the registered name of the component. * * @return string * * @throws \\RuntimeException */ protected static function getFacadeAccessor() &#123; throw new RuntimeException('Facade does not implement getFacadeAccessor method.'); &#125; /** * Get the root object behind the facade. * * @return mixed */ public static function getFacadeRoot() &#123; return static::resolveFacadeInstance(static::getFacadeAccessor()); &#125; /** * Resolve the facade root instance from the container. * * @param string|object $name * @return mixed */ protected static function resolveFacadeInstance($name) &#123; return static::$app-&gt;instances[$name]; &#125; public static function __callStatic($method, $args) &#123; $instance = static::getFacadeRoot(); if (! $instance) &#123; throw new RuntimeException('A facade root has not been set.'); &#125; switch (count($args)) &#123; case 0: return $instance-&gt;$method(); case 1: return $instance-&gt;$method($args[0]); case 2: return $instance-&gt;$method($args[0], $args[1]); case 3: return $instance-&gt;$method($args[0], $args[1], $args[2]); case 4: return $instance-&gt;$method($args[0], $args[1], $args[2], $args[3]); default: return call_user_func_array([$instance, $method], $args); &#125; &#125;&#125; 其中：1、 $app中存放的就是一个IOC容器实例，它是在框架初始化时，通过 setFacadeApplication() 这个静态方法设置的2、 它实现了__callStatic 魔术方法3、 getFacadeAccessor() 方法需要子类去继承，返回一个string的标识，通过这个标识，IOC容器便能返回它所绑定类（框架初始化或其它时刻绑定）的对象4、通过 $instance 调用具体的方法 我们尝试创建自定义的 Facade,第一步，实现自己类的具体业务逻辑12345678&lt;?phpclass Test1&#123; public function hello() &#123; print(\"hello world\"); &#125;&#125; 第二步，实现自定义类的 Facede123456789101112131415161718&lt;?phpnamespace facades;/** * Class Test1 * @package facades * * @method static setOverRecommendInfo [设置播放完毕时的回调函数] * @method static setHandlerPlayer [明确指定下一首时的执行类] */class Test1Facade extends Facade&#123; protected static function getFacadeAccessor() &#123; return 'test1'; &#125; &#125; 第三步，使用自定义的 Facade123use facades\\Test1Facade;Test1Facade::hello(); // 这是 Facade 调用 整个过程如下:1、 facades\\Test1Facade 调用静态方法 hello() 时，由于没有定义此方法，会调用callStatic;2、 在callStatic 中，首先是获取对应的实例，即 return static::$app-&gt;instances[$name];。这其中的 $name，即为 facades\\Test1 里的 test13、 $app， 即为 IOC 容器，类的实例化过程，就交由它来处理。 路由1、 路由组12345678910Route::group( [ 'prefix'=&gt;'admin', 'namespace'=&gt;'\\App\\Admin\\Controllers', 'middleware'=&gt;'auth:web' ], function()&#123; Route::get('user/&#123;id&#125;/&#123;name&#125;', 'UserController@show')-&gt;where(['id' =&gt; '[0-9]+', 'name' =&gt; '[a-z]+']); &#125;); 将多个Route合并到一个组中，同个组中的Route共享路由组的属性，如：前缀，中间件和命名空间等其中：prefix 前缀：会追加到路由组中每个路由的URI前缀相当于 Route::get(‘/admin/user/{id}/{name}’, ‘UserController@show’); namespace namespace：省去了组里每个路由都需要加上命名空间，修改时也方便。该路会访问到\\App\\Admin\\Controllers\\UserController@show’。有一点需要注意，使用时需要把RouteServiceProvider里面的$namespace置为空，否则该值会追加在你的路由前面。 middleware 中间件：是从路由到控制器中间的一个层，可作数据校验和登陆验证等，web.php中默认有csrf等校验，可通过app/Http/Kernel.php中的$middlewareGroups查看。 参数绑定：参数可以用正则来约束，如果你希望路由参数在全局范围内都遵循一个确定的正则表达式约束，则可以在 RouteServiceProvider 的 boot 方法里定义Route::pattern(‘id’, ‘[0-9]+’); 2、 资源路由1Route::resource('user', 'UserController', ['only'=&gt;['index','create','store']]); 用artisan生成控制器时就可以直接生成对应的方法1php artisan make:controller UserController --resource 动作 URI 操作 路由名称 含义 GET /users index users.index 获取列表数据 GET /users/create create users.create 创建（显示表单） POST /users store users.store 保存创建建数据 GET /users/{id} show users.show 获取单个id资源数据 GET /users/{id}/edit edit users.edit 编辑（显示表单） PUT/PATCH /users/{id} update users.update 保存资源修改数据 DELETE /users/{id} destroy users.destroy 提交删除资源 注意： 由于HTML的form中不支持PUT、TATCH、DELETE提交，所以框架默认将”_method”名字的值作为请求方法，故表单提交时要注意是否需要加入下面元素1&lt;input type=\"hidden\" name=\"_method\" value=\"PUT\"&gt;","categories":[],"tags":[{"name":"Laravel","slug":"Laravel","permalink":"http://luckymartinlee.github.io/tags/Laravel/"},{"name":"PHP","slug":"PHP","permalink":"http://luckymartinlee.github.io/tags/PHP/"}]},{"title":"MySQL -- 索引 一 基本概念","slug":"mysql_1-3","date":"2013-05-22T10:45:50.000Z","updated":"2021-01-02T05:00:47.844Z","comments":true,"path":"2013/05/22/mysql_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2013/05/22/mysql_1-3/","excerpt":"","text":"数据库索引的概念常见索引类型MySQL常见索引有：主键索引、唯一索引、普通索引、全文索引、组合索引 普通索引：最基本的索引，没有任何限制。唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。主键索引：它是一种特殊的唯一索引，不允许有空值，在一张表中只能定义一个主键索引。主键用于唯一标识一条记录，使用关键字 PRIMARY KEY 来创建。主键分为复合主键和联合主键。全文索引：针对较大的数据，生成全文索引很耗时好空间。组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。 索引存储结构B+树：B+树数据结构以平衡树的形式来组织，因为是树型结构，所以更适合用来处理排序，范围查找等功能。相对 Hash ，B+树在查找单条记录的速度虽然比不上 Hash ，但是因为更适合排序等操作，所以他更受用户的欢迎。毕竟不可能只对数据库进行单条记录的操作. Hash：Hash 用的较少，它是把数据的索引以 Hash 形式组织起来，因此当查找某一条记录，速度非常快。缺点是 Hash 结构，每个键只对应一个值，分布方式是散列的。所以 Hash 并不支持范围查找和排序等功能. 数据库索引的增删改创建索引 执行 CREATE TABLE 语句时可以创建索引, 见MySQL从入门到放弃 – 语法1:数据库表操作。 使用 ALTER TABLE 语句时可以创建索引 1234mysql&gt;ALTER TABLE table_name ADD INDEX index_name (column_list) COMMENT '普通索引';mysql&gt;ALTER TABLE table_name ADD UNIQUE index_name (column_list) COMMENT '唯一索引';mysql&gt;ALTER TABLE table_name ADD PRIMARY KEY (column_list) COMMENT '主键';mysql&gt;ALTER TABLE table_name ADD FULLTEXT index_name (column_list) COMMENT '全文索引'; 使用 CREATE INDEX 语句时可以创建索引注: 不能用CREATE INDEX语句创建PRIMARY KEY索引 123mysql&gt;CREATE INDEX index_name ON table_name (column_list) COMMENT '普通索引';mysql&gt;CREATE UNIQUE INDEX index_name ON table_name (column_list) COMMENT '唯一索引;mysql&gt;CREATE FULLTEXT INDEX index_name ON table_name (column_list) COMMENT '全文索引'; 删除索引如删除多列组合索引的某列，则该列也会从索引中删除。如删除组合索引的所有列，则整个索引将被删除。 只使用 DORP 关键字1mysql&gt;DROP INDEX 索引名 ON 表名 列名; 使用 ALTER DORP 两个关键字123456mysql&gt;ALTER TABLE 表名 DROP INDEX 索引名 列名;mysql&gt;ALTER TABLE 表名 DROP UNIQUE 索引名 列名;// 因为一个表只可能有一个PRIMARY KEY索引，因此也可不指定索引名。// 如果没有创建PRIMARY KEY索引，但表具有一个或多个UNIQUE索引，则MySQL将删除第一个UNIQUE索引。mysql&gt;ALTER TABLE 表名 DROP PRIMARY KEY 索引名 列名; 重建索引长时间运行数据库后，索引有可能被损坏，这时就需要重建。重建索引可以提高检索效率。1mysql&gt;REPAIR TABLE table_name QUICK; 查询索引使用 INDEX 关键字1mysql&gt;SHOW INDEX FROM table_name; 使用 KEYS 关键字1mysql&gt;SHOW KEYS FROM table_name;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]},{"title":"MySQL -- 字段","slug":"mysql_1-2","date":"2013-05-20T02:37:45.000Z","updated":"2021-01-02T04:57:39.183Z","comments":true,"path":"2013/05/20/mysql_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2013/05/20/mysql_1-2/","excerpt":"","text":"新增字段新增字段时，关键词 COLUMN 可以省略 新增一个字段，默认值为空1ALTER TABLE `user` ADD COLUMN `new_feild1` VARCHAR(20) DEFAULT NULL; 新增一个字段，默认值为不能为空，备注 ‘备用字段’，新字段 放在 username 后1ALTER TABLE `user` ADD COLUMN `new_feild2` VARCHAR(20) NOT NULL COMMENT '备用字段' AFTER `username`;; 删除字段1ALTER TABLE `user` DROP COLUMN `new_feild1`; 修改字段修改字段有两种方式方式一 使用 MODIFY 关键字 (主要用于修改字段类型)修改类型为 VARCHAR(10)，默认值为 空字符串1ALTER TABLE `user` MODIFY `new_feild2` VARCHAR(10) DEFAULT '' COMMENT '修改字段'; 方式二 使用 CHANGE 关键字 (主要用于修改字段名称)修改字段名，并设为整型，默认值为 01ALTER TABLE `user` CHANGE `new_feild2` `new_feild3` INT DEFAULT 0 COMMENT '修改字段'; 何时使用 MODIFY ，何时使用 CHANGE ，其实无可厚非的，最主要是个人的习惯。 查询字段名和字段注释1SELECT `COLUMN_NAME`,`COLUMN_COMMENT`,`COLUMN_TYPE` FROM `information_schema`.`COLUMNS` WHERE TABLE_NAME='表名' AND TABLE_SCHEMA='数据库名';","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]},{"title":"MySQL -- 库 与 表","slug":"mysql_1-1","date":"2013-05-10T09:13:12.000Z","updated":"2021-01-02T06:02:47.454Z","comments":true,"path":"2013/05/10/mysql_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2013/05/10/mysql_1-1/","excerpt":"","text":"数据库的创建与删除创建数据库注: CREATE DATABASE 不支持 comment。 COLLATE: 校对集,在某个字符集的情况下，字符集的排列顺序应该是什么，称之为校对集。1CREATE DATABASE IF NOT EXISTS `my_db` DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 删除数据库1DROP DATABASE IF EXIST `my_db`; 查看数据库列表1SHOW DATABASES; 查看数据库DDL1SHOW CREATE DATABASE `my_db`; 查看当前使用的数据库1SELECT database(); 切换数据库1USE `my_db2`; 数据库表的增删改创建数据库表注: CHAR列的长度固定为创建表时声明的长度。长度可以为从0到255的任何值。当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉，所以，我们在存储时字符串右边不能有空格，即使有，查询出来后也会被删除。VARCHAR列中的值为可变长字符串, 同CHAR对比，VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则使用两个字节),VARCHAR值保存时不进行填充。当值保存和检索时尾部的空格仍保留，符合标准SQL. BINARY和VARBINARY类型类似于CHAR和VARCHAR类型，但是不同的是，它们存储的不是字符字符串，而是二进制串. 保存BINARY值时，在它们右边填充0x00(零字节)值以达到指定长度。取值时不删除尾部的字节; 对于VARBINARY，插入时不填充字符，选择时不裁剪字节. 长度限制:CHAR、VARCAHR的长度是指字符的长度，例如CHAR[3]则只能放字符串”123”，如果插入数据”1234”，则从高位截取，变为”123”。 VARCAHR同理。TINYINT、SMALLINT、MEDIUMINT、INT和BIGINT的长度，其实和数据的大小无关！Length指的是显示宽度;FLOAT、DOUBLE和DECIMAL的长度指的是全部数位（包括小数点后面的），例如DECIMAL(4,1)指的是全部位数为4，小数点后1位，如果插入1234，则查询的数据是999.9; BLOB, TEXT, GEOMETRY or JSON 不能有默认值. FULLTEXT 索引仅可用于 MyISAM 表, 为 CHAR, VARCHAR, TEXT 列 创建全文索引. 123456789101112131415161718# 1. 一般create table 语句CREATE TABLE IF NOT EXISTS `users` ( `id` INT(11) UNSIGNED NOT NULL [PRIMARY KEY] AUTO_INCREMENT COMMENT '用户id', `name` VARCHAR(20) NOT NULL DEFAULT '' COMMENT '姓名', `sex` TINYINT(1) UNSIGNED NOT NULL DEFAULT 1 COMMENT '性别', `age` TINYINT(2) UNSIGNED NOT NULL DEFAULT 1 COMMENT '年龄', `address` TEXT(500) NOT NULL COMMENT '地址', PRIMARY KEY (`id`) COMMENT '主键', INDEX `age_idx` (`age`) COMMENT '普通索引', UNIQUE `name_idx`(`name`(20)) COMMENT '唯一索引', FULLTEXT `address` (`address`) COMMENT '全文索引') ENGINE = MyISAM DEFAULT CHARSET utf8 COLLATE utf8_general_ci AUTO_INCREMENT=1 COMMENT '用户信息表';# 2. create table like 参照已有表的定义，来定义新的表CREATE TABLE IF NOT EXISTS `users_like` LIKE `users`;# 3. 根据select 的结果集来创建表CREATE TABLE IF NOT EXISTS `user_select` AS SELECT `id`, `name` FROM `users`; 删除数据库表1DROP DATABASE `users`; 修改表自增起始ID1ALTER TABLE `users` AUTO_INCREMENT=2; 查看数据库表的列表1SHOW TABLES;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"}]}]}