<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Martin Li&#39;s Personal Website - 李杨的个人站点</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://luckymartinlee.github.io/"/>
  <updated>2020-12-07T12:52:37.204Z</updated>
  <id>http://luckymartinlee.github.io/</id>
  
  <author>
    <name>Martin Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>压力测试工具 ab (apache bench) 使用</title>
    <link href="http://luckymartinlee.github.io/2019/05/10/ab_1-1/"/>
    <id>http://luckymartinlee.github.io/2019/05/10/ab_1-1/</id>
    <published>2019-05-10T10:13:12.000Z</published>
    <updated>2020-12-07T12:52:37.204Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络服务性能相关概念"><a href="#网络服务性能相关概念" class="headerlink" title="网络服务性能相关概念"></a>网络服务性能相关概念</h2><h3 id="服务器平均请求处理时间（Time-per-request-across-all-concurrent-requests）"><a href="#服务器平均请求处理时间（Time-per-request-across-all-concurrent-requests）" class="headerlink" title="服务器平均请求处理时间（Time per request: across all concurrent requests）"></a>服务器平均请求处理时间（Time per request: across all concurrent requests）</h3><p>即是在某个并发用户数下服务器处理一条请求的平均时间</p><p>服务器平均请求处理时间 = 处理完成所有请求数所花费的时间 / 总请求数<br>Time per request(across all concurrent requests) = Time taken for / testsComplete requests</p><h3 id="用户平均请求等待时间（Time-per-request）"><a href="#用户平均请求等待时间（Time-per-request）" class="headerlink" title="用户平均请求等待时间（Time per request）"></a>用户平均请求等待时间（Time per request）</h3><p>即是用户获得相应的平均等待时间</p><p>用户平均请求等待时间 = 处理完成所有请求数所花费的时间/ （总请求数 / 并发用户数），即<br>Time per request = Time taken for tests /（ Complete requests / Concurrency Level）</p><h3 id="吞吐率（Requests-per-second，QPS，RPS）"><a href="#吞吐率（Requests-per-second，QPS，RPS）" class="headerlink" title="吞吐率（Requests per second，QPS，RPS）"></a>吞吐率（Requests per second，QPS，RPS）</h3><p>即是在某个并发用户数下单位时间内处理的请求数,单位是reqs/s, 也是 “服务器平均请求处理时间” d的倒数。某个并发用户数下单位时间内能处理的最大请求数，称之为最大吞吐率。</p><p>吞吐率 = 总请求数 / 处理完成这些请求数所花费的时间<br>Request per second = Complete requests / Time taken for tests</p><h3 id="并发连接数（The-number-of-concurrent-connections）"><a href="#并发连接数（The-number-of-concurrent-connections）" class="headerlink" title="并发连接数（The number of concurrent connections）"></a>并发连接数（The number of concurrent connections）</h3><p>即是在某个时刻服务器所接受的请求数目，就是会话数量。</p><h3 id="并发用户数（The-number-of-concurrent-users，Concurrency-Level）"><a href="#并发用户数（The-number-of-concurrent-users，Concurrency-Level）" class="headerlink" title="并发用户数（The number of concurrent users，Concurrency Level）"></a>并发用户数（The number of concurrent users，Concurrency Level）</h3><p>即是指某个时刻使用系统的用户数(可能有一个用户有一个或者多个连接)。<br>要注意区分和并发连接数之间的区别，一个用户可能同时会产生多个会话，也即连接数。</p><p>总结:<br>如果要说QPS时，一定需要指明是多少并发用户数下的QPS，否则豪无意义。<br>因为单用户数的40QPS和20并发用户数下的40QPS是两个不同的概念，前者说明该服务可以在一秒内串行执行40个请求，而后者说明在并发20个请求的情况下，一秒内该应用能处理40个请求。<br>当QPS相同时，越大的并发用户数，说明了网站服务并发处理能力越好。<br>对于当前的web服务器，其处理单个用户的请求肯定戳戳有余，这个时候会存在资源浪费的情况（一方面该服务器可能有多个cpu，但是只处理单个进程，另一方面，在处理一个进程中，有些阶段可能是IO阶段，这个时候会造成CPU等待，但是有没有其他请求进程可以被处理）。<br>而当并发数设置的过大时，每秒钟都会有很多请求需要处理，会造成进程（线程）频繁切换，反正真正用于处理请求的时间变少，每秒能够处理的请求数反而变少，同时用户的请求等待时间也会变大，甚至超过用户的心理底线，等待时间过长。</p><p>所以在最小并发数和最大并发数之间，一定有一个最合适的并发数值，在并发数下，QPS能够达到最大。</p><p>但是，这个并发并非是一个最佳的并发，因为当QPS到达最大时的并发，可能已经造成用户的等待时间变得超过了其最优值，所以对于一个系统，其最佳的并发数，一定需要结合QPS，用户的等待时间来综合确定。</p><p>下面这张图是应用服务器关于并发用户数，QPS，用户平均等待时间的一张关系图，对于实际的系统，也应该是对于不同的并发数，进行多次测试，获取到这些数值后，画出这样一张图出来，以便于分析出系统的最佳并发用户数。</p><p><img src="/post_imgs/server_performance_relations_1.png" alt=""><br>响应时间关系图</p><h2 id="ab-简介和使用实例"><a href="#ab-简介和使用实例" class="headerlink" title="ab 简介和使用实例"></a>ab 简介和使用实例</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>ab全称为：apache bench<br>它是apache自带的压力测试工具。ab非常实用，它不仅可以对apache服务器进行网站访问压力测试，也可以对或其它类型的服务器进行压力测试。比如nginx、tomcat、IIS等。<br>我们可以直接安装apache的工具包httpd-tools。如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install httpd-tools</span><br></pre></td></tr></table></figure></p><p>使用ab –V命令检测是否安装成功。如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ab -V</span><br></pre></td></tr></table></figure></p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ab -n 100 -c 10 -H <span class="string">"token: fioj3iorm2aoi4ej"</span> -p /data/postdata.txt -T application/x-www-form-urlencoded <span class="string">"http://127.0.0.1/test"</span></span><br></pre></td></tr></table></figure><p>上面命令的含义是<br>设置请求header中参数token = fioj3iorm2aoi4ej，向 <a href="http://127.0.0.1/test" target="_blank" rel="noopener">http://127.0.0.1/test</a> 地址发送POST请求，POST表单数据存放在<br>本地文件/data/postdata.txt中，其content-type格式为  application/x-www-form-urlencoded，发送的并发请求数为 10，总请求数为 100。<br>成功后返回：<br><img src="/post_imgs/ab_result.png" alt=""></p><h3 id="测试结果含义"><a href="#测试结果含义" class="headerlink" title="测试结果含义"></a>测试结果含义</h3><table><thead><tr><th>返回值名称</th><th>含义</th></tr></thead><tbody><tr><td>Server Software</td><td>web服务器软件及版本</td></tr><tr><td>Server Hostname</td><td>表示请求的URL中的主机部分名称</td></tr><tr><td>Server Port</td><td>被测试的Web服务器的监听端口</td></tr><tr><td>Document Path</td><td>请求的页面路径</td></tr><tr><td>Document Length</td><td>页面大小</td></tr><tr><td>Concurrency Level</td><td>并发请求数</td></tr><tr><td>Time taken for tests</td><td>整个测试持续的时间,测试总共花费的时间</td></tr><tr><td>Complete requests</td><td>完成的请求数</td></tr><tr><td>Failed requests</td><td>失败的请求数，这里的失败是指请求的连接服务器、发送数据、接收数据等环节发生异常，以及无响应后超时的情况。对于超时时间的设置可以用ab的-t参数。如果接受到的http响应数据的头信息中含有2xx以外的状态码，则会在测试结果显示另一个名为“Non-2xx responses”的统计项，用于统计这部分请求数，这些请求并不算是失败的请求。</td></tr><tr><td>Write errors</td><td>写入错误</td></tr><tr><td>Total transferred</td><td>总共传输字节数,整个场景中的网络传输量,包含http的头信息等。使用ab的-v参数即可查看详细的http头信息。</td></tr><tr><td>HTML transferred</td><td>html字节数，整个场景中的HTML内容传输量。也就是减去了Total transferred中http响应数据中头信息的长度。</td></tr><tr><td><strong>Requests per second</strong></td><td>每秒处理的请求数，服务器的吞吐量，大家最关心的指标之一</td></tr><tr><td><strong>Time per request</strong></td><td>用户平均请求等待时间，大家最关心的指标之二</td></tr><tr><td><strong>Time per request</strong></td><td>服务器平均处理时间，大家最关心的指标之三</td></tr><tr><td>Transfer rate</td><td>平均传输速率（每秒收到的速率）平均每秒网络上的流量，可以很好的说明服务器在处理能力达到限制时，其出口带宽的需求量，也可以帮助排除是否存在网络流量过大导致响应时间延长的问题。</td></tr></tbody></table><p>下面段表示网络上消耗的时间的分解<br><img src="/post_imgs/ab_results_2.png" alt=""></p><p>下面这段是每个请求处理时间的分布情况，50%的处理时间在4930ms内，66%的处理时间在5008ms内…，重要的是看90%的处理时间。<br><img src="/post_imgs/ab_results_3.png" alt=""></p><h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><p>压力测试需要当登录怎么办？<br>1、先用账户和密码在浏览器登录后，用开发者工具找到会话的Cookie值（Session ID）记下来。<br>2、使用下面命令传入Cookie值<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ab －n 100 －C key＝value http://127.0.0.1/<span class="built_in">test</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络服务性能相关概念&quot;&gt;&lt;a href=&quot;#网络服务性能相关概念&quot; class=&quot;headerlink&quot; title=&quot;网络服务性能相关概念&quot;&gt;&lt;/a&gt;网络服务性能相关概念&lt;/h2&gt;&lt;h3 id=&quot;服务器平均请求处理时间（Time-per-request-acro
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://luckymartinlee.github.io/tags/Linux/"/>
    
      <category term="Shell" scheme="http://luckymartinlee.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>Spark 算子详解</title>
    <link href="http://luckymartinlee.github.io/2019/05/10/spark_1-2/"/>
    <id>http://luckymartinlee.github.io/2019/05/10/spark_1-2/</id>
    <published>2019-05-10T10:13:12.000Z</published>
    <updated>2020-12-09T08:49:01.150Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算子分类"><a href="#算子分类" class="headerlink" title="算子分类"></a>算子分类</h2><p>1、 Transformation算子(转换算子)，此类算子操作是延迟计算的，即是将要从一个RDD转换成另一个RDD的操作，但不是马上执行，并不触发提交Job作业，需要等到有Action操作的时候才会真正触发运算。<br>根据操作数据类型的不同，可细分为 Value数据类型的Transformation算子 和 Key-Value数据类型的Transfromation算子<br>2、Action算子(行动算子), 这类算子会触发 SparkContext 提交Job作业，并将数据输出 Spark系统。</p><h2 id="重难-Transformation算子"><a href="#重难-Transformation算子" class="headerlink" title="重难 Transformation算子"></a>重难 Transformation算子</h2><p>glom<br>该函数是将RDD中每一个分区中各个元素合并成一个Array，这样每一个分区就只有一个数组元素<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">a.glom.collect</span><br><span class="line">//输出</span><br><span class="line">res66: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9))</span><br></pre></td></tr></table></figure></p><p>mapPartitions(function)<br>与 map 类似，但函数单独在 RDD 的每个分区上运行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val list = List(1, 2, 3, 4, 5, 6)</span><br><span class="line">sc.parallelize(list, 3).mapPartitions(iterator =&gt; &#123;</span><br><span class="line">  val buffer = new ListBuffer[Int]</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    buffer.append(iterator.next() * 100)</span><br><span class="line">  &#125;</span><br><span class="line">  buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br><span class="line">//输出结果</span><br><span class="line">100 200 300 400 500 600</span><br></pre></td></tr></table></figure></p><p>join<br>在一个(K, V)和(K, W)类型的RDD 上调用时，返回一个(K, (V, W)) pairs 的 RDD，等价于内连接操作(不含 V或W 为空的)。<br>执行外连接，可以使用：<br>leftOuterJoin (不含 W 为空)<br>rightOuterJoin  (不含 W 为空)<br>fullOuterJoin (包含V 和 W 为空)</p><p>sample<br>数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction,小于等于1)、随机数生成器的种子 (seed)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val list = List(1, 2, 3, 4, 5, 6)</span><br><span class="line">sc.parallelize(list).sample(withReplacement = <span class="literal">false</span>, fraction = 0.5).foreach(println)</span><br><span class="line">//输出结果随机</span><br></pre></td></tr></table></figure></p><p>groupByKey<br>按照键进行分组,在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable<v>)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val list = List((<span class="string">"hadoop"</span>, 2), (<span class="string">"spark"</span>, 3), (<span class="string">"spark"</span>, 5), (<span class="string">"storm"</span>, 6), (<span class="string">"hadoop"</span>, 2))</span><br><span class="line">sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)</span><br><span class="line">//输出：</span><br><span class="line">(spark,List(3, 5))</span><br><span class="line">(hadoop,List(2, 2))</span><br><span class="line">(storm,List(6))</span><br></pre></td></tr></table></figure></v></p><p>cogroup<br>先同一个 (K, V) RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val list01 = List((1, <span class="string">"a"</span>),(1, <span class="string">"a"</span>), (2, <span class="string">"b"</span>), (3, <span class="string">"e"</span>))</span><br><span class="line">val list02 = List((1, <span class="string">"A"</span>), (2, <span class="string">"B"</span>), (3, <span class="string">"E"</span>))</span><br><span class="line">val list03 = List((1, <span class="string">"[ab]"</span>), (2, <span class="string">"[bB]"</span>), (3, <span class="string">"eE"</span>),(3, <span class="string">"eE"</span>))</span><br><span class="line">sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)</span><br><span class="line">// 输出</span><br><span class="line">(1,(CompactBuffer(a, a),CompactBuffer(A),CompactBuffer([ab])))</span><br><span class="line">(3,(CompactBuffer(e),CompactBuffer(E),CompactBuffer(eE, eE)))</span><br><span class="line">(2,(CompactBuffer(b),CompactBuffer(B),CompactBuffer([bB]))</span><br></pre></td></tr></table></figure></p><p>reduceByKey<br>按照键进行归约操作<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val list = List((<span class="string">"hadoop"</span>, 2), (<span class="string">"spark"</span>, 3), (<span class="string">"spark"</span>, 5), (<span class="string">"storm"</span>, 6), (<span class="string">"hadoop"</span>, 2))</span><br><span class="line">sc.parallelize(list).reduceByKey(_ + _).foreach(println)</span><br><span class="line">//输出</span><br><span class="line">(spark,8)</span><br><span class="line">(hadoop,4)</span><br><span class="line">(storm,6)</span><br></pre></td></tr></table></figure></p><p>sortBy(function) &amp; sortByKey<br>按照键进行排序，需要 collect 等action算子后才是有序的<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val list01 = List((100, <span class="string">"hadoop"</span>), (90, <span class="string">"spark"</span>), (120, <span class="string">"storm"</span>))</span><br><span class="line">sc.parallelize(list01).sortByKey(ascending = <span class="literal">false</span>).collect.foreach(println)</span><br><span class="line">// 输出</span><br><span class="line">(120,storm)</span><br><span class="line">(100,hadoop)</span><br><span class="line">(90,spark)</span><br></pre></td></tr></table></figure></p><p>按照指定function进行排序,需要 collect 等action算子后才是有序的<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val list02 = List((<span class="string">"hadoop"</span>,100), (<span class="string">"spark"</span>,90), (<span class="string">"storm"</span>,120))</span><br><span class="line">sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=<span class="literal">false</span>).collect.foreach(println)</span><br><span class="line">// 输出</span><br><span class="line">(storm,120)</span><br><span class="line">(hadoop,100)</span><br><span class="line">(spark,90)</span><br></pre></td></tr></table></figure></p><p>aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])<br>当针对(K，V)对的数据集时，返回（K，U）对的数据集，先对分区内执行seqOp函数，zeroValue 聚合每个键的值，再对分区间执行combOp函数。与groupByKey 类似，reduce 任务的数量可通过第二个参数 numPartitions 进行配置。示例如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 为了清晰，以下所有参数均使用具名传参</span><br><span class="line">val list = List((<span class="string">"hadoop"</span>, 3), (<span class="string">"hadoop"</span>, 2), (<span class="string">"spark"</span>, 4), (<span class="string">"spark"</span>, 3), (<span class="string">"storm"</span>, 6), (<span class="string">"storm"</span>, 8))</span><br><span class="line">sc.parallelize(list,numSlices = 2).aggregateByKey(zeroValue = 0,numPartitions = 3)(</span><br><span class="line">      seqOp = math.max(_, _),</span><br><span class="line">      combOp = _ + _</span><br><span class="line">    ).collect.foreach(println)</span><br><span class="line">//输出结果：</span><br><span class="line">(hadoop,3)</span><br><span class="line">(storm,8)</span><br><span class="line">(spark,7)</span><br></pre></td></tr></table></figure></p><p>这里使用了 numSlices = 2 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下：<br><img src="/post_imgs/spark_1.png" alt=""></p><p>基于同样的执行流程，如果 numSlices = 1，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(hadoop,3)</span><br><span class="line">(storm,8)</span><br><span class="line">(spark,4)</span><br></pre></td></tr></table></figure></p><p>同样的，如果每个单词对一个分区，即 numSlices = 6，此时相当于求和操作，执行结果为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(hadoop,5)</span><br><span class="line">(storm,14)</span><br><span class="line">(spark,7)</span><br></pre></td></tr></table></figure></p><p>aggregateByKey(zeroValue = 0,numPartitions = 3) 的第二个参数 numPartitions 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 getNumPartitions 方法获取分区数量</p><p>combineByKey<a href="createCombiner:(V" target="_blank" rel="noopener">C</a> C,<br>　　mergeValue:(C, V) C,<br>　　mergeCombiners:(C, C) C,<br>　　partitioner:Partitioner,<br>　　mapSideCombine:Boolean=true,<br>　　serializer:Serializer=null<br>):RDD[(K,C)] </p><p>参数：createCombiner:V=&gt;C　　分组内的创建组合的函数。即是对读进来的数据进行初始化，其把当前的值作为参数，可以对该值做一些转换操作，转换为我们想要的数据格式<br>参数：mergeValue:(C,V)=&gt;C　　该函数主要是分区内的合并函数，作用在每一个分区内部。其功能主要是将V合并到之前(createCombiner)的元素C上,注意，这里的C指的是上一函数转换之后的数据格式，而这里的V指的是原始数据格式(上一函数为转换之前的)<br>参数：mergeCombiners:(C,C)=&gt;R　　该函数主要是进行分区之间合并，此时是将两个C合并为一个C，例如两个C:(Int)进行相加之后得到一个R:(Int)<br>参数：partitioner:自定义分区数，默认是hashPartitioner<br>参数：mapSideCombine:Boolean=true　　该参数是设置是否在map端进行combine操作，为了减小传输量，很多 combine 可以在 map 端先做，比如叠加，可以先在一个 partition 中把所有相同的 key 的 value 叠加，<br>参数：serializerClass： String = null，传输需要序列化，用户可以自定义序列化类</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val ls3 = List((<span class="string">"001"</span>, <span class="string">"011"</span>), (<span class="string">"001"</span>,<span class="string">"012"</span>), (<span class="string">"002"</span>, <span class="string">"011"</span>), (<span class="string">"002"</span>, <span class="string">"013"</span>), (<span class="string">"002"</span>, <span class="string">"014"</span>))</span><br><span class="line">val d1 = sc.parallelize(ls3,2)</span><br><span class="line">d1.combineByKey(</span><br><span class="line">(v: (String)) =&gt; (v, 1),</span><br><span class="line">(acc: (String, Int),v: (String)) =&gt; (v+<span class="string">":"</span>+acc._1,acc._2+1),</span><br><span class="line">(p1:(String,Int),p2:(String,Int)) =&gt; (p1._1 + <span class="string">":"</span> + p2._1,p1._2 + p2._2)</span><br><span class="line">).collect().foreach(println)</span><br><span class="line">//输出</span><br><span class="line">(002,(014:013:011,3))</span><br><span class="line">(001,(012:011,2))</span><br></pre></td></tr></table></figure><h2 id="重难-Action算子"><a href="#重难-Action算子" class="headerlink" title="重难 Action算子"></a>重难 Action算子</h2><p>takeOrdered<br>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 n 个元素。需要注意的是 takeOrdered 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 Ordering[T] 实现自定义比较器，然后将其作为隐式参数引入。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span><br><span class="line">class CustomOrdering extends Ordering[(Int, String)] &#123;</span><br><span class="line">    override def compare(x: (Int, String), y: (Int, String)): Int</span><br><span class="line">    = <span class="keyword">if</span> (x._2.length &gt; y._2.length) 1 <span class="keyword">else</span> -1</span><br><span class="line">&#125;</span><br><span class="line">val list = List((1, <span class="string">"hadoop"</span>), (1, <span class="string">"storm"</span>), (1, <span class="string">"azkaban"</span>), (1, <span class="string">"hive"</span>))</span><br><span class="line">//  引入隐式默认值</span><br><span class="line">implicit val implicitOrdering = new CustomOrdering</span><br><span class="line">sc.parallelize(list).takeOrdered(5)</span><br><span class="line">//输出</span><br><span class="line">Array((1,hive), (1,storm), (1,hadoop), (1,azkaban)</span><br></pre></td></tr></table></figure></p><p>take(n)<br>将RDD中的前 n 个元素作为一个 array 数组返回,是无序的。</p><p>first<br>返回 RDD 中的第一个元素，等价于 take(1)。</p><p>top（num：Int）（implicit ord：Ordering[T]）：Array[T]<br>默认返回最大的k个元素，可以定义排序的方式Ordering[T]。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class CustomOrdering extends Ordering[Int] &#123;</span><br><span class="line">    override def compare(x: Int, y: Int): Int</span><br><span class="line">    = <span class="keyword">if</span> (x &gt; y) -1 <span class="keyword">else</span> 1</span><br><span class="line">&#125;</span><br><span class="line">val list0 = List(3,5,1,6,2)</span><br><span class="line">//  引入隐式默认值</span><br><span class="line">implicit val implicitOrdering = new CustomOrdering</span><br><span class="line">sc.parallelize(list0).top(5)</span><br><span class="line">//输出</span><br><span class="line">Array(1,2,3,5,6)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;算子分类&quot;&gt;&lt;a href=&quot;#算子分类&quot; class=&quot;headerlink&quot; title=&quot;算子分类&quot;&gt;&lt;/a&gt;算子分类&lt;/h2&gt;&lt;p&gt;1、 Transformation算子(转换算子)，此类算子操作是延迟计算的，即是将要从一个RDD转换成另一个RDD的操作，
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://luckymartinlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Spark" scheme="http://luckymartinlee.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark RDD的Stage划分</title>
    <link href="http://luckymartinlee.github.io/2018/10/13/spark_1-1/"/>
    <id>http://luckymartinlee.github.io/2018/10/13/spark_1-1/</id>
    <published>2018-10-13T02:23:11.000Z</published>
    <updated>2020-12-10T01:00:21.602Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>RDD(resilient distributed dataset) 弹性分布式数据集,RDD表示一个不可变的、可分区的、支持并行计算的元素集合（类似于 Scala 中的不可变集合），RDD可以通过 HDFS、Scala集合、RDD转换、外部的数据集（支持InputFormat）获取，<br>并且 Spark 将 RDD 存储在内存中，可以非常高效的重复利用或者在某些计算节点故障时自动数据恢复。</p><h2 id="RDD依赖-lineage-血统"><a href="#RDD依赖-lineage-血统" class="headerlink" title="RDD依赖 - lineage(血统)"></a>RDD依赖 - lineage(血统)</h2><p>在对RDD应用转换操作时，产生的新 RDD 对旧 RDD 会有一种依赖关系称为 Lineage(血统).<br>Spark应用在计算时会根据 Lineage 逆向推导出所有Stage（阶段），每一个 Stage 的分区数量决定了任务的并行度，一个 Stage 实现任务的本地计算（大数据计算时网络传输时比较耗时的.</p><p>RDD 两种 Lineage 关系，宽窄依赖,v它们和Stage划分有极为紧密关系<br>窄依赖 (Narrow Dependency): 父RDD的一个分区对应一个子RDD的分区（1:1）或者多个父RDD的分区对应一个子RDD的分区（N：1）.</p><p>宽依赖 (Wide Dependency): 父RDD的一个分区对应多个子RDD的分区（1：N）.<br><img src="/post_imgs/spark_1_1.png" alt=""><br><img src="/post_imgs/spark_1_2.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是RDD&quot;&gt;&lt;a href=&quot;#什么是RDD&quot; class=&quot;headerlink&quot; title=&quot;什么是RDD&quot;&gt;&lt;/a&gt;什么是RDD&lt;/h2&gt;&lt;p&gt;RDD(resilient distributed dataset) 弹性分布式数据集,RDD表示一个不可变
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://luckymartinlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Spark" scheme="http://luckymartinlee.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop从入门到放弃(四) -- YARN</title>
    <link href="http://luckymartinlee.github.io/2018/06/22/hadoop-1-4/"/>
    <id>http://luckymartinlee.github.io/2018/06/22/hadoop-1-4/</id>
    <published>2018-06-22T07:15:21.000Z</published>
    <updated>2020-12-07T12:52:05.467Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是-YARN"><a href="#什么是-YARN" class="headerlink" title="什么是 YARN"></a>什么是 YARN</h2><p>YARN 是 Hadoop2.0 以后的资源管理器，负责整个集群资源的管理和调度</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h3><p>负责:</p><ol><li>分配和调度资源</li><li>启动并监控 ApplicationMaster</li><li>监控 NodeManager</li></ol><h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>负责:</p><ol><li>为 MapReduce 类型程序申请资源，并分配给内部任务</li><li>负责数据的切分</li><li>监控任务的执行以及容错</li></ol><h3 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h3><p>负责:</p><ol><li>管理单个节点</li><li>处理来自 ResouceManager 的命令</li><li>处理来自 ApplicationMaster 的命令</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是-YARN&quot;&gt;&lt;a href=&quot;#什么是-YARN&quot; class=&quot;headerlink&quot; title=&quot;什么是 YARN&quot;&gt;&lt;/a&gt;什么是 YARN&lt;/h2&gt;&lt;p&gt;YARN 是 Hadoop2.0 以后的资源管理器，负责整个集群资源的管理和调度&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://luckymartinlee.github.io/tags/Hadoop/"/>
    
      <category term="大数据" scheme="http://luckymartinlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="YARN" scheme="http://luckymartinlee.github.io/tags/YARN/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop从入门到放弃(三) -- MapReduce</title>
    <link href="http://luckymartinlee.github.io/2018/06/12/hadoop-1-3/"/>
    <id>http://luckymartinlee.github.io/2018/06/12/hadoop-1-3/</id>
    <published>2018-06-12T07:15:40.000Z</published>
    <updated>2020-12-07T12:51:53.831Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MapReduce-编程模型"><a href="#MapReduce-编程模型" class="headerlink" title="MapReduce 编程模型"></a>MapReduce 编程模型</h2><p>举个栗子：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入一个大文件，通过Split切分后，将其分成多个分片，</span><br><span class="line">每个文件分片，由单独的机器去处理，这就是 Map 方法，</span><br><span class="line">然后，将各个机器的计算结果进行汇总并得到最终的结果，这就是 Reduce 方法。</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MapReduce-编程模型&quot;&gt;&lt;a href=&quot;#MapReduce-编程模型&quot; class=&quot;headerlink&quot; title=&quot;MapReduce 编程模型&quot;&gt;&lt;/a&gt;MapReduce 编程模型&lt;/h2&gt;&lt;p&gt;举个栗子：&lt;br&gt;&lt;figure class
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://luckymartinlee.github.io/tags/Hadoop/"/>
    
      <category term="大数据" scheme="http://luckymartinlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="MapReduce" scheme="http://luckymartinlee.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MySQL查询优化(一)</title>
    <link href="http://luckymartinlee.github.io/2018/05/22/mysql-2-1/"/>
    <id>http://luckymartinlee.github.io/2018/05/22/mysql-2-1/</id>
    <published>2018-05-22T10:45:50.000Z</published>
    <updated>2018-05-25T03:31:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="索引建立技巧"><a href="#索引建立技巧" class="headerlink" title="索引建立技巧"></a>索引建立技巧</h2><h3 id="单个字段-“等于”-查询"><a href="#单个字段-“等于”-查询" class="headerlink" title="单个字段 “等于” 查询"></a>单个字段 “等于” 查询</h3><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` = 15</span><br></pre></td></tr></table></figure></p><p>这种情况下，毫无疑问，需要给字段 (f1) 加上索引。</p><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT `f2`,`f3` FROM `tabel1` WHERE `f1` = 15</span><br></pre></td></tr></table></figure></p><p>此时应该创建 (f1,f2,f3) 索引，此索引起到覆盖索引的作用，效率比 (f1) 索引高。切记，不应该创建 (f2,f3,f1) 索引，因为根据索引 “最左原则”，它对 f1 字段起不到查询过滤作用。</p><h3 id="多个字段-“等于”-查询"><a href="#多个字段-“等于”-查询" class="headerlink" title="多个字段 “等于” 查询"></a>多个字段 “等于” 查询</h3><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` = 15 AND `f2` = <span class="string">"abc"</span></span><br></pre></td></tr></table></figure></p><p>这种情况下，应该创建 (f1,f2)索引 或者 (f2,f1)索引 都是可以的。<br>有人会问，如果创建两个单独是索引，分别是 (f1)索引 和 (f2)索引 可以吗？这里不建议这么做，虽然MySQL根据index_merge算法能同时使用这两个索引，但这样效率依旧不如上面联合索引。</p><h3 id="字段-“等于”-和-“不等于”-混合查询"><a href="#字段-“等于”-和-“不等于”-混合查询" class="headerlink" title="字段 “等于” 和 “不等于” 混合查询"></a>字段 “等于” 和 “不等于” 混合查询</h3><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f2` = <span class="string">"abc"</span></span><br></pre></td></tr></table></figure></p><p>对于这种情况，我们要小心处理，因为只要有一列使用了不等于计算，那么它将阻止其他列使用索引。<br>此时我们应该创建 (f2,f1) 索引，这时候f1和f2两个条件都会走索引，这才是我们想要的。而不是 (f1,f2) 索引，这种情况下，只有 f1 会使用索引，相对来说效率较低。</p><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f3` &lt; 100 AND `f2` = <span class="string">"abc"</span></span><br></pre></td></tr></table></figure></p><p>这是有两个 “不等于” 查询，因此我们不可能做到 f1,f2,f3都做到被索引覆盖，<br>此时需要依据实际数据情况，建立 (f2,f1)索引 或 (f2,f3)索引，其中关键是，一定要把 “等于” 字段，放在索引的最左侧。</p><h3 id="多个字段-“等于”-和-“排序”-查询"><a href="#多个字段-“等于”-和-“排序”-查询" class="headerlink" title="多个字段 “等于” 和 “排序” 查询"></a>多个字段 “等于” 和 “排序” 查询</h3><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` = 15 AND `f2` = <span class="string">"abc"</span> ORDER BY `f3`</span><br></pre></td></tr></table></figure></p><p>此时我们建立索引的字段顺序，应该是: “先是过滤字段，后是排序字段”，所以此处应该建立 (f1,f2,f3)索引 或者 (f2,f1,f3)索引。而不应该 建立 (f3,f1,f2)索引 或者 (f3,f2,f1)索引，因为这些只使用了索引排序，没有使用索引过滤。</p><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT `f4`,`f5` FROM `tabel1` WHERE `f1` = 15 AND `f2` = <span class="string">"abc"</span> ORDER BY `f3`</span><br></pre></td></tr></table></figure></p><p>此时我们可以创建 (f1,f2,f3,f4,f5)索引 或 (f2,f1,f3,f4,f5)索引，起到了 过滤，排序和覆盖 三个作用。</p><h3 id="字段-“不等于”-和-“排序”-查询"><a href="#字段-“不等于”-和-“排序”-查询" class="headerlink" title="字段 “不等于” 和 “排序” 查询"></a>字段 “不等于” 和 “排序” 查询</h3><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f2` = <span class="string">"abc"</span> ORDER BY `f3`</span><br></pre></td></tr></table></figure></p><p>此时，需要根据实际数据情况，选择建立 (f2,f1)索引 或 (f2,f3)索引</p><p>形如:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM `tabel1` WHERE `f1` &gt; 15 ORDER BY `f3`</span><br></pre></td></tr></table></figure></p><p>此时，只可能一个字段使用到索引，要么使用 (f1)索引，要么使用 (f2)索引，这要依据集体的数据情况，一般情况下会使用过滤索引，也就是 (f1)索引。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;索引建立技巧&quot;&gt;&lt;a href=&quot;#索引建立技巧&quot; class=&quot;headerlink&quot; title=&quot;索引建立技巧&quot;&gt;&lt;/a&gt;索引建立技巧&lt;/h2&gt;&lt;h3 id=&quot;单个字段-“等于”-查询&quot;&gt;&lt;a href=&quot;#单个字段-“等于”-查询&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="MySQL" scheme="http://luckymartinlee.github.io/tags/MySQL/"/>
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop从入门到放弃(二) -- HDFS</title>
    <link href="http://luckymartinlee.github.io/2018/05/19/hadoop-1-2/"/>
    <id>http://luckymartinlee.github.io/2018/05/19/hadoop-1-2/</id>
    <published>2018-05-19T07:15:45.000Z</published>
    <updated>2020-12-07T12:52:08.189Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Git 分支策略</title>
    <link href="http://luckymartinlee.github.io/2018/04/28/git-1-1/"/>
    <id>http://luckymartinlee.github.io/2018/04/28/git-1-1/</id>
    <published>2018-04-28T02:46:43.000Z</published>
    <updated>2018-05-19T02:45:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常驻分支（主分支）"><a href="#常驻分支（主分支）" class="headerlink" title="常驻分支（主分支）"></a>常驻分支（主分支）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">master 分支</span><br><span class="line">develop 分支</span><br></pre></td></tr></table></figure><h3 id="master-分支"><a href="#master-分支" class="headerlink" title="master 分支"></a>master 分支</h3><p>即是 Git 默认主分支，只用来发布重大版本，是生产环境出于准备就绪状态的最新源码分支。需要对此分支进行严格的控制，可以为每次 master 分支的提交都挂一个钩子脚本，向生产环境自动化构建并发布我们的软件产品。</p><h3 id="develop-分支"><a href="#develop-分支" class="headerlink" title="develop 分支"></a>develop 分支</h3><p>develop 分支作为日常开发分支，可以理解为准备下一次发布的，开发人员最后一次提交的源码分支，这个分支也叫做 集成分支，此分支也作为每日构建（nightly build）自动化任务的源码分支</p><h2 id="临时分支（支持型分支）"><a href="#临时分支（支持型分支）" class="headerlink" title="临时分支（支持型分支）"></a>临时分支（支持型分支）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">feature 功能开发分支（也叫 topic 分支）</span><br><span class="line">release 预发布分支</span><br><span class="line">hotfix 修补 issue 分支</span><br></pre></td></tr></table></figure><p>这些分支是为了准备发布新产品，开发新的功能特性，快速或者紧急修复上线等任务而设立的分支，这些分支都是临时的，使命完成后，都应该删除。</p><h3 id="release-分支"><a href="#release-分支" class="headerlink" title="release 分支"></a>release 分支</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">派生自：develop 分支</span><br><span class="line">需要合并回：develop 或者 master 分支</span><br><span class="line">分支命名规范：release-版本号</span><br></pre></td></tr></table></figure><p>release 分支派生自 develop 分支。假设，当前的生产环境发布的版本（ mster 分支）是 1.1，我们确定新的版本号为 1.2 。通过下面命令派生一个新的 release 分支并以新的版本号为其命名：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b release-1.2 develop</span><br><span class="line">$ git commit -a -m <span class="string">"Bumped version number to 1.2"</span></span><br></pre></td></tr></table></figure></p><p>这个新的 release 分支，从创建到发布出去会存在一段时间，在此期间，可能会有issue修复（bug 修复直接在 release 分支上进行）分支，完成后并入 develop 分支，并放入下一次发布。<br>release 分支真正发布成功后，还有下面的事要做：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// release 分支合并到master</span><br><span class="line">$ git checkout master</span><br><span class="line">$ git merge --no-ff release-1.2</span><br><span class="line">// 在 master 分支上的打一个 tag，作为标签以便作为版本历史的参考</span><br><span class="line">$ git tag -a 1.2</span><br><span class="line">// release 分支产生的改动合并回 develop，以便后续的发布同样包含对这些 bug 的修复</span><br><span class="line">$ git checkout develop</span><br><span class="line">// -no-ff 标记使得合并操作总是产生一次新的提交，避免所有提交的历史信息混在一起</span><br><span class="line">$ git merge --no-ff release-1.2</span><br><span class="line">// 至此 release 分支使命已经完成，应该删除它</span><br><span class="line">$ git branch -d release-1.2</span><br></pre></td></tr></table></figure></p><h3 id="feature-分支"><a href="#feature-分支" class="headerlink" title="feature 分支"></a>feature 分支</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">派生自：develop 分支</span><br><span class="line">需要合并回：develop</span><br><span class="line">分支命名规范：feature-功能特性编号</span><br></pre></td></tr></table></figure><p>feature 分支是用来开发即将发布的新的功能特性。feature 分支的生命周期会和新功能特性的开发周期保持同步，但是最终会合并回 develop 分支或被抛弃(功能特性不需要了)。<br>feature 分支通常仅存在于开发者的代码库中，并不出现在 origin 里。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 从 develop 派生出 feature 分支</span><br><span class="line">$ git checkout -b feature-12345 develop</span><br><span class="line">...</span><br><span class="line">// 功能开发完成后，合并回 develop 分支</span><br><span class="line">$ git checkout develop</span><br><span class="line">$ git merge --no-ff myfeature</span><br><span class="line">// feature 分支使命完成，应该删除</span><br><span class="line">$ git branch -d myfeature</span><br><span class="line">$ git push origin develop</span><br></pre></td></tr></table></figure></p><h3 id="hotfix-分支"><a href="#hotfix-分支" class="headerlink" title="hotfix 分支"></a>hotfix 分支</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">派生自：master 分支</span><br><span class="line">需要合并回：develop 和 master</span><br><span class="line">分支命名规范：hotfix-issue编号</span><br></pre></td></tr></table></figure><p>hotfix 分支 是在实时的生产环境版本出现意外需要快速响应时，从 master 分支相应的 tag 被派生出来。这样做的原因，是为了让团队其中一个人来快速修复生产环境的问题，其他成员可以按工作计划继续工作下去而不受太大影响。<br>假设，当前 master 版本是1.2 ，生产环境出现了较严重的 issue (假设，记录 bug 编号为 10002)，此时就要从 master 分支派生一个 hotfix 分支：<br><code>`</code> bash<br>// 从 master 派生出 hotfix 分支<br>$ git checkout -b hotfix-10002 master<br>…<br>// 修复 bug，提交代码<br>$ git commit -m “Fixed severe production problem”<br>// bug 修复完成后，hotfix 分支需要并回 master 和 develop 分支，以保证接下来的发布也都已经解决了这个 bug<br>$ git checkout master<br>$ git merge –no-ff hotfix-10002<br>// 变更小版本号<br>$ git tag -a 1.2.1<br>$ git checkout develop<br>$ git merge –no-ff hotfix-10002<br>// hotfix 分支使命完成，应该删除<br>$ git branch -d myfeature</p><p>下图形象的总结了以上分支之间的派生关系<br><img src="/post_imgs/git-branch-1.jpg" alt=""></p><p>本文是阅读了<a href="http://nvie.com/posts/a-successful-git-branching-model/" target="_blank" rel="noopener">A successful Git branching model</a>之后的自我总结。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常驻分支（主分支）&quot;&gt;&lt;a href=&quot;#常驻分支（主分支）&quot; class=&quot;headerlink&quot; title=&quot;常驻分支（主分支）&quot;&gt;&lt;/a&gt;常驻分支（主分支）&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
      
    
    </summary>
    
    
      <category term="Git" scheme="http://luckymartinlee.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop从入门到放弃(一) -- 基础概念</title>
    <link href="http://luckymartinlee.github.io/2018/03/09/hadoop_1-1/"/>
    <id>http://luckymartinlee.github.io/2018/03/09/hadoop_1-1/</id>
    <published>2018-03-09T02:47:04.000Z</published>
    <updated>2020-12-07T12:51:03.015Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Hadoop-是什么"><a href="#Hadoop-是什么" class="headerlink" title="Hadoop 是什么"></a>Hadoop 是什么</h2><p>Hadoop 是一个开源的大数据框架，一个分布式计算的解决方案。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop = HDFS(分布式文件系统) + MapReduce(分布式计算)</span><br></pre></td></tr></table></figure></p><p>HDFS 分布式文件系统: 海量存储是大数据的技术的基础<br>MapReduce 编程模型: 分布式计算是大数据应用的解决方案</p><h2 id="HDFS-概念"><a href="#HDFS-概念" class="headerlink" title="HDFS 概念"></a>HDFS 概念</h2><h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>HDFS 上面的文件是按照数据块为单元来存储的，其默认大小为 64MB , 我们可以依据自己的情况进行设置，一般我们设置为 128MB, 备份数是3，也是可以修改的。<br>数据块的大小设置，如设置的太小，小文件也会被切割成多个数据块，访问的时候就要查找多个数据块地址，效率比较低，同时 NameNode 存储了太多的数据块信息，对内存消耗比较多，内存压力大。如果数据块设置过大，就会降低数据并行操作的效率，同时如果系统重启，数据块越大，系统重启的时间就越长<br>使用数据块存储的好处有：</p><ol><li>屏蔽了文件的概念，无论 200KB 还是 200PB 的文件都是按照数据块进行存储，简化了存储系统的设计</li><li>数据块方便数据备份，提高数据容错能力</li></ol><h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>NameNode 相当于 master - slave 体系中的 master , 他的职责有：</p><ol><li>管理文件系统的命名空间</li><li>存放文件元数据</li><li>维护文件系统的所有文件和目录</li><li>维护文件与数据块的映射</li><li>记录每个文件的各个块所在数据节点 DataNode 的信息</li></ol><h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>DataNode 是 HDFS 文件系统的工作节点，负责存储和检索数据块，向 NameNode 更新所存储块的列表</p><h3 id="HDFS-优-缺点"><a href="#HDFS-优-缺点" class="headerlink" title="HDFS 优/缺点"></a>HDFS 优/缺点</h3><p>优点：</p><ol><li>适合大文件存储，支持 TB、PB 级别数据存储，支持副本策略</li><li>HDFS 可以构建在廉价的普通机器上，具备容错和恢复机制</li><li>支持流逝数据访问，一次写入多次读取效率高<br>缺点:</li><li>不适合大量小文件存储</li><li>不适合并发写入，不支持文件随机修改</li><li>不适合随机度等低延时的访问方式</li></ol><h3 id="HDFS-写流程"><a href="#HDFS-写流程" class="headerlink" title="HDFS 写流程"></a>HDFS 写流程</h3><p><img src="/post_imgs/hdfs_write_process.jpg" alt=""><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. Client 向 NameNode 发出写请求，表明要将 data 写入到集群当中。</span><br><span class="line">2. NameNode 中存储了集群中所有 DataNode 信息，收到 Client 请求后，就将可用的 DataNode 信息发送给Client</span><br><span class="line">3. Client 依据收到 NameNode 的信息，先将数据进行分块，如分成两块。然后将 数据块1 和 从NameNode接收到的 DataNode所有节点信息，都发送给 DataNode-1</span><br><span class="line">4. DataNode-1 接收到信息后，先将 数据块1 进行保存，在依据接收的DataNode节点集群信息，将 数据块1 备份到 DataNode-2 和 DataNode-3。 当 DataNode-1，DataNode-2，DataNode-3 完成 数据块1 存储之后，反馈给 NameNode</span><br><span class="line">5. NameNode 收到 DataNode 发来的反馈信息后，更新自己的 DataNode 元数据信息列表。然后告诉 Client 数据块1 已经存储好了，可以存储后面的数据块了</span><br><span class="line">6. Client 收到 NameNode 信息后，开始重复 数据块1的存储步骤，存储 数据块2，至此 HDFS 写数据流程结束</span><br></pre></td></tr></table></figure></p><h3 id="HDFS-写流程-1"><a href="#HDFS-写流程-1" class="headerlink" title="HDFS 写流程"></a>HDFS 写流程</h3><p><img src="/post_imgs/hdfs_read_process.jpg" alt=""><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Client 向 NameNode 发出读请求，表明要从集群中读取文件 data。</span><br><span class="line">2. NameNode 收到 Client 请求后，就将存储了 data 文件数据块的DataNode 节点信息发送给 Client，如上图，DataNode-1 存储了 数据块1，DataNode-2 存储了 数据块2，DataNode-3 存储了 数据块1 和 数据块2</span><br><span class="line">3. Client 依据收到 NameNode 的信息，先从 DataNode-1 读取 数据块1 ，然后再从 DataNode-2 读取 数据块2，如果 DataNode-2 宕机，Client 就会向 DataNode-3 读取 数据块2， 至此 HDFS 读数据流程结束</span><br></pre></td></tr></table></figure></p><h3 id="HDFS-常用命令"><a href="#HDFS-常用命令" class="headerlink" title="HDFS 常用命令"></a>HDFS 常用命令</h3><ol><li>类Linux系统命令: ls, cat, mkdir, rm, chomd, chown, …</li><li>HDFS文件交互命令: copyFromLocal, copyToLocal, get, put</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Hadoop-是什么&quot;&gt;&lt;a href=&quot;#Hadoop-是什么&quot; class=&quot;headerlink&quot; title=&quot;Hadoop 是什么&quot;&gt;&lt;/a&gt;Hadoop 是什么&lt;/h2&gt;&lt;p&gt;Hadoop 是一个开源的大数据框架，一个分布式计算的解决方案。&lt;br&gt;&lt;f
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://luckymartinlee.github.io/tags/Hadoop/"/>
    
      <category term="大数据" scheme="http://luckymartinlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch从入门到放弃(三) -- 高级</title>
    <link href="http://luckymartinlee.github.io/2018/02/01/elasticsearch_1-3/"/>
    <id>http://luckymartinlee.github.io/2018/02/01/elasticsearch_1-3/</id>
    <published>2018-02-01T07:41:28.000Z</published>
    <updated>2020-12-10T06:07:25.816Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="搜索引擎" scheme="http://luckymartinlee.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
      <category term="Elasticsearch" scheme="http://luckymartinlee.github.io/tags/Elasticsearch/"/>
    
      <category term="ES" scheme="http://luckymartinlee.github.io/tags/ES/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch从入门到放弃(二) -- 零基础环境搭建</title>
    <link href="http://luckymartinlee.github.io/2018/01/28/elasticsearch_1-2/"/>
    <id>http://luckymartinlee.github.io/2018/01/28/elasticsearch_1-2/</id>
    <published>2018-01-28T01:33:35.000Z</published>
    <updated>2018-05-09T00:50:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要讲述 Linux 虚拟机环境下 Elasticsearch 5.5 版本的安装。安装 Elasticsearch 之前，请确保你的机器 Java 8 环境已经搭建好，保证环境变量 JAVA_HOME 设置正确。如你还没有安装好 JAVA 8 环境，请参考<a href="/2017/11/04/java_1-1/">linux系统中JAVA环境搭建</a></p><h4 id="下载并解压安装包"><a href="#下载并解压安装包" class="headerlink" title="下载并解压安装包"></a>下载并解压安装包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.0.zip</span><br><span class="line">$ unzip elasticsearch-5.5.0.zip</span><br><span class="line">$ <span class="built_in">cd</span> elasticsearch-5.5.0/</span><br></pre></td></tr></table></figure><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>Elasticsearch 默认只允许本机访问，远程访问，需要修改 config/elasticsearch.yml,<br>改为运行所有人访问 0.0.0.0（生产环境，切不可这样改，可以指定特定 ip 访问 Elasticsearch）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ vim config/elasticsearch.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#network.host: 192.168.0.1</span></span><br><span class="line">network.host: 0.0.0.0</span><br></pre></td></tr></table></figure></p><h4 id="启动程序"><a href="#启动程序" class="headerlink" title="启动程序"></a>启动程序</h4><p>直接运行 bin 目录下的 elasticsearch<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/elasticsearch</span><br></pre></td></tr></table></figure></p><p>此过程中，可能会报出以下错误<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max virtual memory areas vm.maxmapcount [65530] is too low</span><br></pre></td></tr></table></figure></p><p>此时，切换到管理员用户，运行下面的命令即可<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sysctl -w vm.max_map_count=262144</span></span><br></pre></td></tr></table></figure></p><p>重新运行 elasticsearch 即可，这时候访问 9200 端口，得到如下信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"uA7Io-i"</span>, <span class="comment"># node 名称</span></span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elasticsearch"</span>, <span class="comment"># 集群名称 </span></span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"f7y_hJefSw-kQFN-zt-3Cw"</span>, <span class="comment"># 集群唯一 id</span></span><br><span class="line">  <span class="string">"version"</span> : &#123;</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"5.5.0"</span>, <span class="comment"># Elasticsearch 版本号</span></span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"260387d"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2017-06-30T23:16:05.735Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"6.6.0"</span> <span class="comment"># 依赖的 Lucene 版本号</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>到这里说明你的 Elasticsearch 已经安装成功了，按下 Ctrl + C，Elasticsearch 就会停止运行。 想要后台运行 Elasticsearch，输入下面命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/elasticsearch -d</span><br></pre></td></tr></table></figure></p><p>此时想要停止 Elasticsearch ，想要先找到 Elasticsearch 进程，然后 kill 。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ps -ef |grep elasticsearch</span><br><span class="line"></span><br><span class="line">martin    3035  2200  0 Apr19 ?        00:04:29 /usr/bin/java -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch -server -Xss1m -Djava.awt.headless=<span class="literal">true</span> -Dfile.encoding=UTF-8 -Djna.nosys=<span class="literal">true</span> -Djdk.io.permissionsUseCanonicalPath=<span class="literal">true</span> -Dio.netty.noUnsafe=<span class="literal">true</span> -Dio.netty.noKeySetOptimization=<span class="literal">true</span> -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=<span class="literal">false</span> -Dlog4j2.disable.jmx=<span class="literal">true</span> -Dlog4j.skipJansi=<span class="literal">true</span> -XX:+HeapDumpOnOutOfMemoryError -Des.path.home=/home/marting/elasticsearch-5.5.0 -cp /home/martin/elasticsearch-5.5.0/lib/* org.elasticsearch.bootstrap.Elasticsearch</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">kill</span> -2 3035</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文主要讲述 Linux 虚拟机环境下 Elasticsearch 5.5 版本的安装。安装 Elasticsearch 之前，请确保你的机器 Java 8 环境已经搭建好，保证环境变量 JAVA_HOME 设置正确。如你还没有安装好 JAVA 8 环境，请参考&lt;a hre
      
    
    </summary>
    
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="搜索引擎" scheme="http://luckymartinlee.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
      <category term="Elasticsearch" scheme="http://luckymartinlee.github.io/tags/Elasticsearch/"/>
    
      <category term="ES" scheme="http://luckymartinlee.github.io/tags/ES/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch从入门到放弃(一) -- 基本概念</title>
    <link href="http://luckymartinlee.github.io/2018/01/26/elasticsearch_1-1/"/>
    <id>http://luckymartinlee.github.io/2018/01/26/elasticsearch_1-1/</id>
    <published>2018-01-26T07:23:08.000Z</published>
    <updated>2018-05-09T00:50:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><h3 id="通用搜索-与-垂直搜索"><a href="#通用搜索-与-垂直搜索" class="headerlink" title="通用搜索 与 垂直搜索"></a>通用搜索 与 垂直搜索</h3><p>&ensp;&ensp;通用搜索，通俗点说就是在网络上的所有可以获取的信息中进行搜索，涉及所有领域，所有展现形式，包括音频、视频、文字、图片。<br>一句话就是只要信息内容匹配你的搜索条件，就返回给你，知名站点有 Baidu, Google, Bing.<br>&ensp;&ensp;垂直搜索，相对于通用搜索，它是针对特定行业信息的搜索。比如，用于图标搜索的<a href="http://www.easyicon.cn/" target="_blank" rel="noopener">EasyIcon</a>、<br>小说搜索<a href="https://www.owllook.net/" target="_blank" rel="noopener">Owllook </a>等。</p><h3 id="全文检索-与-倒排索引"><a href="#全文检索-与-倒排索引" class="headerlink" title="全文检索 与 倒排索引"></a>全文检索 与 倒排索引</h3><p>&ensp;&ensp;全文检索，概念就是通过计算机实现，你的搜索词出现在文章当中，甚至搜索词与文章有相关性，那么这篇文章就应该被搜索出来。计算机实现这个功能整个过程叫做全文检索。<br>&ensp;&ensp;倒排索引，是实现全文检索的技术手段之一。简单的说就是把文章拆解成一个个简单的词语（此过程叫分词），将文章与这些词语建立起关联关系（即是索引）。用户查询时，也讲搜索词进行分词，再在前面建立的关联关系中查找文章，以此实现全文检索。想更进一步知道什么倒排索引的实现，请看我的另一篇文章《<a href="/2017/04/05/inverted_index1/">什么是倒排索引</a>》</p><h3 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h3><p>&ensp;&ensp;Lucene，是一个开源免费的成熟的Java系的信息检索程序库，全文检索引擎工具包，由Apache软件基金会支持和提供。严格的说，Lucene 不是全文检索引擎或者搜索引擎，它提供了查询组件，索引组件以及文本分析组件，它是一个全文检索引擎的框架。 </p><h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><h3 id="何为-Elasticsearch"><a href="#何为-Elasticsearch" class="headerlink" title="何为 Elasticsearch"></a>何为 Elasticsearch</h3><p>&ensp;&ensp;Elasticsearch，是一款基于Lucene的开源的高扩展的分布式全文检索引擎。Elastic 是 Lucene 的封装，提供了 RESTFull API 的操作接口，开箱即用。它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。<br><a href="http://www.elastic.co/products/elasticsearch" target="_blank" rel="noopener"><img src="/post_imgs/elastic-logo.png" alt=""></a></p><h3 id="Elasticsearch-核心概念"><a href="#Elasticsearch-核心概念" class="headerlink" title="Elasticsearch 核心概念"></a>Elasticsearch 核心概念</h3><h4 id="Near-Realtime-NRT"><a href="#Near-Realtime-NRT" class="headerlink" title="Near Realtime (NRT)"></a>Near Realtime (NRT)</h4><p>&ensp;&ensp;近实时概念包含两层含义，一是数据从写入到可搜索到，时间达到秒级别；二是Elasticsearch 查询、聚合、分析，时间到达秒级别。</p><h4 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h4><p>&ensp;&ensp;Elasticsearch是一个分布式的全文检索引擎，多个程序节点构成一个大的集群，集群中节点即是数据备份节点，也是数据查询分析负载均衡节点。</p><h4 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h4><p>&ensp;&ensp;节点，即是一个Elasicsearch的实例，一台机器可以运行一个或多个节点。</p><h4 id="Shard-primary-shard"><a href="#Shard-primary-shard" class="headerlink" title="Shard(primary shard)"></a>Shard(primary shard)</h4><p>&ensp;&ensp;分片是为了解决海量数据存储问题，Elasticsearch 使用分片机制，将海量数据切分为多个分片存储在不同的节点上。Elasticseach分片就是它的主分片，Elasticsearch默认主分片数量是5。</p><h4 id="replica-replica-shard"><a href="#replica-replica-shard" class="headerlink" title="replica(replica shard)"></a>replica(replica shard)</h4><p>&ensp;&ensp;分片副本，意思显而易见，就是分片的备份，作用是提高集群数据安全，提升系统高可用性，同时副本节点在海量数据检索时也分担检索压力，具备提升 Elasticsearch 请求吞吐量和性能作用。</p><h4 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h4><p>&ensp;&ensp;这里的 Index 不是查询索引的概念，可以理解为同类型数据的库，相当于 MySQL 中的库，但又有不同，不同之处在于此处的Index中存储的都是字段类型基本一致的同类型数据。</p><h4 id="Type"><a href="#Type" class="headerlink" title="Type"></a>Type</h4><p>&ensp;&ensp;这里的 Type ，可以理解为 MySQL 中的表，一个 Index 中可以有多个 Type，且一个 Type存储同种数据。如，一个名为 animal 的 Index 中有 bird 和 fish 两个 Type, 他们有很多共同的属性。</p><h4 id="Document"><a href="#Document" class="headerlink" title="Document"></a>Document</h4><p>&ensp;&ensp;Document，即是 Type 中具体的文档。每个文档都有自己唯一的 id.</p><h4 id="Field"><a href="#Field" class="headerlink" title="Field"></a>Field</h4><p>&ensp;&ensp;Field，是文档的属性或者叫做字段，不同字段，类型不同，分词方式也不同。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;相关概念&quot;&gt;&lt;a href=&quot;#相关概念&quot; class=&quot;headerlink&quot; title=&quot;相关概念&quot;&gt;&lt;/a&gt;相关概念&lt;/h2&gt;&lt;h3 id=&quot;通用搜索-与-垂直搜索&quot;&gt;&lt;a href=&quot;#通用搜索-与-垂直搜索&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="Elasticsearch" scheme="http://luckymartinlee.github.io/tags/Elasticsearch/"/>
    
      <category term="ES" scheme="http://luckymartinlee.github.io/tags/ES/"/>
    
  </entry>
  
  <entry>
    <title>Scala 函数与方法的区分与理解</title>
    <link href="http://luckymartinlee.github.io/2017/12/03/scala_1-3/"/>
    <id>http://luckymartinlee.github.io/2017/12/03/scala_1-3/</id>
    <published>2017-12-03T01:33:25.000Z</published>
    <updated>2020-12-10T07:32:26.930Z</updated>
    
    <content type="html"><![CDATA[<p>方法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def add(x:Int, y: Int) = x + y</span><br><span class="line">add: (x: Int, y: Int)Int</span><br><span class="line"></span><br><span class="line">scala&gt; add(1, 2)</span><br><span class="line">res0: Int = 3</span><br></pre></td></tr></table></figure></p><p>函数：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val add_f = (x: Int, y: Int) =&gt; x + y</span><br><span class="line">add_f: (Int, Int) =&gt; Int = &lt;function2&gt;</span><br><span class="line"></span><br><span class="line">// 根据内容可以看出add_f是一个函数Function</span><br><span class="line"></span><br><span class="line">scala&gt; add_f(1, 2)</span><br><span class="line">res1: Int = 3</span><br></pre></td></tr></table></figure></p><p>上面 ‘=’号右边的内容 (x: Int, y: Int) =&gt; x + y是一个函数体，<br>方法只能用 def 接收，函数可以用 def 接收，也可以用 val 接收。<br>当函数用 def 来接收之后，不再显示为 function ，转换为方法。<br>方法可以省略参数，函数不可以。 函数可以作为方法的参数。 看下面的例子：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = () =&gt; 100</span><br><span class="line">a: () =&gt; Int = &lt;function0&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; val a = =&gt; 100</span><br><span class="line">&lt;console&gt;:1: error: illegal start of simple expression</span><br></pre></td></tr></table></figure></p><p>看这里: val a = =&gt; 100 // 当函数参数为空时报错</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;方法：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;s
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://luckymartinlee.github.io/tags/Linux/"/>
    
      <category term="Scala" scheme="http://luckymartinlee.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>什么是倒排索引</title>
    <link href="http://luckymartinlee.github.io/2017/12/03/inverted_index_1/"/>
    <id>http://luckymartinlee.github.io/2017/12/03/inverted_index_1/</id>
    <published>2017-12-03T01:06:12.000Z</published>
    <updated>2018-05-02T08:24:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>在搜索引擎的设计逻辑中，为每一搜索目标文件都生成一个唯一的 ID ，而文件的内容可以看成是很多提取出来的关键词的集合，提取关键词的过程叫做 “分词”。<br>例如，文件1的 ID 是 1001，经过分词，总共提取出30个关键词，那么搜索引擎就会记录每个关键词出现在文章当中的位置和出现次数。</p><p>下面说说什么是 倒排索引，有倒排索引，相应的肯定就有正向索引。</p><h3 id="正向索引"><a href="#正向索引" class="headerlink" title="正向索引"></a>正向索引</h3><p>正向索引的结构：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1001(文档1) &gt; &#123;中国(关键词1):&#123;出现次数:2;出现位置：10,15&#125;,&#123;劳动(关键词2):&#123;出现次数:3;出现位置：2,7,11&#125;,...</span><br><span class="line">1002(文档2) &gt; &#123;中国(关键词1):&#123;出现次数:3;出现位置：6,23,45&#125;,&#123;体育(关键词3):&#123;出现次数:1;出现位置：1&#125;,...</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>如图所示:<br><img src="/post_imgs/正向索引.jpg" alt=""></p><p>对于 正向索引，假设，用户搜索关键词“中国”，搜索引擎就要完整遍历索引中所有信息，找到包含“中国”的文件，然后依据特定的打分排序算法，整理出文件先后顺序，再返回给用户。可以看出，正向索引明显的弊端就是，对于海量数据的搜索引擎系统，正向索引结构效率低下，无法快速响应用户。</p><h3 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h3><p>倒排索引的结构：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">中国(关键词1) &gt; &#123;1001(文档1):&#123;出现次数:2;出现位置：10,15&#125;,&#123;1002(文档2):&#123;出现次数:3;出现位置：6,23,45&#125;,...</span><br><span class="line">劳动(关键词2)&gt; &#123;1001(文档1):&#123;出现次数:3;出现位置：2,7,11&#125;,...</span><br><span class="line">体育(关键词3)&gt; &#123;1002(文档1):&#123;出现次数:1;出现位置：1&#125;,...</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>如图所示:<br><img src="/post_imgs/倒排索引.jpg" alt=""><br>对于 倒排索引，同样假设，用户搜索关键词“中国”，搜索引擎就要不必完整遍历索引中所有信息，很快就能找到包含“中国”的文件，然后依据特定的打分排序算法，整理出文件先后顺序，返回给用户。可以看出，倒排索引很好解决了正向索引的弊端就是，对于海量数据的搜索引擎系统，倒排索引结构效率较高，可以较快响应用户。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在搜索引擎的设计逻辑中，为每一搜索目标文件都生成一个唯一的 ID ，而文件的内容可以看成是很多提取出来的关键词的集合，提取关键词的过程叫做 “分词”。&lt;br&gt;例如，文件1的 ID 是 1001，经过分词，总共提取出30个关键词，那么搜索引擎就会记录每个关键词出现在文章当中的
      
    
    </summary>
    
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="搜索引擎" scheme="http://luckymartinlee.github.io/tags/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>Scala 闭包(closure) 深入理解</title>
    <link href="http://luckymartinlee.github.io/2017/10/30/scala_1-2/"/>
    <id>http://luckymartinlee.github.io/2017/10/30/scala_1-2/</id>
    <published>2017-10-30T11:24:55.000Z</published>
    <updated>2020-12-10T07:19:32.835Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是-Scala-闭包"><a href="#什么是-Scala-闭包" class="headerlink" title="什么是 Scala 闭包"></a>什么是 Scala 闭包</h2><p>闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。<br>闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。<br>如下实例：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var more =1</span><br><span class="line">more: Int = 1 </span><br><span class="line"></span><br><span class="line">scala&gt; val addMore = (x:Int) =&gt; x + more</span><br><span class="line">addMore: Int =&gt; Int = &lt;function1&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; addMore (100)</span><br><span class="line">res1: Int = 101</span><br></pre></td></tr></table></figure></p><p>其中，我们定义函数变量 addMore 成为一个“闭包”，因为它引用到函数外面定义的变量 more，定义这个函数的过程是将这个自由变量捕获而构成一个封闭的函数。有意思的是，当这个自由变量发生变化时，Scala 的闭包能够捕获到这个变化，因此 Scala 的闭包捕获的是变量本身而不是当时变量的值。</p><p>同样的，如果变量在闭包内发生变化，也会反映到函数外面定义的闭包的值。<br>如下实例:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val someNumbers = List ( -11, -10, -5, 0, 5, 10)</span><br><span class="line">someNumbers: List[Int] = List(-11, -10, -5, 0, 5, 10)</span><br><span class="line"></span><br><span class="line">scala&gt; var sum =0</span><br><span class="line">sum: Int = 0</span><br><span class="line"></span><br><span class="line">scala&gt; someNumbers.foreach ( sum += _)</span><br><span class="line"></span><br><span class="line">scala&gt; sum</span><br><span class="line">res4: Int = -11</span><br></pre></td></tr></table></figure></p><p>上面可以看到在闭包中修改sum的值，其结果还是传递到闭包的外面。<br>&nbsp;<br>那如果一个闭包所访问的变量有几个不同的版本，比如一个闭包使用了一个函数的局部变量（参数），然后这个函数调用很多次，那么所定义的闭包应该使用所引用的局部变量的哪个版本呢？ 简单的说，该闭包定义所引用的变量为定义该闭包时变量的值，也就是定义闭包时相当于保存了当时程序状态的一个快照。比如我们定义下面一个函数闭包:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def makeIncreaser(more:Int) = (x:Int) =&gt; x + more</span><br><span class="line">makeIncreaser: (more: Int)Int =&gt; Int</span><br><span class="line"></span><br><span class="line">scala&gt; val inc1=makeIncreaser(1)</span><br><span class="line">inc1: Int =&gt; Int = &lt;function1&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; val inc9999=makeIncreaser(9999)</span><br><span class="line">inc9999: Int =&gt; Int = &lt;function1&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; inc1(10)</span><br><span class="line">res5: Int = 11</span><br><span class="line"></span><br><span class="line">scala&gt; inc9999(10)</span><br><span class="line">res6: Int = 10009</span><br></pre></td></tr></table></figure></p><p>当你调用makeIncreaser(1)时，你创建了一个闭包，该闭包定义时more的值为1, 而调用makeIncreaser(9999)所创建的闭包的more的值为9999。此后你也无法修改已经返回的闭包的more的值。因此inc1始终为加一，而inc9999始终为加9999.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是-Scala-闭包&quot;&gt;&lt;a href=&quot;#什么是-Scala-闭包&quot; class=&quot;headerlink&quot; title=&quot;什么是 Scala 闭包&quot;&gt;&lt;/a&gt;什么是 Scala 闭包&lt;/h2&gt;&lt;p&gt;闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://luckymartinlee.github.io/tags/Linux/"/>
    
      <category term="Scala" scheme="http://luckymartinlee.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>Scala 柯里化(curry) 深入理解</title>
    <link href="http://luckymartinlee.github.io/2017/10/23/scala_1-1/"/>
    <id>http://luckymartinlee.github.io/2017/10/23/scala_1-1/</id>
    <published>2017-10-23T01:44:17.000Z</published>
    <updated>2020-12-10T07:47:23.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是柯里化函数"><a href="#什么是柯里化函数" class="headerlink" title="什么是柯里化函数"></a>什么是柯里化函数</h2><p>有多个参数列表 的函数就是柯里化函数，所谓的参数列表就是使用小括号括起来的函数参数列表, 函数柯里化就是 把接受多个参数的函数变换成接受一个单一参数(最初函数的第一个参数)的函数，并且返回接受余下的参数且返回结果的新函数的过程。如下所示<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 非柯里化函数</span><br><span class="line">def sum(x:Int,y:Int)=x+y</span><br><span class="line"></span><br><span class="line">// 柯里化函数</span><br><span class="line">def sum(x:Int)(y:Int) = x + y</span><br></pre></td></tr></table></figure></p><p>实例如下：<br><img src="/post_imgs/scala_1_1.jpg" alt=""></p><p>sum(1)(2) 实际上是依次调用两个普通函数（非柯里化函数），第一次调用使用一个参数 x，返回一个函数类型的值，第二次使用参数y调用这个函数类型的值。</p><p>实质上最先演变成这样一个方法：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def sum(x:Int)=(y:Int) =&gt; x+y</span><br></pre></td></tr></table></figure></p><p>那么这个函数是什么意思呢？ 接收一个x为参数，返回一个匿名函数，该匿名函数的定义是：接收一个Int型参数y，函数体为x+y.</p><h3 id="柯里化的意义"><a href="#柯里化的意义" class="headerlink" title="柯里化的意义"></a>柯里化的意义</h3><p>柯里化的意义在于把多个参数的function等价转化成多个单参数function的级联，这样方便做lambda演算。 同时curry化对类型推演也有帮助，scala的类型推演是局部的，在同一个参数列表中后面的参数不能借助前面的参数类型进行推演，curry化以后，放在两个参数列表里，后面一个参数列表里的参数可以借助前面一个参数列表里的参数类型进行推演。这就是为什么 foldLeft这种函数的定义都是curry的形式。函数柯里化在提高函数适用性和延迟执行或者固定易变因素等方面有着重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是柯里化函数&quot;&gt;&lt;a href=&quot;#什么是柯里化函数&quot; class=&quot;headerlink&quot; title=&quot;什么是柯里化函数&quot;&gt;&lt;/a&gt;什么是柯里化函数&lt;/h2&gt;&lt;p&gt;有多个参数列表 的函数就是柯里化函数，所谓的参数列表就是使用小括号括起来的函数参数列表, 函
      
    
    </summary>
    
    
      <category term="Scala" scheme="http://luckymartinlee.github.io/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>curl 工具使用</title>
    <link href="http://luckymartinlee.github.io/2017/04/20/curl_1-1/"/>
    <id>http://luckymartinlee.github.io/2017/04/20/curl_1-1/</id>
    <published>2017-04-20T06:33:23.000Z</published>
    <updated>2020-12-10T07:43:40.522Z</updated>
    
    <content type="html"><![CDATA[<h3 id="工具简介"><a href="#工具简介" class="headerlink" title="工具简介"></a>工具简介</h3><p>curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思。<br>它的功能非常强大，命令行参数多达几十种。如果熟练的话，完全可以取代 Postman 这一类的图形界面工具。</p><h3 id="参数简介"><a href="#参数简介" class="headerlink" title="参数简介"></a>参数简介</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调试类</span></span><br><span class="line">-v, --verbose                          输出信息</span><br><span class="line">-q, --<span class="built_in">disable</span>                          在第一个参数位置设置后 .curlrc 的设置直接失效，这个参数会影响到 -K, --config -A, --user-agent -e, --referer</span><br><span class="line">-K, --config FILE                      指定配置文件</span><br><span class="line">-L, --location                         跟踪重定向 (H)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CLI显示设置</span></span><br><span class="line">-s, --silent                           Silent模式。不输出任务内容</span><br><span class="line">-S, --show-error                       显示错误. 在选项 -s 中，当 curl 出现错误时将显示</span><br><span class="line">-f, --fail                             不显示 连接失败时HTTP错误信息</span><br><span class="line">-i, --include                          显示 response的header (H/F)</span><br><span class="line">-I, --head                             仅显示 响应文档头</span><br><span class="line">-l, --list-only                        只列出FTP目录的名称 (F)</span><br><span class="line">-<span class="comment">#, --progress-bar                     以进度条 显示传输进度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据传输类</span></span><br><span class="line">-X, --request [GET|POST|PUT|DELETE|…]  使用指定的 http method 例如 -X POST</span><br><span class="line">-H, --header &lt;header&gt;                  设定 request里的header 例如 -H <span class="string">"Content-Type: application/json"</span></span><br><span class="line">-e, --referer                          设定 referer (H)</span><br><span class="line">-d, --data &lt;data&gt;                      设定 http body 默认使用 content-type application/x-www-form-urlencoded (H)</span><br><span class="line">    --data-raw &lt;data&gt;                  ASCII 编码 HTTP POST 数据 (H)</span><br><span class="line">    --data-binary &lt;data&gt;               binary 编码 HTTP POST 数据 (H)</span><br><span class="line">    --data-urlencode &lt;data&gt;            url 编码 HTTP POST 数据 (H)</span><br><span class="line">-G, --get                              使用 HTTP GET 方法发送 -d 数据 (H)</span><br><span class="line">-F, --form &lt;name=string&gt;               模拟 HTTP 表单数据提交 multipart POST (H)</span><br><span class="line">    --form-string &lt;name=string&gt;        模拟 HTTP 表单数据提交 (H)</span><br><span class="line">-u, --user &lt;user:password&gt;             使用帐户，密码 例如 admin:password</span><br><span class="line">-b, --cookie &lt;data&gt;                    cookie 文件 (H)</span><br><span class="line">-j, --junk-session-cookies             读取文件中但忽略会话cookie (H)</span><br><span class="line">-A, --user-agent                       指定客户端的用户代理标头，user-agent设置，默认用户代理字符串是curl/[version] (H)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 传输设置</span></span><br><span class="line">-C, --<span class="built_in">continue</span>-at OFFSET               断点续转</span><br><span class="line">-x, --proxy [PROTOCOL://]HOST[:PORT]   在指定的端口上使用代理</span><br><span class="line">-U, --proxy-user USER[:PASSWORD]       代理用户名及密码</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件操作</span></span><br><span class="line">-T, --upload-file &lt;file&gt;               上传文件</span><br><span class="line">-a, --append                           添加要上传的文件 (F/SFTP)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出设置</span></span><br><span class="line">-o, --output &lt;file&gt;                    将输出写入文件，而非 stdout</span><br><span class="line">-O, --remote-name                      将输出写入远程文件</span><br><span class="line">-D, --dump-header &lt;file&gt;               将头信息写入指定的文件</span><br><span class="line">-c, --cookie-jar &lt;file&gt;                操作结束后，要写入 Cookies 的文件位置</span><br></pre></td></tr></table></figure><h3 id="常用实例"><a href="#常用实例" class="headerlink" title="常用实例"></a>常用实例</h3><p>GET 请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://www.yahoo.com/login.cgi?user=XXXXXXXXX&amp;password=XXXXXX</span><br></pre></td></tr></table></figure></p><p>POST 请求<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -d <span class="string">"user=XXXXXXXX&amp;password=XXXXX"</span> http://www.yahoo.com/login.cgi</span><br><span class="line">// POST 文件</span><br><span class="line">curl -F upload= <span class="variable">$localfile</span>  -F <span class="variable">$btn_name</span>=<span class="variable">$btn_value</span> http://192.168.10.1/www/focus/up_file.cgi</span><br></pre></td></tr></table></figure></p><p>分块下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">curl -r  0 -10240  -o <span class="string">"zhao.part1"</span>  http://192.168.10.1/www/focus/zhao1.mp3 &amp;\</span><br><span class="line">curl -r 10241 -20480  -o <span class="string">"zhao.part1"</span>  http://192.168.10.1/www/focus/zhao1.mp3 &amp;\</span><br><span class="line">curl -r 20481 -40960  -o <span class="string">"zhao.part1"</span>  http://192.168.10.1/www/focus/zhao1.mp3 &amp;\</span><br><span class="line">curl -r 40961 - -o  <span class="string">"zhao.part1"</span>  http://192.168.10.1/www/focus/zhao1.mp3</span><br><span class="line">...</span><br><span class="line">// 合并块文件</span><br><span class="line">cat zhao.part* &gt; zhao.mp3</span><br></pre></td></tr></table></figure></p><p>ftp 下载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -O ftp://用户名:密码@192.168.10.1:21/www/focus/enhouse/index.php</span><br></pre></td></tr></table></figure></p><p>ftp 上传<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -T upload_test.php ftp://用户名:密码@192.168.10.1:21/www/focus/enhouse/</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;工具简介&quot;&gt;&lt;a href=&quot;#工具简介&quot; class=&quot;headerlink&quot; title=&quot;工具简介&quot;&gt;&lt;/a&gt;工具简介&lt;/h3&gt;&lt;p&gt;curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思。&lt;br&gt;它
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://luckymartinlee.github.io/tags/Linux/"/>
    
      <category term="Shell" scheme="http://luckymartinlee.github.io/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB从入门到放弃(一) -- 基础1</title>
    <link href="http://luckymartinlee.github.io/2016/10/25/mongodb_1-1/"/>
    <id>http://luckymartinlee.github.io/2016/10/25/mongodb_1-1/</id>
    <published>2016-10-25T05:25:15.000Z</published>
    <updated>2018-05-09T00:49:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的 NoSQL 数据库，为 WEB 应用提供可扩展的高性能数据存储解决方案。<br>MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。</p><h2 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h2><p>NoSQL(NoSQL = Not Only SQL )，指的是非关系型的数据库，是对不同于传统的关系型数据库的数据库管理系统的统称。<br>NoSQL 用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。</p><h3 id="RDBMS-vs-NoSQL"><a href="#RDBMS-vs-NoSQL" class="headerlink" title="RDBMS vs NoSQL"></a>RDBMS vs NoSQL</h3><p>RDBMS</p><ul><li>高度组织化结构化数据</li><li>结构化查询语言(SQL)</li><li>数据和关系都存储在单独的表中。</li><li>数据操纵语言，数据定义语言</li><li>严格的一致性</li><li>基础事务</li></ul><p>NoSQL</p><ul><li>代表着不仅仅是SQL</li><li>没有声明性查询语言</li><li>没有预定义的模式</li><li>键值对(key=&gt;value)存储，列存储，文档存储，图形数据库</li><li>最终一致性，而非ACID属性</li><li>非结构化和不可预知的数据</li><li>CAP定理</li><li>高性能，高可用性和可伸缩性</li></ul><h3 id="NoSQL的优-缺点"><a href="#NoSQL的优-缺点" class="headerlink" title="NoSQL的优/缺点"></a>NoSQL的优/缺点</h3><p>优点:</p><ul><li>高可扩展性</li><li>分布式计算</li><li>低成本</li><li>架构的灵活性，半结构化数据</li><li>没有复杂的关系</li></ul><p>缺点:</p><ul><li>没有标准化</li><li>有限的查询功能（到目前为止）</li><li>最终一致是不直观的程序</li></ul><h2 id="MongoDB-特点"><a href="#MongoDB-特点" class="headerlink" title="MongoDB 特点"></a>MongoDB 特点</h2><ul><li>MongoDB安装简单，MongoDB 是一个面向文档存储的数据库，操作起来简单、容易。</li><li>你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。</li><li>你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。</li><li>如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是分片。</li><li>Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。</li><li>MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。</li><li>Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。</li><li>GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。</li><li>MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。</li><li>MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。</li></ul><h2 id="MongoDB-基本概念"><a href="#MongoDB-基本概念" class="headerlink" title="MongoDB 基本概念"></a>MongoDB 基本概念</h2><p>在 MongoDB 中基本的概念有数据库、集合、文档、域、索引等，相较传统的关系型数据库对比图如下：</p><table><thead><tr><th>SQL术语/概念</th><th style="text-align:center">MongoDB术语/概念</th><th style="text-align:right">解释/说明</th></tr></thead><tbody><tr><td>database</td><td style="text-align:center">database</td><td style="text-align:right">数据库</td></tr><tr><td>table</td><td style="text-align:center">collection</td><td style="text-align:right">数据库表/集合</td></tr><tr><td>row</td><td style="text-align:center">document</td><td style="text-align:right">数据记录行/文档</td></tr><tr><td>column</td><td style="text-align:center">field</td><td style="text-align:right">数据字段/域</td></tr><tr><td>index</td><td style="text-align:center">index</td><td style="text-align:right">索引</td></tr><tr><td>table joins</td><td style="text-align:center">嵌入文档</td><td style="text-align:right">表连接,MongoDB不支持表连接,但是有内嵌文档可以替代</td></tr><tr><td>primary key</td><td style="text-align:center">primary key</td><td style="text-align:right">主键,MongoDB自动将_id字段设置为主键</td></tr></tbody></table><p>举例如下：<br><img src="/post_imgs/mongodb1-2.jpg" alt=""></p><h2 id="安装-64-位-Linux上的安装"><a href="#安装-64-位-Linux上的安装" class="headerlink" title="安装(64 位 Linux上的安装)"></a>安装(64 位 Linux上的安装)</h2><p>下载并解压tgz包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl -O https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.6.tgz    <span class="comment"># 下载</span></span><br><span class="line">$ tar -zxvf mongodb-linux-x86_64-3.0.6.tgz                                   <span class="comment"># 解压</span></span><br><span class="line"></span><br><span class="line">$ mv mongodb-linux-x86_64-3.0.6/ /usr/<span class="built_in">local</span>/mongodb                          <span class="comment"># 将解压包拷贝到指定目录</span></span><br></pre></td></tr></table></figure></p><p>添加 MongoDB 可执行文件路径到 PATH<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/mongodb/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure></p><p>创建数据存储目录，MongoDB 默认数据存储路径(dbpath)是 /data/db<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /data/db</span><br></pre></td></tr></table></figure></p><p>启动 MongoDB 服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/mongodb/bin/mongod</span><br></pre></td></tr></table></figure></p><p>MongoDB web 界面<br>MongoDB 3.2 及之前版本，提供了简单的 HTTP 用户界面，此功能需要在启动服务的时候添加参数 rest, 默认端口28017<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/<span class="built_in">local</span>/mongodb/bin/mongod --dbpath=/data/db --rest</span><br></pre></td></tr></table></figure></p><p>如下图：<br><img src="/post_imgs/mongodb1-1.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的 NoSQL 数据库，为 WEB 应用提供可扩展的高性能数据存储解决方案。&lt;br&gt;MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。&lt;/p&gt;
&lt;h2 
      
    
    </summary>
    
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MongoDB" scheme="http://luckymartinlee.github.io/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>MySQL从入门到放弃(四) -- 索引</title>
    <link href="http://luckymartinlee.github.io/2016/05/22/mysql_1-4/"/>
    <id>http://luckymartinlee.github.io/2016/05/22/mysql_1-4/</id>
    <published>2016-05-22T10:45:50.000Z</published>
    <updated>2018-05-09T00:49:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据库索引的概念"><a href="#数据库索引的概念" class="headerlink" title="数据库索引的概念"></a>数据库索引的概念</h2><h3 id="常见索引类型"><a href="#常见索引类型" class="headerlink" title="常见索引类型"></a>常见索引类型</h3><p>MySQL常见索引有：主键索引、唯一索引、普通索引、全文索引、组合索引</p><p>普通索引：最基本的索引，没有任何限制。<br>唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。<br>主键索引：它是一种特殊的唯一索引，不允许有空值，在一张表中只能定义一个主键索引。<br>主键用于唯一标识一条记录，使用关键字 PRIMARY KEY 来创建。主键分为复合主键和联合主键。<br>全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。<br>组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。</p><h3 id="索引存储结构"><a href="#索引存储结构" class="headerlink" title="索引存储结构"></a>索引存储结构</h3><p>B+树：B+树数据结构以平衡树的形式来组织，因为是树型结构，所以更适合用来处理排序，范围查找等功能。<br>相对 Hash ，B+树在查找单条记录的速度虽然比不上 Hash ，但是因为更适合排序等操作，所以他更受用户的欢迎。<br>毕竟不可能只对数据库进行单条记录的操作.</p><p>Hash：Hash 用的较少，它是把数据的索引以 Hash 形式组织起来，因此当查找某一条记录，速度非常快。<br>缺点是 Hash 结构，每个键只对应一个值，分布方式是散列的。所以 Hash 并不支持范围查找和排序等功能.</p><h2 id="数据库索引的增删改"><a href="#数据库索引的增删改" class="headerlink" title="数据库索引的增删改"></a>数据库索引的增删改</h2><h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><ol><li>执行 CREATE TABLE 语句时可以创建索引, 见<a href="/2016/05/10/mysql_1-1/">MySQL从入门到放弃 – 语法1:数据库表操作</a>。</li><li><p>使用 ALTER TABLE 语句时可以创建索引</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;ALTER TABLE table_name ADD INDEX index_name (column_list) COMMENT <span class="string">'普通索引'</span>;</span><br><span class="line">mysql&gt;ALTER TABLE table_name ADD UNIQUE index_name (column_list) COMMENT <span class="string">'唯一索引'</span>;</span><br><span class="line">mysql&gt;ALTER TABLE table_name ADD PRIMARY KEY (column_list) COMMENT <span class="string">'主键'</span>;</span><br><span class="line">mysql&gt;ALTER TABLE table_name ADD FULLTEXT index_name (column_list) COMMENT <span class="string">'全文索引'</span>;</span><br></pre></td></tr></table></figure></li><li><p>使用 CREATE INDEX 语句时可以创建索引<br><strong>注</strong>: 不能用CREATE INDEX语句创建PRIMARY KEY索引</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;CREATE INDEX index_name ON table_name (column_list) COMMENT <span class="string">'普通索引'</span>;</span><br><span class="line">mysql&gt;CREATE UNIQUE INDEX index_name ON table_name (column_list) COMMENT <span class="string">'唯一索引;</span></span><br><span class="line"><span class="string">mysql&gt;CREATE FULLTEXT INDEX index_name ON table_name (column_list) COMMENT '</span>全文索引<span class="string">';</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><p>如删除多列组合索引的某列，则该列也会从索引中删除。<br>如删除组合索引的所有列，则整个索引将被删除。</p><p>只使用 DORP 关键字<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;DROP INDEX 索引名 ON 表名 列名;</span><br></pre></td></tr></table></figure></p><p>使用 ALTER DORP 两个关键字<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;ALTER TABLE 表名 DROP INDEX 索引名 列名;</span><br><span class="line">mysql&gt;ALTER TABLE 表名 DROP UNIQUE 索引名 列名;</span><br><span class="line"></span><br><span class="line">// 因为一个表只可能有一个PRIMARY KEY索引，因此也可不指定索引名。</span><br><span class="line">// 如果没有创建PRIMARY KEY索引，但表具有一个或多个UNIQUE索引，则MySQL将删除第一个UNIQUE索引。</span><br><span class="line">mysql&gt;ALTER TABLE 表名 DROP PRIMARY KEY 索引名 列名;</span><br></pre></td></tr></table></figure></p><h3 id="重建索引"><a href="#重建索引" class="headerlink" title="重建索引"></a>重建索引</h3><p>长时间运行数据库后，索引有可能被损坏，这时就需要重建。重建索引可以提高检索效率。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;REPAIR TABLE table_name QUICK;</span><br></pre></td></tr></table></figure></p><h3 id="查询索引"><a href="#查询索引" class="headerlink" title="查询索引"></a>查询索引</h3><p>使用 INDEX 关键字<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;SHOW INDEX FROM table_name;</span><br></pre></td></tr></table></figure></p><p>使用 KEYS 关键字<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;SHOW KEYS FROM table_name;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据库索引的概念&quot;&gt;&lt;a href=&quot;#数据库索引的概念&quot; class=&quot;headerlink&quot; title=&quot;数据库索引的概念&quot;&gt;&lt;/a&gt;数据库索引的概念&lt;/h2&gt;&lt;h3 id=&quot;常见索引类型&quot;&gt;&lt;a href=&quot;#常见索引类型&quot; class=&quot;headerli
      
    
    </summary>
    
    
      <category term="MySQL" scheme="http://luckymartinlee.github.io/tags/MySQL/"/>
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>MySQL从入门到放弃(三) -- 字段</title>
    <link href="http://luckymartinlee.github.io/2016/05/20/mysql_1-3/"/>
    <id>http://luckymartinlee.github.io/2016/05/20/mysql_1-3/</id>
    <published>2016-05-20T02:37:45.000Z</published>
    <updated>2018-05-09T00:49:50.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="新增字段"><a href="#新增字段" class="headerlink" title="新增字段"></a>新增字段</h3><p>新增字段时，关键词 COLUMN 可以省略</p><p>新增一个字段，默认值为空<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE `user` ADD COLUMN `new_feild1` VARCHAR(20) DEFAULT NULL;</span><br></pre></td></tr></table></figure></p><p>新增一个字段，默认值为不能为空，备注 ‘备用字段’，新字段 放在 username 后<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE `user` ADD COLUMN `new_feild2` VARCHAR(20) NOT NULL COMMENT <span class="string">'备用字段'</span> AFTER `username`;;</span><br></pre></td></tr></table></figure></p><h3 id="删除字段"><a href="#删除字段" class="headerlink" title="删除字段"></a>删除字段</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE `user` DROP COLUMN `new_feild1`;</span><br></pre></td></tr></table></figure><h3 id="修改字段"><a href="#修改字段" class="headerlink" title="修改字段"></a>修改字段</h3><p>修改字段有两种方式<br>方式一 使用 MODIFY 关键字 (主要用于修改字段类型)<br>修改类型为 VARCHAR(10)，默认值为 空字符串<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE `user` MODIFY `new_feild2` VARCHAR(10) DEFAULT <span class="string">''</span> COMMENT <span class="string">'修改字段'</span>;</span><br></pre></td></tr></table></figure></p><p>方式二 使用 CHANGE 关键字 (主要用于修改字段名称)<br>修改字段名，并设为整型，默认值为 0<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE `user` CHANGE `new_feild2` `new_feild3` INT DEFAULT 0 COMMENT <span class="string">'修改字段'</span>;</span><br></pre></td></tr></table></figure></p><p>何时使用 MODIFY ，何时使用 CHANGE ，其实无可厚非的，最主要是个人的习惯。</p><h3 id="查询字段名和字段注释"><a href="#查询字段名和字段注释" class="headerlink" title="查询字段名和字段注释"></a>查询字段名和字段注释</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT `COLUMN_NAME`,`COLUMN_COMMENT`,`COLUMN_TYPE` FROM `information_schema`.`COLUMNS` WHERE TABLE_NAME=<span class="string">'表名'</span> AND TABLE_SCHEMA=<span class="string">'数据库名'</span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;新增字段&quot;&gt;&lt;a href=&quot;#新增字段&quot; class=&quot;headerlink&quot; title=&quot;新增字段&quot;&gt;&lt;/a&gt;新增字段&lt;/h3&gt;&lt;p&gt;新增字段时，关键词 COLUMN 可以省略&lt;/p&gt;
&lt;p&gt;新增一个字段，默认值为空&lt;br&gt;&lt;figure class=&quot;hi
      
    
    </summary>
    
    
      <category term="MySQL" scheme="http://luckymartinlee.github.io/tags/MySQL/"/>
    
      <category term="数据库" scheme="http://luckymartinlee.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
</feed>
