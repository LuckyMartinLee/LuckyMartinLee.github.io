{"meta":{"title":"Martin Li's Personal Website - 李杨的个人站点","subtitle":null,"description":null,"author":"Martin Li","url":"http://luckymartinlee.github.io"},"pages":[{"title":"Martin Li (李 杨)","date":"2017-06-18T02:01:39.000Z","updated":"2020-12-10T06:07:24.815Z","comments":true,"path":"about/index.html","permalink":"http://luckymartinlee.github.io/about/index.html","excerpt":"","text":"Photo&ensp;&ensp;Click here About Me&ensp;&ensp;My name is Martin Li. And you can call me Li Yang (李 杨). I was born in 1988. &ensp;&ensp;I have a Bachelor degree, and once worked for Iflytek CO.,LTD. (科大讯飞) in Hefei AnHui as a senior software engineer. &ensp;&ensp;I am a software developer focusing on web technology, big data and a strong advocate and believer of Free Software. Now I am employed by LexisNexis (律商联讯) as a Manager Software Engineering. &ensp;&ensp;In my spare time, I have broad interests like many other young people. I like reading book, exchanging with other people in the forum on line, watching movie and taking a leisurely walk outdoors. Skills&ensp;&ensp;Python: ★★★★☆&ensp;&ensp;Scala: ★★★★☆&ensp;&ensp;PHP: ★★★★☆&ensp;&ensp;Java: ★★★★☆&ensp;&ensp;Spark: ★★★★☆&ensp;&ensp;Solr: ★★★★☆&ensp;&ensp;Elasticsearch: ★★★★☆&ensp;&ensp;MySQL: ★★★★☆&ensp;&ensp;MongoDB: ★★★★☆&ensp;&ensp;Hadoop: ★★★☆☆&ensp;&ensp;Docker: ★★★★☆&ensp;&ensp;Git/SVN: ★★★★☆&ensp;&ensp;Nginx: ★★★★☆&ensp;&ensp;NodeJs: ★★★☆☆&ensp;&ensp;Javascript: ★★★☆☆&ensp;&ensp;Html+Css: ★★★☆☆ Works&ensp;&ensp;Global China Law&ensp;&ensp;律商网&ensp;&ensp;雅集网&ensp;&ensp;畅言云系列&ensp;&ensp;&ensp;&ensp;畅言云&ensp;&ensp;&ensp;&ensp;畅言-教学通&ensp;&ensp;&ensp;&ensp;畅言云-资源中心 ContactsEmail:&ensp;&ensp;lucky.martin.lee@gmail.com&ensp;&ensp;541079843@qq.comWechat:&ensp;&ensp;"},{"title":"Martin Li (李 杨)","date":"2018-04-20T07:22:10.000Z","updated":"2020-12-10T06:12:20.137Z","comments":true,"path":"martin_photo/index.html","permalink":"http://luckymartinlee.github.io/martin_photo/index.html","excerpt":"","text":"&nbsp;&nbsp;&ensp;&ensp; &ensp;&ensp;2017年4月,摄于上海, LexisNexis Annual Hackathon 2017 &nbsp;&nbsp;&ensp;&ensp; &ensp;&ensp;2016年10月,摄于 北京"}],"posts":[{"title":"压力测试工具 ab (apache bench) 使用","slug":"ab_1-1","date":"2019-05-10T10:13:12.000Z","updated":"2020-12-07T12:52:37.204Z","comments":true,"path":"2019/05/10/ab_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2019/05/10/ab_1-1/","excerpt":"","text":"网络服务性能相关概念服务器平均请求处理时间（Time per request: across all concurrent requests）即是在某个并发用户数下服务器处理一条请求的平均时间 服务器平均请求处理时间 = 处理完成所有请求数所花费的时间 / 总请求数Time per request(across all concurrent requests) = Time taken for / testsComplete requests 用户平均请求等待时间（Time per request）即是用户获得相应的平均等待时间 用户平均请求等待时间 = 处理完成所有请求数所花费的时间/ （总请求数 / 并发用户数），即Time per request = Time taken for tests /（ Complete requests / Concurrency Level） 吞吐率（Requests per second，QPS，RPS）即是在某个并发用户数下单位时间内处理的请求数,单位是reqs/s, 也是 “服务器平均请求处理时间” d的倒数。某个并发用户数下单位时间内能处理的最大请求数，称之为最大吞吐率。 吞吐率 = 总请求数 / 处理完成这些请求数所花费的时间Request per second = Complete requests / Time taken for tests 并发连接数（The number of concurrent connections）即是在某个时刻服务器所接受的请求数目，就是会话数量。 并发用户数（The number of concurrent users，Concurrency Level）即是指某个时刻使用系统的用户数(可能有一个用户有一个或者多个连接)。要注意区分和并发连接数之间的区别，一个用户可能同时会产生多个会话，也即连接数。 总结:如果要说QPS时，一定需要指明是多少并发用户数下的QPS，否则豪无意义。因为单用户数的40QPS和20并发用户数下的40QPS是两个不同的概念，前者说明该服务可以在一秒内串行执行40个请求，而后者说明在并发20个请求的情况下，一秒内该应用能处理40个请求。当QPS相同时，越大的并发用户数，说明了网站服务并发处理能力越好。对于当前的web服务器，其处理单个用户的请求肯定戳戳有余，这个时候会存在资源浪费的情况（一方面该服务器可能有多个cpu，但是只处理单个进程，另一方面，在处理一个进程中，有些阶段可能是IO阶段，这个时候会造成CPU等待，但是有没有其他请求进程可以被处理）。而当并发数设置的过大时，每秒钟都会有很多请求需要处理，会造成进程（线程）频繁切换，反正真正用于处理请求的时间变少，每秒能够处理的请求数反而变少，同时用户的请求等待时间也会变大，甚至超过用户的心理底线，等待时间过长。 所以在最小并发数和最大并发数之间，一定有一个最合适的并发数值，在并发数下，QPS能够达到最大。 但是，这个并发并非是一个最佳的并发，因为当QPS到达最大时的并发，可能已经造成用户的等待时间变得超过了其最优值，所以对于一个系统，其最佳的并发数，一定需要结合QPS，用户的等待时间来综合确定。 下面这张图是应用服务器关于并发用户数，QPS，用户平均等待时间的一张关系图，对于实际的系统，也应该是对于不同的并发数，进行多次测试，获取到这些数值后，画出这样一张图出来，以便于分析出系统的最佳并发用户数。 响应时间关系图 ab 简介和使用实例简介ab全称为：apache bench它是apache自带的压力测试工具。ab非常实用，它不仅可以对apache服务器进行网站访问压力测试，也可以对或其它类型的服务器进行压力测试。比如nginx、tomcat、IIS等。我们可以直接安装apache的工具包httpd-tools。如下：1yum -y install httpd-tools 使用ab –V命令检测是否安装成功。如下：1ab -V 实例1ab -n 100 -c 10 -H \"token: fioj3iorm2aoi4ej\" -p /data/postdata.txt -T application/x-www-form-urlencoded \"http://127.0.0.1/test\" 上面命令的含义是设置请求header中参数token = fioj3iorm2aoi4ej，向 http://127.0.0.1/test 地址发送POST请求，POST表单数据存放在本地文件/data/postdata.txt中，其content-type格式为 application/x-www-form-urlencoded，发送的并发请求数为 10，总请求数为 100。成功后返回： 测试结果含义 返回值名称 含义 Server Software web服务器软件及版本 Server Hostname 表示请求的URL中的主机部分名称 Server Port 被测试的Web服务器的监听端口 Document Path 请求的页面路径 Document Length 页面大小 Concurrency Level 并发请求数 Time taken for tests 整个测试持续的时间,测试总共花费的时间 Complete requests 完成的请求数 Failed requests 失败的请求数，这里的失败是指请求的连接服务器、发送数据、接收数据等环节发生异常，以及无响应后超时的情况。对于超时时间的设置可以用ab的-t参数。如果接受到的http响应数据的头信息中含有2xx以外的状态码，则会在测试结果显示另一个名为“Non-2xx responses”的统计项，用于统计这部分请求数，这些请求并不算是失败的请求。 Write errors 写入错误 Total transferred 总共传输字节数,整个场景中的网络传输量,包含http的头信息等。使用ab的-v参数即可查看详细的http头信息。 HTML transferred html字节数，整个场景中的HTML内容传输量。也就是减去了Total transferred中http响应数据中头信息的长度。 Requests per second 每秒处理的请求数，服务器的吞吐量，大家最关心的指标之一 Time per request 用户平均请求等待时间，大家最关心的指标之二 Time per request 服务器平均处理时间，大家最关心的指标之三 Transfer rate 平均传输速率（每秒收到的速率）平均每秒网络上的流量，可以很好的说明服务器在处理能力达到限制时，其出口带宽的需求量，也可以帮助排除是否存在网络流量过大导致响应时间延长的问题。 下面段表示网络上消耗的时间的分解 下面这段是每个请求处理时间的分布情况，50%的处理时间在4930ms内，66%的处理时间在5008ms内…，重要的是看90%的处理时间。 常见问题压力测试需要当登录怎么办？1、先用账户和密码在浏览器登录后，用开发者工具找到会话的Cookie值（Session ID）记下来。2、使用下面命令传入Cookie值1ab －n 100 －C key＝value http://127.0.0.1/test","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://luckymartinlee.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://luckymartinlee.github.io/tags/Shell/"}]},{"title":"Spark 算子详解","slug":"spark_1-2","date":"2019-05-10T10:13:12.000Z","updated":"2020-12-09T08:49:01.150Z","comments":true,"path":"2019/05/10/spark_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2019/05/10/spark_1-2/","excerpt":"","text":"算子分类1、 Transformation算子(转换算子)，此类算子操作是延迟计算的，即是将要从一个RDD转换成另一个RDD的操作，但不是马上执行，并不触发提交Job作业，需要等到有Action操作的时候才会真正触发运算。根据操作数据类型的不同，可细分为 Value数据类型的Transformation算子 和 Key-Value数据类型的Transfromation算子2、Action算子(行动算子), 这类算子会触发 SparkContext 提交Job作业，并将数据输出 Spark系统。 重难 Transformation算子glom该函数是将RDD中每一个分区中各个元素合并成一个Array，这样每一个分区就只有一个数组元素1234val a = sc.parallelize(1 to 9, 3)a.glom.collect//输出res66: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9)) mapPartitions(function)与 map 类似，但函数单独在 RDD 的每个分区上运行12345678910val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list, 3).mapPartitions(iterator =&gt; &#123; val buffer = new ListBuffer[Int] while (iterator.hasNext) &#123; buffer.append(iterator.next() * 100) &#125; buffer.toIterator&#125;).foreach(println)//输出结果100 200 300 400 500 600 join在一个(K, V)和(K, W)类型的RDD 上调用时，返回一个(K, (V, W)) pairs 的 RDD，等价于内连接操作(不含 V或W 为空的)。执行外连接，可以使用：leftOuterJoin (不含 W 为空)rightOuterJoin (不含 W 为空)fullOuterJoin (包含V 和 W 为空) sample数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction,小于等于1)、随机数生成器的种子 (seed)123val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list).sample(withReplacement = false, fraction = 0.5).foreach(println)//输出结果随机 groupByKey按照键进行分组,在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable)123456val list = List((\"hadoop\", 2), (\"spark\", 3), (\"spark\", 5), (\"storm\", 6), (\"hadoop\", 2))sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)//输出：(spark,List(3, 5))(hadoop,List(2, 2))(storm,List(6)) cogroup先同一个 (K, V) RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组12345678val list01 = List((1, \"a\"),(1, \"a\"), (2, \"b\"), (3, \"e\"))val list02 = List((1, \"A\"), (2, \"B\"), (3, \"E\"))val list03 = List((1, \"[ab]\"), (2, \"[bB]\"), (3, \"eE\"),(3, \"eE\"))sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)// 输出(1,(CompactBuffer(a, a),CompactBuffer(A),CompactBuffer([ab])))(3,(CompactBuffer(e),CompactBuffer(E),CompactBuffer(eE, eE)))(2,(CompactBuffer(b),CompactBuffer(B),CompactBuffer([bB])) reduceByKey按照键进行归约操作123456val list = List((\"hadoop\", 2), (\"spark\", 3), (\"spark\", 5), (\"storm\", 6), (\"hadoop\", 2))sc.parallelize(list).reduceByKey(_ + _).foreach(println)//输出(spark,8)(hadoop,4)(storm,6) sortBy(function) &amp; sortByKey按照键进行排序，需要 collect 等action算子后才是有序的123456val list01 = List((100, \"hadoop\"), (90, \"spark\"), (120, \"storm\"))sc.parallelize(list01).sortByKey(ascending = false).collect.foreach(println)// 输出(120,storm)(100,hadoop)(90,spark) 按照指定function进行排序,需要 collect 等action算子后才是有序的123456val list02 = List((\"hadoop\",100), (\"spark\",90), (\"storm\",120))sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=false).collect.foreach(println)// 输出(storm,120)(hadoop,100)(spark,90) aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])当针对(K，V)对的数据集时，返回（K，U）对的数据集，先对分区内执行seqOp函数，zeroValue 聚合每个键的值，再对分区间执行combOp函数。与groupByKey 类似，reduce 任务的数量可通过第二个参数 numPartitions 进行配置。示例如下：12345678910// 为了清晰，以下所有参数均使用具名传参val list = List((\"hadoop\", 3), (\"hadoop\", 2), (\"spark\", 4), (\"spark\", 3), (\"storm\", 6), (\"storm\", 8))sc.parallelize(list,numSlices = 2).aggregateByKey(zeroValue = 0,numPartitions = 3)( seqOp = math.max(_, _), combOp = _ + _ ).collect.foreach(println)//输出结果：(hadoop,3)(storm,8)(spark,7) 这里使用了 numSlices = 2 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下： 基于同样的执行流程，如果 numSlices = 1，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：123(hadoop,3)(storm,8)(spark,4) 同样的，如果每个单词对一个分区，即 numSlices = 6，此时相当于求和操作，执行结果为：123(hadoop,5)(storm,14)(spark,7) aggregateByKey(zeroValue = 0,numPartitions = 3) 的第二个参数 numPartitions 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 getNumPartitions 方法获取分区数量 combineByKeyC C, mergeValue:(C, V) C, mergeCombiners:(C, C) C, partitioner:Partitioner, mapSideCombine:Boolean=true, serializer:Serializer=null):RDD[(K,C)] 参数：createCombiner:V=&gt;C 分组内的创建组合的函数。即是对读进来的数据进行初始化，其把当前的值作为参数，可以对该值做一些转换操作，转换为我们想要的数据格式参数：mergeValue:(C,V)=&gt;C 该函数主要是分区内的合并函数，作用在每一个分区内部。其功能主要是将V合并到之前(createCombiner)的元素C上,注意，这里的C指的是上一函数转换之后的数据格式，而这里的V指的是原始数据格式(上一函数为转换之前的)参数：mergeCombiners:(C,C)=&gt;R 该函数主要是进行分区之间合并，此时是将两个C合并为一个C，例如两个C:(Int)进行相加之后得到一个R:(Int)参数：partitioner:自定义分区数，默认是hashPartitioner参数：mapSideCombine:Boolean=true 该参数是设置是否在map端进行combine操作，为了减小传输量，很多 combine 可以在 map 端先做，比如叠加，可以先在一个 partition 中把所有相同的 key 的 value 叠加，参数：serializerClass： String = null，传输需要序列化，用户可以自定义序列化类 12345678910val ls3 = List((\"001\", \"011\"), (\"001\",\"012\"), (\"002\", \"011\"), (\"002\", \"013\"), (\"002\", \"014\"))val d1 = sc.parallelize(ls3,2)d1.combineByKey((v: (String)) =&gt; (v, 1),(acc: (String, Int),v: (String)) =&gt; (v+\":\"+acc._1,acc._2+1),(p1:(String,Int),p2:(String,Int)) =&gt; (p1._1 + \":\" + p2._1,p1._2 + p2._2)).collect().foreach(println)//输出(002,(014:013:011,3))(001,(012:011,2)) 重难 Action算子takeOrdered按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 n 个元素。需要注意的是 takeOrdered 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 Ordering[T] 实现自定义比较器，然后将其作为隐式参数引入。1234567891011// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序class CustomOrdering extends Ordering[(Int, String)] &#123; override def compare(x: (Int, String), y: (Int, String)): Int = if (x._2.length &gt; y._2.length) 1 else -1&#125;val list = List((1, \"hadoop\"), (1, \"storm\"), (1, \"azkaban\"), (1, \"hive\"))// 引入隐式默认值implicit val implicitOrdering = new CustomOrderingsc.parallelize(list).takeOrdered(5)//输出Array((1,hive), (1,storm), (1,hadoop), (1,azkaban) take(n)将RDD中的前 n 个元素作为一个 array 数组返回,是无序的。 first返回 RDD 中的第一个元素，等价于 take(1)。 top（num：Int）（implicit ord：Ordering[T]）：Array[T]默认返回最大的k个元素，可以定义排序的方式Ordering[T]。12345678910class CustomOrdering extends Ordering[Int] &#123; override def compare(x: Int, y: Int): Int = if (x &gt; y) -1 else 1&#125;val list0 = List(3,5,1,6,2)// 引入隐式默认值implicit val implicitOrdering = new CustomOrderingsc.parallelize(list0).top(5)//输出Array(1,2,3,5,6)","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"Spark","slug":"Spark","permalink":"http://luckymartinlee.github.io/tags/Spark/"}]},{"title":"Java 线程","slug":"java_1-1","date":"2019-04-11T02:13:32.000Z","updated":"2020-12-15T08:38:07.774Z","comments":true,"path":"2019/04/11/java_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2019/04/11/java_1-1/","excerpt":"","text":"进程与线程进程: 是程序的一次动态执行过程,经历代码加载，代码执行到执行完毕的一个完整的过程。多进程操作系统能同时达运行多个进程，由于 CPU 具备分时机制，所以每个进程都能循环获得自己的CPU 时间片。由于 CPU 执行速度非常快，使得所有程序好像是在同时运行一样。线程: 是进程在执行过程中产生的多个更小的程序单元，这些更小的单元称为线程，这些线程可以同时存在，同时运行，一个进程可能包含多个同时执行的线程。进程和线程一样，都是实现并发的一个基本单位。线程是比进程更小的执行单位，线程是进程的基础之上进行进一步的划分。 Java 中线程实现Java 中实现多线程有两种手段，一种是继承 Thread 类，另一种就是实现 Runnable 接口 实现Runnable 接口方式123456789101112// 实现Runnable接口，作为线程的实现类class MyThread implements Runnable&#123; private String name ; // 表示线程的名称 public MyThread(String name)&#123; this.name = name ; // 通过构造方法配置name属性 &#125; public void run()&#123; // 覆写run()方法，作为线程 的操作主体 for(int i=0;i&lt;10;i++)&#123; System.out.println(name + \"运行，i = \" + i) ; &#125; &#125;&#125; 继承 Thread 类方式123456789101112// 继承Thread类，作为线程的实现类class MyThread extends Thread&#123; private String name ; // 表示线程的名称 public MyThread(String name)&#123; this.name = name ; // 通过构造方法配置name属性 &#125; public void run()&#123; // 覆写run()方法，作为线程 的操作主体 for(int i=0;i&lt;10;i++)&#123; System.out.println(name + \"运行，i = \" + i) ; &#125; &#125;&#125; 二者区别于联系Runnable的实现方式是实现其接口，支持多继承，但基本上用不到Thread的实现方式是继承其类Thread实现了Runnable接口并进行了扩展，Thread和Runnable的实质是继承关系，没有可比性。无论使用Runnable还是Thread，都需要new Thread，然后执行 start 方法。用法上，如果有复杂的线程操作需求，那就选择继承Thread，如果只是简单的执行一个任务，那就实现runnable。 Java 中线程 状态变化创建状态创建了一个线程对象后，新的线程对象便处于新建状态, 此时它已经有了相应的内存空间和其他资源1Thread thread=new Thread(); 就绪状态调用该线程的 start() 方法就可以启动线程。当线程启动时，线程进入就绪状态。此时，线程将进入线程队列排队，等待 CPU 服务，这表明它已经具备了运行条件1thread.start(); 运行状态线程队列中线程获得CPU资源时，线程就进入了运行状态, 此时，自动调用该线程对象的 run() 方法 阻塞状态处于运行状态的线程在某些特殊情况下，如被人为挂起或需要执行耗时的输入/输出操作，会让 CPU 暂时中止自己的执行，此时，进入阻塞状态。或者，在运行状态下，调用了sleep(),suspend()(过时弃用),wait() 等方法，线程都将进入阻塞状态，发生阻塞时线程不能进入排队队列，只有当引起阻塞的原因被消除后，线程才可以转入就绪状态 死亡状态线程调用 stop() 方法时或 run() 方法执行结束后，即处于死亡状态 Java 中线程常用方法thread.sleep()线程休眠: 线程暂缓执行，进入阻塞状态，等到预计时间再执行。线程休眠会交出CPU，让CPU去执行其他的任务。但是有一点要非常注意，sleep方法不会释放锁，也就是说如果当前线程持有对某个对象的锁，则即使调用sleep方法，其他线程也无法访问这个对象 thread.join()等待线程终止: 指在主线程中调用该方法时就会让主线程休眠，进入阻塞状态，让调用join()方法的线程先执行完毕后再开始执行主线程。 thread.yield()线程让步: 暂停当前正在执行的线程对象，并执行其它线程交出cpu, 但不释放锁，不进入阻塞状态，直接进入就绪状态 thread.interrupt()设置中断标志: 只是改变中断状态而已，它不会中断一个正在运行的线程。具体来说就是，调用interrupt()方法只会给线程设置一个为true的中断标志，而设置之后，则根据线程当前状态进行不同的后续操作1、如果线程的当前状态出于非阻塞状态，那么仅仅将线程的中断标志设置为true而已;2、如果线程的当前状态出于阻塞状态，那么将在中断标志设置为true后，还会出现wait()、sleep()、join()方法之一引起的阻塞，那么会将线程的中断标志位重新设置为false，并抛出一个InterruptedException异常。3、如果在中断时，线程正处于非阻塞状态，则将中断标志修改为true，而在此基础上，一旦进入阻塞状态，则按照阻塞状态的情况来进行处理。例如，一个线程在运行状态时，其中断标志设置为true之后，一旦线程调用了wait()、sleep()、join()方法中的一种，立马抛出一个InterruptedException异常，且中断标志被程序自动清除，重新设置为false。调用Thread类的interrupted()方法，其本质只是设置该线程的中断标志，将中断标志设置为true，并根据线程状态决定是否抛出异常 object.wait()线程等待: 让当前正在执行的线程进入线程阻塞状态的等待状态，该方法时用来将当前线程置入“预执行队列”中，并且调用wait()方法后，该线程在wait()方法所在的代码处停止执行，直到接到一些通知或被中断为止1、wait()方法只能在同步代码块或同步方法中调用，故如果调用wait()方法时没有持有适当的锁时，就会抛出异常。2、wait()方法执行后，当前线程释放锁并且与其他线程相互竞争重新获得锁。 object.notify()线程唤醒: notify()方法要在同步代码块或同步方法中调用,用来通知那些等待该对象的对象锁的线程，对其调用wait()方法的对象发出通知让这些线程不再等待，继续执行.如果有多个线程都在等待，则由线程规划器随机挑选出一个呈wait状态的线程将其线程唤醒，继续执行该线程.注意：调用notify()方法后，当前线程并不会马上释放该对象锁，要等到执行notify()方法的线程执行完才会释放对象锁 object.notifyAll()线程唤醒: notifyAll()方法将同一对象锁的所有等待线程全部唤醒 线程状态转换关系图","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://luckymartinlee.github.io/tags/Java/"}]},{"title":"Spark RDD的Stage划分","slug":"spark_1-1","date":"2018-10-13T02:23:11.000Z","updated":"2020-12-10T01:00:21.602Z","comments":true,"path":"2018/10/13/spark_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/10/13/spark_1-1/","excerpt":"","text":"什么是RDDRDD(resilient distributed dataset) 弹性分布式数据集,RDD表示一个不可变的、可分区的、支持并行计算的元素集合（类似于 Scala 中的不可变集合），RDD可以通过 HDFS、Scala集合、RDD转换、外部的数据集（支持InputFormat）获取，并且 Spark 将 RDD 存储在内存中，可以非常高效的重复利用或者在某些计算节点故障时自动数据恢复。 RDD依赖 - lineage(血统)在对RDD应用转换操作时，产生的新 RDD 对旧 RDD 会有一种依赖关系称为 Lineage(血统).Spark应用在计算时会根据 Lineage 逆向推导出所有Stage（阶段），每一个 Stage 的分区数量决定了任务的并行度，一个 Stage 实现任务的本地计算（大数据计算时网络传输时比较耗时的. RDD 两种 Lineage 关系，宽窄依赖,v它们和Stage划分有极为紧密关系窄依赖 (Narrow Dependency): 父RDD的一个分区对应一个子RDD的分区（1:1）或者多个父RDD的分区对应一个子RDD的分区（N：1）. 宽依赖 (Wide Dependency): 父RDD的一个分区对应多个子RDD的分区（1：N）.","categories":[],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"Spark","slug":"Spark","permalink":"http://luckymartinlee.github.io/tags/Spark/"}]},{"title":"Hadoop从入门到放弃(四) -- YARN","slug":"hadoop-1-4","date":"2018-06-22T07:15:21.000Z","updated":"2020-12-07T12:52:05.467Z","comments":true,"path":"2018/06/22/hadoop-1-4/","link":"","permalink":"http://luckymartinlee.github.io/2018/06/22/hadoop-1-4/","excerpt":"","text":"什么是 YARNYARN 是 Hadoop2.0 以后的资源管理器，负责整个集群资源的管理和调度 基本概念ResourceManager负责: 分配和调度资源 启动并监控 ApplicationMaster 监控 NodeManager ApplicationMaster负责: 为 MapReduce 类型程序申请资源，并分配给内部任务 负责数据的切分 监控任务的执行以及容错 NodeManager负责: 管理单个节点 处理来自 ResouceManager 的命令 处理来自 ApplicationMaster 的命令","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://luckymartinlee.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"YARN","slug":"YARN","permalink":"http://luckymartinlee.github.io/tags/YARN/"}]},{"title":"Hadoop从入门到放弃(三) -- MapReduce","slug":"hadoop-1-3","date":"2018-06-12T07:15:40.000Z","updated":"2020-12-07T12:51:53.831Z","comments":true,"path":"2018/06/12/hadoop-1-3/","link":"","permalink":"http://luckymartinlee.github.io/2018/06/12/hadoop-1-3/","excerpt":"","text":"MapReduce 编程模型举个栗子：123输入一个大文件，通过Split切分后，将其分成多个分片，每个文件分片，由单独的机器去处理，这就是 Map 方法，然后，将各个机器的计算结果进行汇总并得到最终的结果，这就是 Reduce 方法。","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://luckymartinlee.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"},{"name":"MapReduce","slug":"MapReduce","permalink":"http://luckymartinlee.github.io/tags/MapReduce/"}]},{"title":"MySQL查询优化(一)","slug":"mysql-2-1","date":"2018-05-22T10:45:50.000Z","updated":"2018-05-25T03:31:30.000Z","comments":true,"path":"2018/05/22/mysql-2-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/05/22/mysql-2-1/","excerpt":"","text":"索引建立技巧单个字段 “等于” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` = 15 这种情况下，毫无疑问，需要给字段 (f1) 加上索引。 形如:1SELECT `f2`,`f3` FROM `tabel1` WHERE `f1` = 15 此时应该创建 (f1,f2,f3) 索引，此索引起到覆盖索引的作用，效率比 (f1) 索引高。切记，不应该创建 (f2,f3,f1) 索引，因为根据索引 “最左原则”，它对 f1 字段起不到查询过滤作用。 多个字段 “等于” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` = 15 AND `f2` = \"abc\" 这种情况下，应该创建 (f1,f2)索引 或者 (f2,f1)索引 都是可以的。有人会问，如果创建两个单独是索引，分别是 (f1)索引 和 (f2)索引 可以吗？这里不建议这么做，虽然MySQL根据index_merge算法能同时使用这两个索引，但这样效率依旧不如上面联合索引。 字段 “等于” 和 “不等于” 混合查询形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f2` = \"abc\" 对于这种情况，我们要小心处理，因为只要有一列使用了不等于计算，那么它将阻止其他列使用索引。此时我们应该创建 (f2,f1) 索引，这时候f1和f2两个条件都会走索引，这才是我们想要的。而不是 (f1,f2) 索引，这种情况下，只有 f1 会使用索引，相对来说效率较低。 形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f3` &lt; 100 AND `f2` = \"abc\" 这是有两个 “不等于” 查询，因此我们不可能做到 f1,f2,f3都做到被索引覆盖，此时需要依据实际数据情况，建立 (f2,f1)索引 或 (f2,f3)索引，其中关键是，一定要把 “等于” 字段，放在索引的最左侧。 多个字段 “等于” 和 “排序” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` = 15 AND `f2` = \"abc\" ORDER BY `f3` 此时我们建立索引的字段顺序，应该是: “先是过滤字段，后是排序字段”，所以此处应该建立 (f1,f2,f3)索引 或者 (f2,f1,f3)索引。而不应该 建立 (f3,f1,f2)索引 或者 (f3,f2,f1)索引，因为这些只使用了索引排序，没有使用索引过滤。 形如:1SELECT `f4`,`f5` FROM `tabel1` WHERE `f1` = 15 AND `f2` = \"abc\" ORDER BY `f3` 此时我们可以创建 (f1,f2,f3,f4,f5)索引 或 (f2,f1,f3,f4,f5)索引，起到了 过滤，排序和覆盖 三个作用。 字段 “不等于” 和 “排序” 查询形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 AND `f2` = \"abc\" ORDER BY `f3` 此时，需要根据实际数据情况，选择建立 (f2,f1)索引 或 (f2,f3)索引 形如:1SELECT * FROM `tabel1` WHERE `f1` &gt; 15 ORDER BY `f3` 此时，只可能一个字段使用到索引，要么使用 (f1)索引，要么使用 (f2)索引，这要依据集体的数据情况，一般情况下会使用过滤索引，也就是 (f1)索引。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"}]},{"title":"Hadoop从入门到放弃(二) -- HDFS","slug":"hadoop-1-2","date":"2018-05-19T07:15:45.000Z","updated":"2020-12-07T12:52:08.189Z","comments":true,"path":"2018/05/19/hadoop-1-2/","link":"","permalink":"http://luckymartinlee.github.io/2018/05/19/hadoop-1-2/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Git 分支策略","slug":"git-1-1","date":"2018-04-28T02:46:43.000Z","updated":"2018-05-19T02:45:22.000Z","comments":true,"path":"2018/04/28/git-1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/04/28/git-1-1/","excerpt":"","text":"常驻分支（主分支）12master 分支develop 分支 master 分支即是 Git 默认主分支，只用来发布重大版本，是生产环境出于准备就绪状态的最新源码分支。需要对此分支进行严格的控制，可以为每次 master 分支的提交都挂一个钩子脚本，向生产环境自动化构建并发布我们的软件产品。 develop 分支develop 分支作为日常开发分支，可以理解为准备下一次发布的，开发人员最后一次提交的源码分支，这个分支也叫做 集成分支，此分支也作为每日构建（nightly build）自动化任务的源码分支 临时分支（支持型分支）123feature 功能开发分支（也叫 topic 分支）release 预发布分支hotfix 修补 issue 分支 这些分支是为了准备发布新产品，开发新的功能特性，快速或者紧急修复上线等任务而设立的分支，这些分支都是临时的，使命完成后，都应该删除。 release 分支123派生自：develop 分支需要合并回：develop 或者 master 分支分支命名规范：release-版本号 release 分支派生自 develop 分支。假设，当前的生产环境发布的版本（ mster 分支）是 1.1，我们确定新的版本号为 1.2 。通过下面命令派生一个新的 release 分支并以新的版本号为其命名：12$ git checkout -b release-1.2 develop$ git commit -a -m \"Bumped version number to 1.2\" 这个新的 release 分支，从创建到发布出去会存在一段时间，在此期间，可能会有issue修复（bug 修复直接在 release 分支上进行）分支，完成后并入 develop 分支，并放入下一次发布。release 分支真正发布成功后，还有下面的事要做：1234567891011// release 分支合并到master$ git checkout master$ git merge --no-ff release-1.2// 在 master 分支上的打一个 tag，作为标签以便作为版本历史的参考$ git tag -a 1.2// release 分支产生的改动合并回 develop，以便后续的发布同样包含对这些 bug 的修复$ git checkout develop// -no-ff 标记使得合并操作总是产生一次新的提交，避免所有提交的历史信息混在一起$ git merge --no-ff release-1.2// 至此 release 分支使命已经完成，应该删除它$ git branch -d release-1.2 feature 分支123派生自：develop 分支需要合并回：develop分支命名规范：feature-功能特性编号 feature 分支是用来开发即将发布的新的功能特性。feature 分支的生命周期会和新功能特性的开发周期保持同步，但是最终会合并回 develop 分支或被抛弃(功能特性不需要了)。feature 分支通常仅存在于开发者的代码库中，并不出现在 origin 里。123456789// 从 develop 派生出 feature 分支$ git checkout -b feature-12345 develop...// 功能开发完成后，合并回 develop 分支$ git checkout develop$ git merge --no-ff myfeature// feature 分支使命完成，应该删除$ git branch -d myfeature$ git push origin develop hotfix 分支123派生自：master 分支需要合并回：develop 和 master分支命名规范：hotfix-issue编号 hotfix 分支 是在实时的生产环境版本出现意外需要快速响应时，从 master 分支相应的 tag 被派生出来。这样做的原因，是为了让团队其中一个人来快速修复生产环境的问题，其他成员可以按工作计划继续工作下去而不受太大影响。假设，当前 master 版本是1.2 ，生产环境出现了较严重的 issue (假设，记录 bug 编号为 10002)，此时就要从 master 分支派生一个 hotfix 分支：` bash// 从 master 派生出 hotfix 分支$ git checkout -b hotfix-10002 master…// 修复 bug，提交代码$ git commit -m “Fixed severe production problem”// bug 修复完成后，hotfix 分支需要并回 master 和 develop 分支，以保证接下来的发布也都已经解决了这个 bug$ git checkout master$ git merge –no-ff hotfix-10002// 变更小版本号$ git tag -a 1.2.1$ git checkout develop$ git merge –no-ff hotfix-10002// hotfix 分支使命完成，应该删除$ git branch -d myfeature 下图形象的总结了以上分支之间的派生关系 本文是阅读了A successful Git branching model之后的自我总结。","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"http://luckymartinlee.github.io/tags/Git/"}]},{"title":"Hadoop从入门到放弃(一) -- 基础概念","slug":"hadoop_1-1","date":"2018-03-09T02:47:04.000Z","updated":"2020-12-07T12:51:03.015Z","comments":true,"path":"2018/03/09/hadoop_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/03/09/hadoop_1-1/","excerpt":"","text":"Hadoop 是什么Hadoop 是一个开源的大数据框架，一个分布式计算的解决方案。1Hadoop = HDFS(分布式文件系统) + MapReduce(分布式计算) HDFS 分布式文件系统: 海量存储是大数据的技术的基础MapReduce 编程模型: 分布式计算是大数据应用的解决方案 HDFS 概念数据块HDFS 上面的文件是按照数据块为单元来存储的，其默认大小为 64MB , 我们可以依据自己的情况进行设置，一般我们设置为 128MB, 备份数是3，也是可以修改的。数据块的大小设置，如设置的太小，小文件也会被切割成多个数据块，访问的时候就要查找多个数据块地址，效率比较低，同时 NameNode 存储了太多的数据块信息，对内存消耗比较多，内存压力大。如果数据块设置过大，就会降低数据并行操作的效率，同时如果系统重启，数据块越大，系统重启的时间就越长使用数据块存储的好处有： 屏蔽了文件的概念，无论 200KB 还是 200PB 的文件都是按照数据块进行存储，简化了存储系统的设计 数据块方便数据备份，提高数据容错能力 NameNodeNameNode 相当于 master - slave 体系中的 master , 他的职责有： 管理文件系统的命名空间 存放文件元数据 维护文件系统的所有文件和目录 维护文件与数据块的映射 记录每个文件的各个块所在数据节点 DataNode 的信息 DataNodeDataNode 是 HDFS 文件系统的工作节点，负责存储和检索数据块，向 NameNode 更新所存储块的列表 HDFS 优/缺点优点： 适合大文件存储，支持 TB、PB 级别数据存储，支持副本策略 HDFS 可以构建在廉价的普通机器上，具备容错和恢复机制 支持流逝数据访问，一次写入多次读取效率高缺点: 不适合大量小文件存储 不适合并发写入，不支持文件随机修改 不适合随机度等低延时的访问方式 HDFS 写流程1234561. Client 向 NameNode 发出写请求，表明要将 data 写入到集群当中。2. NameNode 中存储了集群中所有 DataNode 信息，收到 Client 请求后，就将可用的 DataNode 信息发送给Client3. Client 依据收到 NameNode 的信息，先将数据进行分块，如分成两块。然后将 数据块1 和 从NameNode接收到的 DataNode所有节点信息，都发送给 DataNode-14. DataNode-1 接收到信息后，先将 数据块1 进行保存，在依据接收的DataNode节点集群信息，将 数据块1 备份到 DataNode-2 和 DataNode-3。 当 DataNode-1，DataNode-2，DataNode-3 完成 数据块1 存储之后，反馈给 NameNode5. NameNode 收到 DataNode 发来的反馈信息后，更新自己的 DataNode 元数据信息列表。然后告诉 Client 数据块1 已经存储好了，可以存储后面的数据块了6. Client 收到 NameNode 信息后，开始重复 数据块1的存储步骤，存储 数据块2，至此 HDFS 写数据流程结束 HDFS 写流程1231. Client 向 NameNode 发出读请求，表明要从集群中读取文件 data。2. NameNode 收到 Client 请求后，就将存储了 data 文件数据块的DataNode 节点信息发送给 Client，如上图，DataNode-1 存储了 数据块1，DataNode-2 存储了 数据块2，DataNode-3 存储了 数据块1 和 数据块23. Client 依据收到 NameNode 的信息，先从 DataNode-1 读取 数据块1 ，然后再从 DataNode-2 读取 数据块2，如果 DataNode-2 宕机，Client 就会向 DataNode-3 读取 数据块2， 至此 HDFS 读数据流程结束 HDFS 常用命令 类Linux系统命令: ls, cat, mkdir, rm, chomd, chown, … HDFS文件交互命令: copyFromLocal, copyToLocal, get, put","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://luckymartinlee.github.io/tags/Hadoop/"},{"name":"大数据","slug":"大数据","permalink":"http://luckymartinlee.github.io/tags/大数据/"}]},{"title":"Elasticsearch从入门到放弃(三) -- 高级","slug":"elasticsearch_1-3","date":"2018-02-01T07:41:28.000Z","updated":"2020-12-10T06:07:25.816Z","comments":true,"path":"2018/02/01/elasticsearch_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2018/02/01/elasticsearch_1-3/","excerpt":"","text":"","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://luckymartinlee.github.io/tags/搜索引擎/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"},{"name":"ES","slug":"ES","permalink":"http://luckymartinlee.github.io/tags/ES/"}]},{"title":"Elasticsearch从入门到放弃(二) -- 零基础环境搭建","slug":"elasticsearch_1-2","date":"2018-01-28T01:33:35.000Z","updated":"2018-05-09T00:50:00.000Z","comments":true,"path":"2018/01/28/elasticsearch_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2018/01/28/elasticsearch_1-2/","excerpt":"","text":"本文主要讲述 Linux 虚拟机环境下 Elasticsearch 5.5 版本的安装。安装 Elasticsearch 之前，请确保你的机器 Java 8 环境已经搭建好，保证环境变量 JAVA_HOME 设置正确。如你还没有安装好 JAVA 8 环境，请参考linux系统中JAVA环境搭建 下载并解压安装包123$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.0.zip$ unzip elasticsearch-5.5.0.zip$ cd elasticsearch-5.5.0/ 修改配置文件Elasticsearch 默认只允许本机访问，远程访问，需要修改 config/elasticsearch.yml,改为运行所有人访问 0.0.0.0（生产环境，切不可这样改，可以指定特定 ip 访问 Elasticsearch）1234$ vim config/elasticsearch.yml#network.host: 192.168.0.1network.host: 0.0.0.0 启动程序直接运行 bin 目录下的 elasticsearch1$ ./bin/elasticsearch 此过程中，可能会报出以下错误1max virtual memory areas vm.maxmapcount [65530] is too low 此时，切换到管理员用户，运行下面的命令即可1# sysctl -w vm.max_map_count=262144 重新运行 elasticsearch 即可，这时候访问 9200 端口，得到如下信息：1234567891011121314$ curl http://127.0.0.1:9200&#123; \"name\" : \"uA7Io-i\", # node 名称 \"cluster_name\" : \"elasticsearch\", # 集群名称 \"cluster_uuid\" : \"f7y_hJefSw-kQFN-zt-3Cw\", # 集群唯一 id \"version\" : &#123; \"number\" : \"5.5.0\", # Elasticsearch 版本号 \"build_hash\" : \"260387d\", \"build_date\" : \"2017-06-30T23:16:05.735Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" # 依赖的 Lucene 版本号 &#125;, \"tagline\" : \"You Know, for Search\"&#125; 到这里说明你的 Elasticsearch 已经安装成功了，按下 Ctrl + C，Elasticsearch 就会停止运行。 想要后台运行 Elasticsearch，输入下面命令：1$ ./bin/elasticsearch -d 此时想要停止 Elasticsearch ，想要先找到 Elasticsearch 进程，然后 kill 。12345$ ps -ef |grep elasticsearchmartin 3035 2200 0 Apr19 ? 00:04:29 /usr/bin/java -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch -server -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -Djdk.io.permissionsUseCanonicalPath=true -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j.skipJansi=true -XX:+HeapDumpOnOutOfMemoryError -Des.path.home=/home/marting/elasticsearch-5.5.0 -cp /home/martin/elasticsearch-5.5.0/lib/* org.elasticsearch.bootstrap.Elasticsearch$ kill -2 3035","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://luckymartinlee.github.io/tags/搜索引擎/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"},{"name":"ES","slug":"ES","permalink":"http://luckymartinlee.github.io/tags/ES/"}]},{"title":"Elasticsearch从入门到放弃(一) -- 基本概念","slug":"elasticsearch_1-1","date":"2018-01-26T07:23:08.000Z","updated":"2018-05-09T00:50:00.000Z","comments":true,"path":"2018/01/26/elasticsearch_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2018/01/26/elasticsearch_1-1/","excerpt":"","text":"相关概念通用搜索 与 垂直搜索&ensp;&ensp;通用搜索，通俗点说就是在网络上的所有可以获取的信息中进行搜索，涉及所有领域，所有展现形式，包括音频、视频、文字、图片。一句话就是只要信息内容匹配你的搜索条件，就返回给你，知名站点有 Baidu, Google, Bing.&ensp;&ensp;垂直搜索，相对于通用搜索，它是针对特定行业信息的搜索。比如，用于图标搜索的EasyIcon、小说搜索Owllook 等。 全文检索 与 倒排索引&ensp;&ensp;全文检索，概念就是通过计算机实现，你的搜索词出现在文章当中，甚至搜索词与文章有相关性，那么这篇文章就应该被搜索出来。计算机实现这个功能整个过程叫做全文检索。&ensp;&ensp;倒排索引，是实现全文检索的技术手段之一。简单的说就是把文章拆解成一个个简单的词语（此过程叫分词），将文章与这些词语建立起关联关系（即是索引）。用户查询时，也讲搜索词进行分词，再在前面建立的关联关系中查找文章，以此实现全文检索。想更进一步知道什么倒排索引的实现，请看我的另一篇文章《什么是倒排索引》 Lucene&ensp;&ensp;Lucene，是一个开源免费的成熟的Java系的信息检索程序库，全文检索引擎工具包，由Apache软件基金会支持和提供。严格的说，Lucene 不是全文检索引擎或者搜索引擎，它提供了查询组件，索引组件以及文本分析组件，它是一个全文检索引擎的框架。 Elasticsearch何为 Elasticsearch&ensp;&ensp;Elasticsearch，是一款基于Lucene的开源的高扩展的分布式全文检索引擎。Elastic 是 Lucene 的封装，提供了 RESTFull API 的操作接口，开箱即用。它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。 Elasticsearch 核心概念Near Realtime (NRT)&ensp;&ensp;近实时概念包含两层含义，一是数据从写入到可搜索到，时间达到秒级别；二是Elasticsearch 查询、聚合、分析，时间到达秒级别。 Cluster&ensp;&ensp;Elasticsearch是一个分布式的全文检索引擎，多个程序节点构成一个大的集群，集群中节点即是数据备份节点，也是数据查询分析负载均衡节点。 Node&ensp;&ensp;节点，即是一个Elasicsearch的实例，一台机器可以运行一个或多个节点。 Shard(primary shard)&ensp;&ensp;分片是为了解决海量数据存储问题，Elasticsearch 使用分片机制，将海量数据切分为多个分片存储在不同的节点上。Elasticseach分片就是它的主分片，Elasticsearch默认主分片数量是5。 replica(replica shard)&ensp;&ensp;分片副本，意思显而易见，就是分片的备份，作用是提高集群数据安全，提升系统高可用性，同时副本节点在海量数据检索时也分担检索压力，具备提升 Elasticsearch 请求吞吐量和性能作用。 Index&ensp;&ensp;这里的 Index 不是查询索引的概念，可以理解为同类型数据的库，相当于 MySQL 中的库，但又有不同，不同之处在于此处的Index中存储的都是字段类型基本一致的同类型数据。 Type&ensp;&ensp;这里的 Type ，可以理解为 MySQL 中的表，一个 Index 中可以有多个 Type，且一个 Type存储同种数据。如，一个名为 animal 的 Index 中有 bird 和 fish 两个 Type, 他们有很多共同的属性。 Document&ensp;&ensp;Document，即是 Type 中具体的文档。每个文档都有自己唯一的 id. Field&ensp;&ensp;Field，是文档的属性或者叫做字段，不同字段，类型不同，分词方式也不同。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://luckymartinlee.github.io/tags/Elasticsearch/"},{"name":"ES","slug":"ES","permalink":"http://luckymartinlee.github.io/tags/ES/"}]},{"title":"Scala 函数与方法的区分与理解","slug":"scala_1-4","date":"2017-12-11T01:33:25.000Z","updated":"2020-12-15T09:04:22.621Z","comments":true,"path":"2017/12/11/scala_1-4/","link":"","permalink":"http://luckymartinlee.github.io/2017/12/11/scala_1-4/","excerpt":"","text":"方法：12345scala&gt; def add(x:Int, y: Int) = x + yadd: (x: Int, y: Int)Intscala&gt; add(1, 2)res0: Int = 3 函数：1234567scala&gt; val add_f = (x: Int, y: Int) =&gt; x + yadd_f: (Int, Int) =&gt; Int = &lt;function2&gt;// 根据内容可以看出add_f是一个函数Functionscala&gt; add_f(1, 2)res1: Int = 3 上面 ‘=’号右边的内容 (x: Int, y: Int) =&gt; x + y是一个函数体，方法只能用 def 接收，函数可以用 def 接收，也可以用 val 接收。当函数用 def 来接收之后，不再显示为 function ，转换为方法。方法可以省略参数，函数不可以。 函数可以作为方法的参数。 看下面的例子：12345scala&gt; val a = () =&gt; 100a: () =&gt; Int = &lt;function0&gt;scala&gt; val a = =&gt; 100&lt;console&gt;:1: error: illegal start of simple expression 看这里: val a = =&gt; 100 // 当函数参数为空时报错","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"什么是倒排索引","slug":"inverted_index_1","date":"2017-12-03T01:06:12.000Z","updated":"2018-05-02T08:24:26.000Z","comments":true,"path":"2017/12/03/inverted_index_1/","link":"","permalink":"http://luckymartinlee.github.io/2017/12/03/inverted_index_1/","excerpt":"","text":"在搜索引擎的设计逻辑中，为每一搜索目标文件都生成一个唯一的 ID ，而文件的内容可以看成是很多提取出来的关键词的集合，提取关键词的过程叫做 “分词”。例如，文件1的 ID 是 1001，经过分词，总共提取出30个关键词，那么搜索引擎就会记录每个关键词出现在文章当中的位置和出现次数。 下面说说什么是 倒排索引，有倒排索引，相应的肯定就有正向索引。 正向索引正向索引的结构：1231001(文档1) &gt; &#123;中国(关键词1):&#123;出现次数:2;出现位置：10,15&#125;,&#123;劳动(关键词2):&#123;出现次数:3;出现位置：2,7,11&#125;,...1002(文档2) &gt; &#123;中国(关键词1):&#123;出现次数:3;出现位置：6,23,45&#125;,&#123;体育(关键词3):&#123;出现次数:1;出现位置：1&#125;,...... 如图所示: 对于 正向索引，假设，用户搜索关键词“中国”，搜索引擎就要完整遍历索引中所有信息，找到包含“中国”的文件，然后依据特定的打分排序算法，整理出文件先后顺序，再返回给用户。可以看出，正向索引明显的弊端就是，对于海量数据的搜索引擎系统，正向索引结构效率低下，无法快速响应用户。 倒排索引倒排索引的结构：1234中国(关键词1) &gt; &#123;1001(文档1):&#123;出现次数:2;出现位置：10,15&#125;,&#123;1002(文档2):&#123;出现次数:3;出现位置：6,23,45&#125;,...劳动(关键词2)&gt; &#123;1001(文档1):&#123;出现次数:3;出现位置：2,7,11&#125;,...体育(关键词3)&gt; &#123;1002(文档1):&#123;出现次数:1;出现位置：1&#125;,...... 如图所示:对于 倒排索引，同样假设，用户搜索关键词“中国”，搜索引擎就要不必完整遍历索引中所有信息，很快就能找到包含“中国”的文件，然后依据特定的打分排序算法，整理出文件先后顺序，返回给用户。可以看出，倒排索引很好解决了正向索引的弊端就是，对于海量数据的搜索引擎系统，倒排索引结构效率较高，可以较快响应用户。","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"http://luckymartinlee.github.io/tags/搜索引擎/"}]},{"title":"Scala 闭包(closure) 深入理解","slug":"scala_1-2","date":"2017-10-30T11:24:55.000Z","updated":"2020-12-15T02:08:30.135Z","comments":true,"path":"2017/10/30/scala_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2017/10/30/scala_1-2/","excerpt":"","text":"什么是 Scala 闭包闭包是一个函数，返回值依赖于声明在函数外部的一个或多个变量。闭包通常来讲可以简单的认为是可以访问一个函数里面局部变量的另外一个函数。如下实例：12345678scala&gt; var more =1more: Int = 1 scala&gt; val addMore = (x:Int) =&gt; x + moreaddMore: Int =&gt; Int = &lt;function1&gt;scala&gt; addMore (100)res1: Int = 101 其中，我们定义函数变量 addMore 成为一个“闭包”，因为它引用到函数外面定义的变量 more，定义这个函数的过程是将这个自由变量捕获而构成一个封闭的函数。有意思的是，当这个自由变量发生变化时，Scala 的闭包能够捕获到这个变化，因此 Scala 的闭包捕获的是变量本身而不是当时变量的值。 同样的，如果变量在闭包内发生变化，也会反映到函数外面定义的闭包的值。如下实例:12345678910scala&gt; val someNumbers = List ( -11, -10, -5, 0, 5, 10)someNumbers: List[Int] = List(-11, -10, -5, 0, 5, 10)scala&gt; var sum =0sum: Int = 0scala&gt; someNumbers.foreach ( sum += _)scala&gt; sumres4: Int = -11 上面可以看到在闭包中修改sum的值，其结果还是传递到闭包的外面。&nbsp;那如果一个闭包所访问的变量有几个不同的版本，比如一个闭包使用了一个函数的局部变量（参数），然后这个函数调用很多次，那么所定义的闭包应该使用所引用的局部变量的哪个版本呢？ 简单的说，该闭包定义所引用的变量为定义该闭包时变量的值，也就是定义闭包时相当于保存了当时程序状态的一个快照。比如我们定义下面一个函数闭包:1234567891011121314scala&gt; def makeIncreaser(more:Int) = (x:Int) =&gt; x + moremakeIncreaser: (more: Int)Int =&gt; Intscala&gt; val inc1=makeIncreaser(1)inc1: Int =&gt; Int = &lt;function1&gt;scala&gt; val inc9999=makeIncreaser(9999)inc9999: Int =&gt; Int = &lt;function1&gt;scala&gt; inc1(10)res5: Int = 11scala&gt; inc9999(10)res6: Int = 10009 当你调用makeIncreaser(1)时，你创建了一个闭包，该闭包定义时more的值为1, 而调用makeIncreaser(9999)所创建的闭包的more的值为9999。此后你也无法修改已经返回的闭包的more的值。因此inc1始终为加一，而inc9999始终为加9999.","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"Scala 柯里化(curry) 深入理解","slug":"scala_1-1","date":"2017-10-23T01:44:17.000Z","updated":"2020-12-10T07:47:23.427Z","comments":true,"path":"2017/10/23/scala_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2017/10/23/scala_1-1/","excerpt":"","text":"什么是柯里化函数有多个参数列表 的函数就是柯里化函数，所谓的参数列表就是使用小括号括起来的函数参数列表, 函数柯里化就是 把接受多个参数的函数变换成接受一个单一参数(最初函数的第一个参数)的函数，并且返回接受余下的参数且返回结果的新函数的过程。如下所示12345// 非柯里化函数def sum(x:Int,y:Int)=x+y// 柯里化函数def sum(x:Int)(y:Int) = x + y 实例如下： sum(1)(2) 实际上是依次调用两个普通函数（非柯里化函数），第一次调用使用一个参数 x，返回一个函数类型的值，第二次使用参数y调用这个函数类型的值。 实质上最先演变成这样一个方法：1def sum(x:Int)=(y:Int) =&gt; x+y 那么这个函数是什么意思呢？ 接收一个x为参数，返回一个匿名函数，该匿名函数的定义是：接收一个Int型参数y，函数体为x+y. 柯里化的意义柯里化的意义在于把多个参数的function等价转化成多个单参数function的级联，这样方便做lambda演算。 同时curry化对类型推演也有帮助，scala的类型推演是局部的，在同一个参数列表中后面的参数不能借助前面的参数类型进行推演，curry化以后，放在两个参数列表里，后面一个参数列表里的参数可以借助前面一个参数列表里的参数类型进行推演。这就是为什么 foldLeft这种函数的定义都是curry的形式。函数柯里化在提高函数适用性和延迟执行或者固定易变因素等方面有着重要的作用，加上scala语言本身就是推崇简洁编码，使得同样功能的函数在定义与转换的时候会更加灵活多样。","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"Scala 偏函数与部分应用函数的区分与理解","slug":"scala_1-3","date":"2017-07-11T13:33:18.000Z","updated":"2020-12-15T09:31:22.079Z","comments":true,"path":"2017/07/11/scala_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2017/07/11/scala_1-3/","excerpt":"","text":"部分应用函数 (Partial Applied Function)部分应用函数是缺少部分参数的函数，是一个逻辑上概念, 比如：1def sum(x: Int)(y: Int) = x + y 当调用sum的时候，如果不提供所有的参数或某些参数还未知时，比如sum _ , sum(3)(: Int), sum(: Int)(3), 这样就生成了所谓的部分应用函数。部分应用函数只是逻辑上的一个表达，scala编译器会用Function1， Function2这些类来表示它. 偏函数 Partial Function偏函数是只对函数定义域的一个子集进行定义的函数, 对于这个参数范围外的参数则抛出异常，这样的函数就是偏函数（顾名思异就是这个函数只处理传入来的部分参数）。 scala中用scala.PartialFunction[T,S] Trait 来表示,其中接收一个类型为 T 的参数，返回一个类型为 S 的结果。 1234val signal: PartialFunction[Int, Int] = &#123; case x if x &gt;= 1 =&gt; 1 case x if x &lt;= -1 =&gt; -1&#125; 这个signal所引用的函数除了0值外，对所有整数都定义了相应的操作。 signal(0) 会抛出异常，因此使用前最好先signal.isDefinedAt(0)判断一下。 偏函数主要用于这样一种场景：对某些值现在还无法给出具体的操作（即需求还不明朗），也有可能存在几种处理方式（视乎具体的需求）；我们可以先对需求明确的部分进行定义，比如上述除了0外的所有整数域，然后根据具体情况补充对其他域的定义，比如 :12345val composed_signal: PartialFunction[Int,Int] = signal.orElse&#123;case 0 =&gt; 0&#125;composed_signal(0) // 返回 0 或者对定义域进行一定的偏移（假如需求做了变更, 1 为无效的点） 1234567val new_signal: Function1[Int, Int] = signal.compose&#123; case x =&gt; x - 1&#125;new_signal(1) // throw exceptionnew_signal(0) // 返回 -1 new_signal(2) // 返回 1 还可以用andThen将两个相关的偏函数串接起来12345678910111213val another_signal: PartialFunction[Int, Int] = &#123; case 0 =&gt; 0 case x if x &gt; 0 =&gt; x - 1 case x if x &lt; 0 =&gt; x + 1&#125;val then_signal = another_signal andThen signalthen_signal(0) // throw exceptionthen_signal(-1) // throw exceptionthen_signal(1) // throw exceptionthen_signal(2) // 返回 1then_signal(-2) // 返回 -1 这里的then_signal 剔除了-1, 0, 1三个点的定义","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://luckymartinlee.github.io/tags/Scala/"}]},{"title":"curl 工具使用","slug":"curl_1-1","date":"2017-04-20T06:33:23.000Z","updated":"2020-12-10T07:43:40.522Z","comments":true,"path":"2017/04/20/curl_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2017/04/20/curl_1-1/","excerpt":"","text":"工具简介curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思。它的功能非常强大，命令行参数多达几十种。如果熟练的话，完全可以取代 Postman 这一类的图形界面工具。 参数简介123456789101112131415161718192021222324252627282930313233343536373839404142434445# 调试类-v, --verbose 输出信息-q, --disable 在第一个参数位置设置后 .curlrc 的设置直接失效，这个参数会影响到 -K, --config -A, --user-agent -e, --referer-K, --config FILE 指定配置文件-L, --location 跟踪重定向 (H)# CLI显示设置-s, --silent Silent模式。不输出任务内容-S, --show-error 显示错误. 在选项 -s 中，当 curl 出现错误时将显示-f, --fail 不显示 连接失败时HTTP错误信息-i, --include 显示 response的header (H/F)-I, --head 仅显示 响应文档头-l, --list-only 只列出FTP目录的名称 (F)-#, --progress-bar 以进度条 显示传输进度# 数据传输类-X, --request [GET|POST|PUT|DELETE|…] 使用指定的 http method 例如 -X POST-H, --header &lt;header&gt; 设定 request里的header 例如 -H \"Content-Type: application/json\"-e, --referer 设定 referer (H)-d, --data &lt;data&gt; 设定 http body 默认使用 content-type application/x-www-form-urlencoded (H) --data-raw &lt;data&gt; ASCII 编码 HTTP POST 数据 (H) --data-binary &lt;data&gt; binary 编码 HTTP POST 数据 (H) --data-urlencode &lt;data&gt; url 编码 HTTP POST 数据 (H)-G, --get 使用 HTTP GET 方法发送 -d 数据 (H)-F, --form &lt;name=string&gt; 模拟 HTTP 表单数据提交 multipart POST (H) --form-string &lt;name=string&gt; 模拟 HTTP 表单数据提交 (H)-u, --user &lt;user:password&gt; 使用帐户，密码 例如 admin:password-b, --cookie &lt;data&gt; cookie 文件 (H)-j, --junk-session-cookies 读取文件中但忽略会话cookie (H)-A, --user-agent 指定客户端的用户代理标头，user-agent设置，默认用户代理字符串是curl/[version] (H)# 传输设置-C, --continue-at OFFSET 断点续转-x, --proxy [PROTOCOL://]HOST[:PORT] 在指定的端口上使用代理-U, --proxy-user USER[:PASSWORD] 代理用户名及密码# 文件操作-T, --upload-file &lt;file&gt; 上传文件-a, --append 添加要上传的文件 (F/SFTP)# 输出设置-o, --output &lt;file&gt; 将输出写入文件，而非 stdout-O, --remote-name 将输出写入远程文件-D, --dump-header &lt;file&gt; 将头信息写入指定的文件-c, --cookie-jar &lt;file&gt; 操作结束后，要写入 Cookies 的文件位置 常用实例GET 请求1curl http://www.yahoo.com/login.cgi?user=XXXXXXXXX&amp;password=XXXXXX POST 请求123curl -d \"user=XXXXXXXX&amp;password=XXXXX\" http://www.yahoo.com/login.cgi// POST 文件curl -F upload= $localfile -F $btn_name=$btn_value http://192.168.10.1/www/focus/up_file.cgi 分块下载1234567curl -r 0 -10240 -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3 &amp;\\curl -r 10241 -20480 -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3 &amp;\\curl -r 20481 -40960 -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3 &amp;\\curl -r 40961 - -o \"zhao.part1\" http://192.168.10.1/www/focus/zhao1.mp3...// 合并块文件cat zhao.part* &gt; zhao.mp3 ftp 下载1curl -O ftp://用户名:密码@192.168.10.1:21/www/focus/enhouse/index.php ftp 上传1curl -T upload_test.php ftp://用户名:密码@192.168.10.1:21/www/focus/enhouse/","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://luckymartinlee.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://luckymartinlee.github.io/tags/Shell/"}]},{"title":"MongoDB从入门到放弃(一) -- 基础1","slug":"mongodb_1-1","date":"2016-10-25T05:25:15.000Z","updated":"2018-05-09T00:49:52.000Z","comments":true,"path":"2016/10/25/mongodb_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2016/10/25/mongodb_1-1/","excerpt":"","text":"MongoDB 是一个由 C++ 语言编写的基于分布式文件存储的 NoSQL 数据库，为 WEB 应用提供可扩展的高性能数据存储解决方案。MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 NoSQLNoSQL(NoSQL = Not Only SQL )，指的是非关系型的数据库，是对不同于传统的关系型数据库的数据库管理系统的统称。NoSQL 用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 RDBMS vs NoSQLRDBMS 高度组织化结构化数据 结构化查询语言(SQL) 数据和关系都存储在单独的表中。 数据操纵语言，数据定义语言 严格的一致性 基础事务 NoSQL 代表着不仅仅是SQL 没有声明性查询语言 没有预定义的模式 键值对(key=&gt;value)存储，列存储，文档存储，图形数据库 最终一致性，而非ACID属性 非结构化和不可预知的数据 CAP定理 高性能，高可用性和可伸缩性 NoSQL的优/缺点优点: 高可扩展性 分布式计算 低成本 架构的灵活性，半结构化数据 没有复杂的关系 缺点: 没有标准化 有限的查询功能（到目前为止） 最终一致是不直观的程序 MongoDB 特点 MongoDB安装简单，MongoDB 是一个面向文档存储的数据库，操作起来简单、容易。 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是分片。 Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。 GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。 MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。 MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 MongoDB 基本概念在 MongoDB 中基本的概念有数据库、集合、文档、域、索引等，相较传统的关系型数据库对比图如下： SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 嵌入文档 表连接,MongoDB不支持表连接,但是有内嵌文档可以替代 primary key primary key 主键,MongoDB自动将_id字段设置为主键 举例如下： 安装(64 位 Linux上的安装)下载并解压tgz包1234$ curl -O https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.6.tgz # 下载$ tar -zxvf mongodb-linux-x86_64-3.0.6.tgz # 解压$ mv mongodb-linux-x86_64-3.0.6/ /usr/local/mongodb # 将解压包拷贝到指定目录 添加 MongoDB 可执行文件路径到 PATH1$ sudo export PATH=/usr/local/mongodb/bin:$PATH 创建数据存储目录，MongoDB 默认数据存储路径(dbpath)是 /data/db1$ sudo mkdir -p /data/db 启动 MongoDB 服务1$ /usr/local/mongodb/bin/mongod MongoDB web 界面MongoDB 3.2 及之前版本，提供了简单的 HTTP 用户界面，此功能需要在启动服务的时候添加参数 rest, 默认端口280171$ /usr/local/mongodb/bin/mongod --dbpath=/data/db --rest 如下图：","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"},{"name":"MongoDB","slug":"MongoDB","permalink":"http://luckymartinlee.github.io/tags/MongoDB/"}]},{"title":"MySQL从入门到放弃(四) -- 索引","slug":"mysql_1-4","date":"2016-05-22T10:45:50.000Z","updated":"2018-05-09T00:49:52.000Z","comments":true,"path":"2016/05/22/mysql_1-4/","link":"","permalink":"http://luckymartinlee.github.io/2016/05/22/mysql_1-4/","excerpt":"","text":"数据库索引的概念常见索引类型MySQL常见索引有：主键索引、唯一索引、普通索引、全文索引、组合索引 普通索引：最基本的索引，没有任何限制。唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。主键索引：它是一种特殊的唯一索引，不允许有空值，在一张表中只能定义一个主键索引。主键用于唯一标识一条记录，使用关键字 PRIMARY KEY 来创建。主键分为复合主键和联合主键。全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。 索引存储结构B+树：B+树数据结构以平衡树的形式来组织，因为是树型结构，所以更适合用来处理排序，范围查找等功能。相对 Hash ，B+树在查找单条记录的速度虽然比不上 Hash ，但是因为更适合排序等操作，所以他更受用户的欢迎。毕竟不可能只对数据库进行单条记录的操作. Hash：Hash 用的较少，它是把数据的索引以 Hash 形式组织起来，因此当查找某一条记录，速度非常快。缺点是 Hash 结构，每个键只对应一个值，分布方式是散列的。所以 Hash 并不支持范围查找和排序等功能. 数据库索引的增删改创建索引 执行 CREATE TABLE 语句时可以创建索引, 见MySQL从入门到放弃 – 语法1:数据库表操作。 使用 ALTER TABLE 语句时可以创建索引 1234mysql&gt;ALTER TABLE table_name ADD INDEX index_name (column_list) COMMENT '普通索引';mysql&gt;ALTER TABLE table_name ADD UNIQUE index_name (column_list) COMMENT '唯一索引';mysql&gt;ALTER TABLE table_name ADD PRIMARY KEY (column_list) COMMENT '主键';mysql&gt;ALTER TABLE table_name ADD FULLTEXT index_name (column_list) COMMENT '全文索引'; 使用 CREATE INDEX 语句时可以创建索引注: 不能用CREATE INDEX语句创建PRIMARY KEY索引 123mysql&gt;CREATE INDEX index_name ON table_name (column_list) COMMENT '普通索引';mysql&gt;CREATE UNIQUE INDEX index_name ON table_name (column_list) COMMENT '唯一索引;mysql&gt;CREATE FULLTEXT INDEX index_name ON table_name (column_list) COMMENT '全文索引'; 删除索引如删除多列组合索引的某列，则该列也会从索引中删除。如删除组合索引的所有列，则整个索引将被删除。 只使用 DORP 关键字1mysql&gt;DROP INDEX 索引名 ON 表名 列名; 使用 ALTER DORP 两个关键字123456mysql&gt;ALTER TABLE 表名 DROP INDEX 索引名 列名;mysql&gt;ALTER TABLE 表名 DROP UNIQUE 索引名 列名;// 因为一个表只可能有一个PRIMARY KEY索引，因此也可不指定索引名。// 如果没有创建PRIMARY KEY索引，但表具有一个或多个UNIQUE索引，则MySQL将删除第一个UNIQUE索引。mysql&gt;ALTER TABLE 表名 DROP PRIMARY KEY 索引名 列名; 重建索引长时间运行数据库后，索引有可能被损坏，这时就需要重建。重建索引可以提高检索效率。1mysql&gt;REPAIR TABLE table_name QUICK; 查询索引使用 INDEX 关键字1mysql&gt;SHOW INDEX FROM table_name; 使用 KEYS 关键字1mysql&gt;SHOW KEYS FROM table_name;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"}]},{"title":"MySQL从入门到放弃(三) -- 字段","slug":"mysql_1-3","date":"2016-05-20T02:37:45.000Z","updated":"2018-05-09T00:49:50.000Z","comments":true,"path":"2016/05/20/mysql_1-3/","link":"","permalink":"http://luckymartinlee.github.io/2016/05/20/mysql_1-3/","excerpt":"","text":"新增字段新增字段时，关键词 COLUMN 可以省略 新增一个字段，默认值为空1ALTER TABLE `user` ADD COLUMN `new_feild1` VARCHAR(20) DEFAULT NULL; 新增一个字段，默认值为不能为空，备注 ‘备用字段’，新字段 放在 username 后1ALTER TABLE `user` ADD COLUMN `new_feild2` VARCHAR(20) NOT NULL COMMENT '备用字段' AFTER `username`;; 删除字段1ALTER TABLE `user` DROP COLUMN `new_feild1`; 修改字段修改字段有两种方式方式一 使用 MODIFY 关键字 (主要用于修改字段类型)修改类型为 VARCHAR(10)，默认值为 空字符串1ALTER TABLE `user` MODIFY `new_feild2` VARCHAR(10) DEFAULT '' COMMENT '修改字段'; 方式二 使用 CHANGE 关键字 (主要用于修改字段名称)修改字段名，并设为整型，默认值为 01ALTER TABLE `user` CHANGE `new_feild2` `new_feild3` INT DEFAULT 0 COMMENT '修改字段'; 何时使用 MODIFY ，何时使用 CHANGE ，其实无可厚非的，最主要是个人的习惯。 查询字段名和字段注释1SELECT `COLUMN_NAME`,`COLUMN_COMMENT`,`COLUMN_TYPE` FROM `information_schema`.`COLUMNS` WHERE TABLE_NAME='表名' AND TABLE_SCHEMA='数据库名';","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"}]},{"title":"MySQL从入门到放弃(二) -- 表","slug":"mysql_1-2","date":"2016-05-13T05:31:56.000Z","updated":"2018-05-09T00:49:50.000Z","comments":true,"path":"2016/05/13/mysql_1-2/","link":"","permalink":"http://luckymartinlee.github.io/2016/05/13/mysql_1-2/","excerpt":"","text":"数据库表的增删改创建数据库表注: CHAR列的长度固定为创建表时声明的长度。长度可以为从0到255的任何值。当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉，所以，我们在存储时字符串右边不能有空格，即使有，查询出来后也会被删除。VARCHAR列中的值为可变长字符串, 同CHAR对比，VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则使用两个字节),VARCHAR值保存时不进行填充。当值保存和检索时尾部的空格仍保留，符合标准SQL. BINARY和VARBINARY类型类似于CHAR和VARCHAR类型，但是不同的是，它们存储的不是字符字符串，而是二进制串. 保存BINARY值时，在它们右边填充0x00(零字节)值以达到指定长度。取值时不删除尾部的字节; 对于VARBINARY，插入时不填充字符，选择时不裁剪字节. 长度限制:CHAR、VARCAHR的长度是指字符的长度，例如CHAR[3]则只能放字符串”123”，如果插入数据”1234”，则从高位截取，变为”123”。 VARCAHR同理。TINYINT、SMALLINT、MEDIUMINT、INT和BIGINT的长度，其实和数据的大小无关！Length指的是显示宽度;FLOAT、DOUBLE和DECIMAL的长度指的是全部数位（包括小数点后面的），例如DECIMAL(4,1)指的是全部位数为4，小数点后1位，如果插入1234，则查询的数据是999.9; BLOB, TEXT, GEOMETRY or JSON 不能有默认值. FULLTEXT 索引仅可用于 MyISAM 表, 为 CHAR, VARCHAR, TEXT 列 创建全文索引. 123456789101112131415161718# 1. 一般create table 语句CREATE TABLE IF NOT EXISTS `users` ( `id` INT(11) UNSIGNED NOT NULL [PRIMARY KEY] AUTO_INCREMENT COMMENT '用户id', `name` VARCHAR(20) NOT NULL DEFAULT '' COMMENT '姓名', `sex` TINYINT(1) UNSIGNED NOT NULL DEFAULT 1 COMMENT '性别', `age` TINYINT(2) UNSIGNED NOT NULL DEFAULT 1 COMMENT '年龄', `address` TEXT(500) NOT NULL COMMENT '地址', PRIMARY KEY (`id`) COMMENT '主键', INDEX `age_idx` (`age`) COMMENT '普通索引', UNIQUE `name_idx`(`name`(20)) COMMENT '唯一索引', FULLTEXT `address` (`address`) COMMENT '全文索引') ENGINE = MyISAM DEFAULT CHARSET utf8 COLLATE utf8_general_ci AUTO_INCREMENT=1 COMMENT '用户信息表';# 2. create table like 参照已有表的定义，来定义新的表CREATE TABLE IF NOT EXISTS `users_like` LIKE `users`;# 3. 根据select 的结果集来创建表CREATE TABLE IF NOT EXISTS `user_select` AS SELECT `id`, `name` FROM `users`; 删除数据库表1DROP DATABASE `users`; 修改表自增起始ID1ALTER TABLE `users` AUTO_INCREMENT=2; 查看数据库表的列表1SHOW TABLES;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"}]},{"title":"MySQL从入门到放弃(一) -- 库","slug":"mysql_1-1","date":"2016-05-10T09:13:12.000Z","updated":"2018-05-09T00:49:48.000Z","comments":true,"path":"2016/05/10/mysql_1-1/","link":"","permalink":"http://luckymartinlee.github.io/2016/05/10/mysql_1-1/","excerpt":"","text":"数据库的创建与删除创建数据库注: CREATE DATABASE 不支持 comment。 COLLATE: 校对集,在某个字符集的情况下，字符集的排列顺序应该是什么，称之为校对集。1CREATE DATABASE IF NOT EXISTS `my_db` DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 删除数据库1DROP DATABASE IF EXIST `my_db`; 查看数据库列表1SHOW DATABASES; 查看数据库DDL1SHOW CREATE DATABASE `my_db`; 查看当前使用的数据库1SELECT database(); 切换数据库1USE `my_db2`;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luckymartinlee.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://luckymartinlee.github.io/tags/数据库/"}]}]}